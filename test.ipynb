{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = Llama(\n",
    "  model_path=\"/teamspace/studios/this_studio/mistral-7b-gguf/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",  # Download the model file first\n",
    "  n_ctx=32768,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=0         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    ")\n",
    "\n",
    "# Simple inference example\n",
    "output = llm(\n",
    "  \"<s>[INST] {prompt} [/INST]\", # Prompt\n",
    "  max_tokens=512,  # Generate up to 512 tokens\n",
    "  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "  echo=True        # Whether to echo the prompt\n",
    ")\n",
    "\n",
    "# Chat Completion API\n",
    "\n",
    "llm = Llama(model_path=\"./mistral-7b-instruct-v0.2.Q4_K_M.gguf\", chat_format=\"llama-2\")  # Set chat_format according to the model you are using\n",
    "llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a story about llamas.\"\n",
    "        }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download TheBloke/TinyLlama-1.1B-intermediate-step-1431k-3T-GGUF tinyllama-1.1b-intermediate-step-1431k-3t.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = Llama(\n",
    "  model_path=\"/teamspace/studios/this_studio/mistral-7b-gguf/tinyllama-1.1b-intermediate-step-1431k-3t.Q4_K_M.gguf\",  # Download the model file first\n",
    "  n_ctx=2048,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads=8,            # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers=0         # The number of layers to offload to GPU, if you have GPU acceleration available\n",
    ")\n",
    "\n",
    "# Simple inference example\n",
    "output = llm(\n",
    "#   \"{prompt}\", # Prompt\n",
    "  prompt=\"Write a story about llamas.\",\n",
    "  max_tokens=512,  # Generate up to 512 tokens\n",
    "  stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n",
    "  echo=True        # Whether to echo the prompt\n",
    ")\n",
    "\n",
    "# Chat Completion API\n",
    "\n",
    "# llm = Llama(model_path=\"./tinyllama-1.1b-intermediate-step-1431k-3t.Q4_K_M.gguf\", chat_format=\"llama-2\")  # Set chat_format according to the model you are using\n",
    "# llm.create_chat_completion(\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are a story writing assistant.\"},\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Write a story about llamas.\"\n",
    "#         }\n",
    "#     ]\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import CTransformers\n",
    "llm = CTransformers(model='/teamspace/studios/this_studio/mistral-7b-gguf/tinyllama-1.1b-intermediate-step-1431k-3t.Q4_K_M.gguf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI think I might have seen that on the web site, but I don\\'t know all there\\'s to it.\\nThe 1001 version can still be used for SX815/SX816 and the newer devices will support the 2001 version.\\nWhat is this RW? Can I get some help with this?\\nIt is a \"register write\" which basically tells the camera that you have opened or closed the shutter, and what value it was. The value itself (i.e., your password) is not sent to the camera, but only the register number.\\nI\\'ve had some problems in the past with the 1001 version.\\nThere\\'s a problem with the 2001 version though because your password has to be encoded using hexadecimal (instead of base 64). The 2001 version only supports up to 16 characters, and you can only have 8 bits for each character in your encoded data (one character plus its 7 bits of value), so the 2001 version will not work with passwords that are longer than that.\\nOn the other hand, I haven'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is qlora?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " have an enormous impact on the future of the industry.\n",
      "This year, AI was all over the show floor and there are a number of companies that are using AI in their products. Here's a quick look at some of them:\n",
      "Fulcrum is an automated, artificial intelligence-powered software solution that helps manufacturers improve productivity and efficiency by automating processes, removing errors, and freeing up employees to focus on more complex tasks.\n",
      "Kapil Khera, co-founder and CEO of Fulcrum, told Inman: \"I don't know a single company that doesn't have AI in their product. I think it's the most important thing to come out of 2018.\"\n",
      "The company is currently working on machine learning tools and workflow applications with machine intelligence technology so you can really begin building these powerful AI-powered solutions. The full AI-focused ecosystem, with automation in product design and sophisticated analytics, will be available to all customers by the end of 2018.\n",
      "Kapil Khera, co-founder and CEO of Fulcrum\n",
      "Surely"
     ]
    }
   ],
   "source": [
    "for text in llm(\"AI is going to\", stream=True):\n",
    "    print(text, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results on the cpu using ctransformers\n",
    "### - 27.4 seconds\n",
    "### output:\n",
    "<p>\n",
    "have an enormous impact on the future of the industry.\n",
    "This year, AI was all over the show floor and there are a number of companies that are using AI in their products. Here's a quick look at some of them:\n",
    "Fulcrum is an automated, artificial intelligence-powered software solution that helps manufacturers improve productivity and efficiency by automating processes, removing errors, and freeing up employees to focus on more complex tasks.\n",
    "Kapil Khera, co-founder and CEO of Fulcrum, told Inman: \"I don't know a single company that doesn't have AI in their product. I think it's the most important thing to come out of 2018.\"\n",
    "The company is currently working on machine learning tools and workflow applications with machine intelligence technology so you can really begin building these powerful AI-powered solutions. The full AI-focused ecosystem, with automation in product design and sophisticated analytics, will be available to all customers by the end of 2018.\n",
    "Kapil Khera, co-founder and CEO of Fulcrum\n",
    "Surely\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_response_tiny_llama(text):\n",
    "    path='/teamspace/studios/this_studio/TinyLlama-1.1B-Chat-v1.0-GPTQ'\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "    prompt = text\n",
    "    system_message = \"You are a story writing assistant\"\n",
    "    prompt_template=f'''<|system|>\n",
    "    {system_message}</s>\n",
    "    <|user|>\n",
    "    {prompt}</s>\n",
    "    <|assistant|>\n",
    "    '''\n",
    "\n",
    "    print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "    input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "    output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "    return(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/modeling_utils.py:4371: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s> <|system|>\n",
      "    You are a story writing assistant</s> \n",
      "    <|user|>\n",
      "    what is qlora?</s> \n",
      "    <|assistant|>\n",
      "    qlora is a cloud-based platform for writing and publishing short stories. It allows users to create and publish their stories, edit and format their work, and share them with the world. Qlora is a free and open-source platform that is designed to be user-friendly and intuitive, making it easy for anyone to write and publish stories.</s>\n"
     ]
    }
   ],
   "source": [
    "print(get_response_tiny_llama(\"what is qlora?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# result on gpu 2.8 seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
