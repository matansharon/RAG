{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in llm(\"AI is going to\", stream=True):\n",
    "    print(text, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results on the cpu using ctransformers\n",
    "### - 27.4 seconds\n",
    "### output:\n",
    "<p>\n",
    "have an enormous impact on the future of the industry.\n",
    "This year, AI was all over the show floor and there are a number of companies that are using AI in their products. Here's a quick look at some of them:\n",
    "Fulcrum is an automated, artificial intelligence-powered software solution that helps manufacturers improve productivity and efficiency by automating processes, removing errors, and freeing up employees to focus on more complex tasks.\n",
    "Kapil Khera, co-founder and CEO of Fulcrum, told Inman: \"I don't know a single company that doesn't have AI in their product. I think it's the most important thing to come out of 2018.\"\n",
    "The company is currently working on machine learning tools and workflow applications with machine intelligence technology so you can really begin building these powerful AI-powered solutions. The full AI-focused ecosystem, with automation in product design and sophisticated analytics, will be available to all customers by the end of 2018.\n",
    "Kapil Khera, co-founder and CEO of Fulcrum\n",
    "Surely\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_response_tiny_llama(text):\n",
    "    path='/teamspace/studios/this_studio/TinyLlama-1.1B-Chat-v1.0-GPTQ'\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "    prompt = text\n",
    "    system_message = \"You are a story writing assistant\"\n",
    "    prompt_template=f'''<|system|>\n",
    "    {system_message}</s>\n",
    "    <|user|>\n",
    "    {prompt}</s>\n",
    "    <|assistant|>\n",
    "    '''\n",
    "\n",
    "    print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "    input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "    output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "    return(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_response_tiny_llama(\"what is qlora?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# result on gpu 2.8 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# handling the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_pdf_to_text(file_path):\n",
    "    import PyPDF2\n",
    "\n",
    "    read_pdf = PyPDF2.PdfReader(file_path)\n",
    "    pages = read_pdf.pages\n",
    "    res=''\n",
    "    for page in pages:\n",
    "        res+=page.extract_text()\n",
    "    return res\n",
    "pages=convert_from_pdf_to_text('/Users/matansharon/python/opensource_rag/pdf_files/A semantic loss for ontology classification.pdf')\n",
    "\n",
    "import os\n",
    "pdf_list=os.listdir(\"/Users/matansharon/python/opensource_rag/pdf_files\")\n",
    "text_list=os.listdir(\"/Users/matansharon/python/opensource_rag/text_files\")\n",
    "\n",
    "for pdf in pdf_list:\n",
    "    temp=pdf.replace(\".pdf\",\".txt\")\n",
    "    if temp not in text_list:\n",
    "        text=convert_from_pdf_to_text(\"pdf_files/\"+pdf)\n",
    "        with open(\"text_files/\"+temp, 'w') as f:\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_pdf_to_text(file_path):\n",
    "    import PyPDF2\n",
    "\n",
    "    read_pdf = PyPDF2.PdfReader(file_path)\n",
    "    pages = read_pdf.pages\n",
    "    res=''\n",
    "    for page in pages:\n",
    "        res+=page.extract_text()\n",
    "    return res\n",
    "pages=convert_from_pdf_to_text('/Users/matansharon/python/opensource_rag/pdf_files/A semantic loss for ontology classification.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client=chromadb.Client()\n",
    "import os\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "    api_key=os.environ.get(\"HuggingFace_API_KEY\"),\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "collection=client.get_or_create_collection('test',embedding_function=huggingface_ef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ids', 'embeddings', 'metadatas', 'documents', 'uris', 'data'])\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "persist_directory = \"chroma_storage\"\n",
    "client = chromadb.PersistentClient(path=persist_directory)\n",
    "collection=client.get_collection(\"file1_collection\")\n",
    "print(collection.get().keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['45', '46', '3', '25', '22']],\n",
       " 'distances': [[0.62945196401265,\n",
       "   1.2014087659918908,\n",
       "   1.4490561050844064,\n",
       "   1.457502030657284,\n",
       "   1.4734288925551329]],\n",
       " 'metadatas': [[{'filename': 'Component Separation method for CMB using Convolutional Neural Networks.pdf',\n",
       "    'page_number': 0},\n",
       "   {'filename': 'Component Separation method for CMB using Convolutional Neural Networks.pdf',\n",
       "    'page_number': 1},\n",
       "   {'filename': 'qlora.pdf', 'page_number': 3},\n",
       "   {'filename': 'qlora.pdf', 'page_number': 25},\n",
       "   {'filename': 'qlora.pdf', 'page_number': 22}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Component Separation method for CMB using Convolutional Neural Networks\\nA. Quintana1,2,3, B. Ruiz-Granados4, P. Ruiz-Lapuente1,2\\n1Instituto de F´ ısica Fundamental (IFF-CSIC), Madrid, Spain,\\n2Institut de Ci` encies del Cosmos (UB-IEEC), Barcelona, Spain\\n3Universidad Internacional de Valencia, Valencia, Spain\\n4Universidad de C´ ordoba, C´ ordoba, Spain\\nThe aim of this project is to recover the CMB anisotropies maps in temperature and polar-\\nized intensity by means of a deep convolutional neural network (CNN) which, after appropiate\\ntraining, can remove the foregrounds from Planck and QUIJOTE data. The results are then\\ncompared with those obtained by COMMANDER , based on Bayesian parametric component sep-\\naration. The CNN successfully recovered the CMB signal for both All Sky and Partial Sky\\nmaps showing frequency dependant results, being optimum for central frequencies where there\\nis less contamination by foregrounds emissions such as galactic synchrotron and thermal dust\\nemissions. Recovered maps in temperature are consistent with those obtained by Planck Col-\\nlaboration, while polarized intensity has been recovered as a new observable. The polarized\\nintensity maps recovered from QUIJOTE experiment are novel and of potential interest to the\\nscientific community for the detection of primordial gravitational waves. The way forward will\\nbe to recover the maps at higher NSIDE and make them available to the scientific community.\\n1 Introduction\\nPrevious works1have successfully demonstrated the use of Deep Learning techniques, such as\\nConvolutional Neural Networks (CNN), for the extraction of CMB temperature maps, in partic-\\nular with U-Net architecture2. In this work a U-Net CNN is trained from simulated observational\\nmaps, with the aim of recovering the CMB signal in temperature and polarized intensity from\\nPlanckaand QUIJOTEbobservational maps. In order to validate the methodology, the recov-\\nered CMB maps are compared with those obtained by Planck Collaboration with COMMANDER as\\na different Component Separation Method.\\n2 Methodology and Results\\nA CNN with U-Net architecture is developed and trained in order to be able to recover the CMB\\nsignal. To train the CNN, 803 mock maps are simulated at NSIDE 64 at different frequency\\nbands, both in temperature and polarized intensity for Planck and QUIJOTE maps, meaning\\nthat the CNN has to be trained four times. A different dataset of 209 mock maps are also\\ngenerated in order to validate the CNN, and finally 11 mock maps are generated for testing the\\nCNN. For each frequency, a different realization of the clean CMB signal generated with PySM is\\nprovided to the CNN. Once the CNN has been proved to be properly trained with the validation\\nand test dataset, it is directly applied to resolve the Planck and QUIJOTE maps.\\nahttps://www.cosmos.esa.int/web/planck\\nbhttps://www.iac.es/es/proyectos/experimento-quijote-cmbarXiv:2405.04564v1  [astro-ph.CO]  7 May 2024',\n",
       "   'Figure 1 – Left: Recovered CMB maps in temperature at frequencies 30, 44, 70, 100, 143, 217 and 353 GHz from\\nPlanck. Right: Recovered CMB maps in polarized intensity at frequencies 11, 13, 17 and 19 GHz from QUIJOTE.\\nFigure 2 – Comparison between CMB signal in temperature recovered from Planck (left) and in polarized intensity\\nfrom QUIJOTE (right). CMB recovered by the CNN (up), by COMMANDER (middle) and residuals (down) are shown.\\nIn Figure 1, it is observed that below 44 GHz and above 217 GHZ the CNN struggles\\nto recover the CMB signal in temperature since synchrotron and thermal dust foregrounds\\nemissions, respectively, are dominant. In Figure 2, on the left side it is seen that recovered\\nmap in temperature is consistent with the CMB obtained by Planck. On the right side, while\\ndeviations from results in polarized intensity with respect to Planck are apparent by eye, it can\\nbe proved that the map recovered by the CNN is significantly more Gaussian than Planck, which\\ncould be a hint of a better CMB signal; deep statistical analysis is yet to be performed though.\\n3 Conclusions\\nCMB anisotropies maps have been successfully recovered by a CNN for both All Sky and Partial\\nSky maps. Recovered maps are frequency dependant, with optimum results for central frequen-\\ncies. It is the first time that polarized intensity defined as a scalar IP=p\\nQ2+U2is recovered\\nas an observable. While all simulations have been run at NSIDE 64, the next step will be to\\nrepeat the procedure at NSIDE 512.\\nReferences\\n1. G.J. Wang, H.L. Shi, Y.P. Yan, J.Q. Xia, Y.Y. Zhao, S.Y. Li, and J.F. Li, Astrophysical\\nJournal Supplement Series 260(1) , 13 (2022)\\n2. O.Ronneberger, P. Fischer, and T.Brox, arxiv: 1505.04597\\n3. Planck Collaboration. Astronomy and Astrophysics 641, A1 (2020)\\n4. Rubi˜ no-Mart´ ın et al. Monthly Notices of the Royal Astronomical Society 519(3) , 3383\\n(2023)',\n",
       "   'Parameter Efficient Finetuning (PEFT) method, most of the memory footprint for LLM finetuning\\ncomes from activation gradients and not from the learned LoRA parameters. For a 7B LLaMA\\nmodel trained on FLAN v2 with a batch size of 1, with LoRA weights equivalent to commonly used\\n0.2% of the original model weights[ 28,37], the LoRA input gradients have a memory footprint\\nof 567 MB while the LoRA parameters take up only 26 MB. With gradient checkpointing [ 9], the\\ninput gradients reduce to an average of 18 MB per sequence making them more memory intensive\\nthan all LoRA weights combined. In comparison, the 4-bit base model consumes 5,048 MB of\\nmemory. This highlights that gradient checkpointing is important but also that aggressively reducing\\nthe amount of LoRA parameter yields only minor memory benefits. This means we can use more\\nadapters without significantly increasing the overall training memory footprint (see Appendix G\\nfor a detailed breakdown). As discussed later, this is crucial for recovering full 16-bit precision\\nperformance.\\n3 QL ORA Finetuning\\nQLORAachieves high-fidelity 4-bit finetuning via two techniques we propose—4-bit NormalFloat\\n(NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to\\nprevent memory spikes during gradient checkpointing from causing out-of-memory errors that have\\ntraditionally made finetuning on a single machine difficult for large models.\\nQLORAhas one low-precision storage data type, in our case usually 4-bit, and one computation data\\ntype that is usually BFloat16. In practice, this means whenever a QLORAweight tensor is used, we\\ndequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit.\\nWe now discuss the components of QL ORA followed by a formal definition of QL ORA.\\n4-bit NormalFloat Quantization The NormalFloat (NF) data type builds on Quantile Quantization\\n[15] which is an information-theoretically optimal data type that ensures each quantization bin has an\\nequal number of values assigned from the input tensor. Quantile quantization works by estimating\\nthe quantile of the input tensor through the empirical cumulative distribution function.\\nThe main limitation of quantile quantization is that the process of quantile estimation is expensive.\\nTherefore fast quantile approximation algorithms, such as SRAM quantiles [ 15], are used to estimate\\nthem. Due to the approximate nature of these quantile estimation algorithms, the data type has large\\nquantization errors for outliers, which are often the most important values.\\nExpensive quantile estimates and approximation errors can be avoided when input tensors come from\\na distribution fixed up to a quantization constant. In such cases, input tensors have the same quantiles\\nmaking exact quantile estimation computationally feasible.\\nSince pretrained neural network weights usually have a zero-centered normal distribution with\\nstandard deviation σ(see Appendix F), we can transform all weights to a single fixed distribution by\\nscaling σsuch that the distribution fits exactly into the range of our data type. For our data type, we\\nset the arbitrary range [−1,1]. As such, both the quantiles for the data type and the neural network\\nweights need to be normalized into this range.\\nThe information theoretically optimal data type for zero-mean normal distributions with arbitrary\\nstandard deviations σin the range [−1,1]is computed as follows: (1) estimate the 2k+ 1quantiles\\nof a theoretical N(0,1)distribution to obtain a k-bit quantile quantization data type for normal distri-\\nbutions, (2) take this data type and normalize its values into the [−1,1]range, (3) quantize an input\\nweight tensor by normalizing it into the [−1,1]range through absolute maximum rescaling.\\nOnce the weight range and data type range match, we can quantize as usual. Step (3) is equivalent to\\nrescaling the standard deviation of the weight tensor to match the standard deviation of the k-bit data\\ntype. More formally, we estimate the 2kvalues qiof the data type as follows:\\nqi=1\\n2\\x12\\nQX\\x12i\\n2k+ 1\\x13\\n+QX\\x12i+ 1\\n2k+ 1\\x13\\x13\\n, (4)\\nwhere QX(·)is the quantile function of the standard normal distribution N(0,1). A problem for\\na symmetric k-bit quantization is that this approach does not have an exact representation of zero,\\nwhich is an important property to quantize padding and other zero-valued elements with no error. To\\n4',\n",
       "   'LLaMA model size0%25%50%75%100%\\n7B (6.9 GB) 13B (11.3 GB) 33B (24.7 GB) 65B (45.0 GB)Input gradient Optimizer Weight gradient Adapters ModelFigure 6: Breakdown of the memory footprint of different LLaMA models. The input gradient size is for batch\\nsize 1 and sequence length 512 and is estimated only for adapters and the base model weights (no attention).\\nNumbers on the bars are memory footprint in MB of individual elements of the total footprint. While some\\nmodels do not quite fit on certain GPUs, paged optimzier provide enough memory to make these models fit.\\nG Memory Footprint\\nThe memory footpring for QLoRA training with different LLaMA base models can be seen in\\nFigure 6. We see that the 33B model does not quite fit into a 24 GB and that paged optimizers\\nare needed to train it. Depicted is also batch size 1 with a sequence length of 512 and gradient\\ncheckpointning. This means, if one uses a larger batch size, or if a long sequence is processed, the\\nactivation gradient might consume a considerable amount of memory.\\nTable 13: The complete ordering induced by pairwise GPT-4 judgments between systems\\nModel Params Size\\nGuanaco 65B 41 GB\\nGuanaco 33B 21 GB\\nVicuna 13B 26 GB\\nChatGPT-3.5 Turbo N/A N/A\\nBard N/A N/A\\nGuanaco 13B 10 GB\\nGuanaco 7B 5 GB\\n26',\n",
       "   'Parameters Dataset Batch size LR Steps Source Length Target Length\\n7B All 16 2e-4 10000 384 128\\n7B OASST1 16 2e-4 1875 - 512\\n7B HH-RLHF 16 2e-4 10000 - 768\\n7B Longform 16 2e-4 4000 512 1024\\n13B All 16 2e-4 10000 384 128\\n13B OASST1 16 2e-4 1875 - 512\\n13B HH-RLHF 16 2e-4 10000 - 768\\n13B Longform 16 2e-4 4000 512 1024\\n33B All 32 1e-4 5000 384 128\\n33B OASST1 16 1e-4 1875 - 512\\n33B HH-RLHF 32 1e-4 5000 - 768\\n33B Longform 32 1e-4 2343 512 1024\\n65B All 64 1e-4 2500 384 128\\n65B OASST1 16 1e-4 1875 - 512\\n65B HH-RLHF 64 1e-4 2500 - 768\\n65B Longform 32 1e-4 2343 512 1024\\nTable 9: Training hyperparameters for QL ORA finetuning on different datasets and across model sizes.\\nSelf-Instruct, Alpaca, Unnatural Instructions The Self-Instruct, Alpaca, and Unnatural Instruc-\\ntions datasets [ 59,55,26] are instruction tuning datasets collected with various approaches of model\\ndistillation from GPT-3 Instruct and ChatGPT. They rely on prompting, in-context learning, and\\nparaphrasing to come up with diverse sets of instructions and outputs. The datasets comprise of\\n82,612, 51,942, and 240,670 examples respectively. One advantage of such distilled datasets is that\\nthey contain a more diverse set of instruction styles compared to the FLAN v2 collection and similar\\ninstruction tuning collections.\\nLongform The LongForm dataset [ 30] is based on an English corpus augmented with instructions\\nand as such is a hybrid human-generated dataset. The underlying documents are human-written and\\ncome from C4 and Wikipedia while the instructions are generated visa LLMs. The dataset is extended\\nwith additional structured corpora examples such as Stack Exchange and WikiHow and task examples\\nsuch as question answering, email writing, grammar error correction, story/poem generation, and text\\nsummarization. The dataset contains 23,700 examples.\\nChip2 is part of the OIG Laion dataset. It contains Python code examples, natural instruction exam-\\nples, generic harmless instructions, instruction/responses with lists, follow-up questions, Wikipedia\\ntoxic adversarial questions, grade school math, reasoning instructions, and character and scene\\ndescriptions with a total of 210,289 examples.\\nB.2 Hyperparameters\\nWe provide the exact hyperparameters used in our QLORAfinetuning experiments. We find hyper-\\nparameters to be largely robust across datasets. We use the MMLU 5-shot dev set for validation\\nand hyperparameter tuning. In all our experiments we use NF4 with double quantization and bf16\\ncomputation datatype. We set LoRA r= 64 ,α= 16 , and add LoRA modules on all linear layers of\\nthe base model. We also use Adam beta2 of 0.999, max grad norm of 0.3 and LoRA dropout of 0.1\\nfor models up to 13B and 0.05 for 33B and 65B models. Following previous work on instruction\\nfinetuning [ 62,60] and after benchmarking other linear and cosine schedules, we use a constant\\nlearning rate schedule. We use group-by-length to group examples of similar lengths in the same\\nbatch (note this will produce a oscillating loss curve). The hyperparameters we tune for each model\\nsize are shown in Table 9.\\nB.3 Ablations\\nWhile it is general practice in the literature to only train on the response in instruction following\\ndatasets, we study the effect of training on the instruction in addition to the response in Table 10. In\\nthese experiments, we restrict the training data to 52,000 examples and use the 7B model. Over four\\ndifferent instruction tuning datasets, we find that only training on the target is beneficial to MMLU\\n23']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['45', '46', '3', '25', '22']], 'distances': [[0.62945196401265, 1.2014087659918908, 1.4490561050844064, 1.457502030657284, 1.4734288925551329]], 'metadatas': [[{'filename': 'Component Separation method for CMB using Convolutional Neural Networks.pdf', 'page_number': 0}, {'filename': 'Component Separation method for CMB using Convolutional Neural Networks.pdf', 'page_number': 1}, {'filename': 'qlora.pdf', 'page_number': 3}, {'filename': 'qlora.pdf', 'page_number': 25}, {'filename': 'qlora.pdf', 'page_number': 22}]], 'embeddings': None, 'documents': [['Component Separation method for CMB using Convolutional Neural Networks\\nA. Quintana1,2,3, B. Ruiz-Granados4, P. Ruiz-Lapuente1,2\\n1Instituto de F´ ısica Fundamental (IFF-CSIC), Madrid, Spain,\\n2Institut de Ci` encies del Cosmos (UB-IEEC), Barcelona, Spain\\n3Universidad Internacional de Valencia, Valencia, Spain\\n4Universidad de C´ ordoba, C´ ordoba, Spain\\nThe aim of this project is to recover the CMB anisotropies maps in temperature and polar-\\nized intensity by means of a deep convolutional neural network (CNN) which, after appropiate\\ntraining, can remove the foregrounds from Planck and QUIJOTE data. The results are then\\ncompared with those obtained by COMMANDER , based on Bayesian parametric component sep-\\naration. The CNN successfully recovered the CMB signal for both All Sky and Partial Sky\\nmaps showing frequency dependant results, being optimum for central frequencies where there\\nis less contamination by foregrounds emissions such as galactic synchrotron and thermal dust\\nemissions. Recovered maps in temperature are consistent with those obtained by Planck Col-\\nlaboration, while polarized intensity has been recovered as a new observable. The polarized\\nintensity maps recovered from QUIJOTE experiment are novel and of potential interest to the\\nscientific community for the detection of primordial gravitational waves. The way forward will\\nbe to recover the maps at higher NSIDE and make them available to the scientific community.\\n1 Introduction\\nPrevious works1have successfully demonstrated the use of Deep Learning techniques, such as\\nConvolutional Neural Networks (CNN), for the extraction of CMB temperature maps, in partic-\\nular with U-Net architecture2. In this work a U-Net CNN is trained from simulated observational\\nmaps, with the aim of recovering the CMB signal in temperature and polarized intensity from\\nPlanckaand QUIJOTEbobservational maps. In order to validate the methodology, the recov-\\nered CMB maps are compared with those obtained by Planck Collaboration with COMMANDER as\\na different Component Separation Method.\\n2 Methodology and Results\\nA CNN with U-Net architecture is developed and trained in order to be able to recover the CMB\\nsignal. To train the CNN, 803 mock maps are simulated at NSIDE 64 at different frequency\\nbands, both in temperature and polarized intensity for Planck and QUIJOTE maps, meaning\\nthat the CNN has to be trained four times. A different dataset of 209 mock maps are also\\ngenerated in order to validate the CNN, and finally 11 mock maps are generated for testing the\\nCNN. For each frequency, a different realization of the clean CMB signal generated with PySM is\\nprovided to the CNN. Once the CNN has been proved to be properly trained with the validation\\nand test dataset, it is directly applied to resolve the Planck and QUIJOTE maps.\\nahttps://www.cosmos.esa.int/web/planck\\nbhttps://www.iac.es/es/proyectos/experimento-quijote-cmbarXiv:2405.04564v1  [astro-ph.CO]  7 May 2024', 'Figure 1 – Left: Recovered CMB maps in temperature at frequencies 30, 44, 70, 100, 143, 217 and 353 GHz from\\nPlanck. Right: Recovered CMB maps in polarized intensity at frequencies 11, 13, 17 and 19 GHz from QUIJOTE.\\nFigure 2 – Comparison between CMB signal in temperature recovered from Planck (left) and in polarized intensity\\nfrom QUIJOTE (right). CMB recovered by the CNN (up), by COMMANDER (middle) and residuals (down) are shown.\\nIn Figure 1, it is observed that below 44 GHz and above 217 GHZ the CNN struggles\\nto recover the CMB signal in temperature since synchrotron and thermal dust foregrounds\\nemissions, respectively, are dominant. In Figure 2, on the left side it is seen that recovered\\nmap in temperature is consistent with the CMB obtained by Planck. On the right side, while\\ndeviations from results in polarized intensity with respect to Planck are apparent by eye, it can\\nbe proved that the map recovered by the CNN is significantly more Gaussian than Planck, which\\ncould be a hint of a better CMB signal; deep statistical analysis is yet to be performed though.\\n3 Conclusions\\nCMB anisotropies maps have been successfully recovered by a CNN for both All Sky and Partial\\nSky maps. Recovered maps are frequency dependant, with optimum results for central frequen-\\ncies. It is the first time that polarized intensity defined as a scalar IP=p\\nQ2+U2is recovered\\nas an observable. While all simulations have been run at NSIDE 64, the next step will be to\\nrepeat the procedure at NSIDE 512.\\nReferences\\n1. G.J. Wang, H.L. Shi, Y.P. Yan, J.Q. Xia, Y.Y. Zhao, S.Y. Li, and J.F. Li, Astrophysical\\nJournal Supplement Series 260(1) , 13 (2022)\\n2. O.Ronneberger, P. Fischer, and T.Brox, arxiv: 1505.04597\\n3. Planck Collaboration. Astronomy and Astrophysics 641, A1 (2020)\\n4. Rubi˜ no-Mart´ ın et al. Monthly Notices of the Royal Astronomical Society 519(3) , 3383\\n(2023)', 'Parameter Efficient Finetuning (PEFT) method, most of the memory footprint for LLM finetuning\\ncomes from activation gradients and not from the learned LoRA parameters. For a 7B LLaMA\\nmodel trained on FLAN v2 with a batch size of 1, with LoRA weights equivalent to commonly used\\n0.2% of the original model weights[ 28,37], the LoRA input gradients have a memory footprint\\nof 567 MB while the LoRA parameters take up only 26 MB. With gradient checkpointing [ 9], the\\ninput gradients reduce to an average of 18 MB per sequence making them more memory intensive\\nthan all LoRA weights combined. In comparison, the 4-bit base model consumes 5,048 MB of\\nmemory. This highlights that gradient checkpointing is important but also that aggressively reducing\\nthe amount of LoRA parameter yields only minor memory benefits. This means we can use more\\nadapters without significantly increasing the overall training memory footprint (see Appendix G\\nfor a detailed breakdown). As discussed later, this is crucial for recovering full 16-bit precision\\nperformance.\\n3 QL ORA Finetuning\\nQLORAachieves high-fidelity 4-bit finetuning via two techniques we propose—4-bit NormalFloat\\n(NF4) quantization and Double Quantization. Additionally, we introduce Paged Optimizers, to\\nprevent memory spikes during gradient checkpointing from causing out-of-memory errors that have\\ntraditionally made finetuning on a single machine difficult for large models.\\nQLORAhas one low-precision storage data type, in our case usually 4-bit, and one computation data\\ntype that is usually BFloat16. In practice, this means whenever a QLORAweight tensor is used, we\\ndequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit.\\nWe now discuss the components of QL ORA followed by a formal definition of QL ORA.\\n4-bit NormalFloat Quantization The NormalFloat (NF) data type builds on Quantile Quantization\\n[15] which is an information-theoretically optimal data type that ensures each quantization bin has an\\nequal number of values assigned from the input tensor. Quantile quantization works by estimating\\nthe quantile of the input tensor through the empirical cumulative distribution function.\\nThe main limitation of quantile quantization is that the process of quantile estimation is expensive.\\nTherefore fast quantile approximation algorithms, such as SRAM quantiles [ 15], are used to estimate\\nthem. Due to the approximate nature of these quantile estimation algorithms, the data type has large\\nquantization errors for outliers, which are often the most important values.\\nExpensive quantile estimates and approximation errors can be avoided when input tensors come from\\na distribution fixed up to a quantization constant. In such cases, input tensors have the same quantiles\\nmaking exact quantile estimation computationally feasible.\\nSince pretrained neural network weights usually have a zero-centered normal distribution with\\nstandard deviation σ(see Appendix F), we can transform all weights to a single fixed distribution by\\nscaling σsuch that the distribution fits exactly into the range of our data type. For our data type, we\\nset the arbitrary range [−1,1]. As such, both the quantiles for the data type and the neural network\\nweights need to be normalized into this range.\\nThe information theoretically optimal data type for zero-mean normal distributions with arbitrary\\nstandard deviations σin the range [−1,1]is computed as follows: (1) estimate the 2k+ 1quantiles\\nof a theoretical N(0,1)distribution to obtain a k-bit quantile quantization data type for normal distri-\\nbutions, (2) take this data type and normalize its values into the [−1,1]range, (3) quantize an input\\nweight tensor by normalizing it into the [−1,1]range through absolute maximum rescaling.\\nOnce the weight range and data type range match, we can quantize as usual. Step (3) is equivalent to\\nrescaling the standard deviation of the weight tensor to match the standard deviation of the k-bit data\\ntype. More formally, we estimate the 2kvalues qiof the data type as follows:\\nqi=1\\n2\\x12\\nQX\\x12i\\n2k+ 1\\x13\\n+QX\\x12i+ 1\\n2k+ 1\\x13\\x13\\n, (4)\\nwhere QX(·)is the quantile function of the standard normal distribution N(0,1). A problem for\\na symmetric k-bit quantization is that this approach does not have an exact representation of zero,\\nwhich is an important property to quantize padding and other zero-valued elements with no error. To\\n4', 'LLaMA model size0%25%50%75%100%\\n7B (6.9 GB) 13B (11.3 GB) 33B (24.7 GB) 65B (45.0 GB)Input gradient Optimizer Weight gradient Adapters ModelFigure 6: Breakdown of the memory footprint of different LLaMA models. The input gradient size is for batch\\nsize 1 and sequence length 512 and is estimated only for adapters and the base model weights (no attention).\\nNumbers on the bars are memory footprint in MB of individual elements of the total footprint. While some\\nmodels do not quite fit on certain GPUs, paged optimzier provide enough memory to make these models fit.\\nG Memory Footprint\\nThe memory footpring for QLoRA training with different LLaMA base models can be seen in\\nFigure 6. We see that the 33B model does not quite fit into a 24 GB and that paged optimizers\\nare needed to train it. Depicted is also batch size 1 with a sequence length of 512 and gradient\\ncheckpointning. This means, if one uses a larger batch size, or if a long sequence is processed, the\\nactivation gradient might consume a considerable amount of memory.\\nTable 13: The complete ordering induced by pairwise GPT-4 judgments between systems\\nModel Params Size\\nGuanaco 65B 41 GB\\nGuanaco 33B 21 GB\\nVicuna 13B 26 GB\\nChatGPT-3.5 Turbo N/A N/A\\nBard N/A N/A\\nGuanaco 13B 10 GB\\nGuanaco 7B 5 GB\\n26', 'Parameters Dataset Batch size LR Steps Source Length Target Length\\n7B All 16 2e-4 10000 384 128\\n7B OASST1 16 2e-4 1875 - 512\\n7B HH-RLHF 16 2e-4 10000 - 768\\n7B Longform 16 2e-4 4000 512 1024\\n13B All 16 2e-4 10000 384 128\\n13B OASST1 16 2e-4 1875 - 512\\n13B HH-RLHF 16 2e-4 10000 - 768\\n13B Longform 16 2e-4 4000 512 1024\\n33B All 32 1e-4 5000 384 128\\n33B OASST1 16 1e-4 1875 - 512\\n33B HH-RLHF 32 1e-4 5000 - 768\\n33B Longform 32 1e-4 2343 512 1024\\n65B All 64 1e-4 2500 384 128\\n65B OASST1 16 1e-4 1875 - 512\\n65B HH-RLHF 64 1e-4 2500 - 768\\n65B Longform 32 1e-4 2343 512 1024\\nTable 9: Training hyperparameters for QL ORA finetuning on different datasets and across model sizes.\\nSelf-Instruct, Alpaca, Unnatural Instructions The Self-Instruct, Alpaca, and Unnatural Instruc-\\ntions datasets [ 59,55,26] are instruction tuning datasets collected with various approaches of model\\ndistillation from GPT-3 Instruct and ChatGPT. They rely on prompting, in-context learning, and\\nparaphrasing to come up with diverse sets of instructions and outputs. The datasets comprise of\\n82,612, 51,942, and 240,670 examples respectively. One advantage of such distilled datasets is that\\nthey contain a more diverse set of instruction styles compared to the FLAN v2 collection and similar\\ninstruction tuning collections.\\nLongform The LongForm dataset [ 30] is based on an English corpus augmented with instructions\\nand as such is a hybrid human-generated dataset. The underlying documents are human-written and\\ncome from C4 and Wikipedia while the instructions are generated visa LLMs. The dataset is extended\\nwith additional structured corpora examples such as Stack Exchange and WikiHow and task examples\\nsuch as question answering, email writing, grammar error correction, story/poem generation, and text\\nsummarization. The dataset contains 23,700 examples.\\nChip2 is part of the OIG Laion dataset. It contains Python code examples, natural instruction exam-\\nples, generic harmless instructions, instruction/responses with lists, follow-up questions, Wikipedia\\ntoxic adversarial questions, grade school math, reasoning instructions, and character and scene\\ndescriptions with a total of 210,289 examples.\\nB.2 Hyperparameters\\nWe provide the exact hyperparameters used in our QLORAfinetuning experiments. We find hyper-\\nparameters to be largely robust across datasets. We use the MMLU 5-shot dev set for validation\\nand hyperparameter tuning. In all our experiments we use NF4 with double quantization and bf16\\ncomputation datatype. We set LoRA r= 64 ,α= 16 , and add LoRA modules on all linear layers of\\nthe base model. We also use Adam beta2 of 0.999, max grad norm of 0.3 and LoRA dropout of 0.1\\nfor models up to 13B and 0.05 for 33B and 65B models. Following previous work on instruction\\nfinetuning [ 62,60] and after benchmarking other linear and cosine schedules, we use a constant\\nlearning rate schedule. We use group-by-length to group examples of similar lengths in the same\\nbatch (note this will produce a oscillating loss curve). The hyperparameters we tune for each model\\nsize are shown in Table 9.\\nB.3 Ablations\\nWhile it is general practice in the literature to only train on the response in instruction following\\ndatasets, we study the effect of training on the instruction in addition to the response in Table 10. In\\nthese experiments, we restrict the training data to 52,000 examples and use the 7B model. Over four\\ndifferent instruction tuning datasets, we find that only training on the target is beneficial to MMLU\\n23']], 'uris': None, 'data': None}\n",
      "0.62945196401265\n",
      "Component Separation method for CMB using Convolutional Neural Networks\n",
      "A. Quintana1,2,3, B. Ruiz-Gr\n",
      "{'filename': 'Component Separation method for CMB using Convolutional Neural Networks.pdf', 'page_number': 0}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1.2014087659918908\n",
      "Figure 1 – Left: Recovered CMB maps in temperature at frequencies 30, 44, 70, 100, 143, 217 and 353 \n",
      "{'filename': 'Component Separation method for CMB using Convolutional Neural Networks.pdf', 'page_number': 1}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(\n",
    "            query_texts=[\"summerize the Component Separation method for CMB using Convolutional Neural Networks document\"], n_results=5, include=[\"documents\", \"metadatas\",\"distances\"])\n",
    "print(results)\n",
    "for i in range(len(results['distances'][0])):\n",
    "    if (abs(results['distances'][0][i]-1)<0.4):\n",
    "        print(results['distances'][0][i])\n",
    "        print(results['documents'][0][i][:100])\n",
    "        print(results['metadatas'][0][i])\n",
    "        print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell is a testing cell for filtering the results of the query by the distance\n",
    "\n",
    "temp_results={\"ids\":[[]],\"distances\":[[]],\"metadatas\":[[]],\"documents\":[[]]}\n",
    "        for i in range(5):\n",
    "            if abs(1-results[\"distances\"][0][i]<0.4): \n",
    "                temp_results[\"ids\"][0].append(results[\"ids\"][0][i])\n",
    "                temp_results[\"distances\"][0].append(results[\"distances\"][0][i])\n",
    "                temp_results[\"metadatas\"][0].append(results[\"metadatas\"][0][i])\n",
    "                temp_results[\"documents\"].append(results[\"documents\"][0][i])\n",
    "        results=temp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file_names=set()\n",
    "for line in collection.get()['metadatas']:\n",
    "    all_file_names.add(line['filename'])\n",
    "all_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in collection.get()['documents']:\n",
    "    print(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "pages=PyPDF2.PdfReader(\"/Users/matansharon/python/opensource_rag/pdf_files/qlora.pdf\").pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510\n",
      "854\n",
      "478\n",
      "777\n",
      "720\n",
      "739\n",
      "668\n",
      "725\n",
      "968\n",
      "821\n",
      "617\n",
      "570\n",
      "528\n",
      "788\n",
      "742\n",
      "616\n",
      "787\n",
      "759\n",
      "772\n",
      "832\n",
      "89\n",
      "572\n",
      "626\n",
      "566\n",
      "265\n",
      "254\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "for page in pages:\n",
    "    print(len(word_tokenize(page.extract_text())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
