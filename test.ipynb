{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #legacy code from the original chatbot\n",
    "\n",
    "# def get_chatGPT_response(query: str, context: List[str]) -> str:\n",
    "#     \"\"\"\n",
    "#     Queries the GPT API to get a response to the question.\n",
    "\n",
    "#     Args:\n",
    "#     query (str): The original query.\n",
    "#     context (List[str]): The context of the query, returned by embedding search.\n",
    "\n",
    "#     Returns:\n",
    "#     A response to the question.\n",
    "#     \"\"\"\n",
    "#     response = openai.chat.completions.create(\n",
    "#         model=\"gpt-3.5-turbo\",\n",
    "#         messages=build_prompt(query, context),\n",
    "#     )\n",
    "\n",
    "#     return response.choices[0].message.content  # type: ignore\n",
    "\n",
    "\n",
    "# def build_prompt(query: str, context: List[str]) -> List[ChatCompletionMessageParam]:\n",
    "#     \"\"\"\n",
    "#     Builds a prompt for the LLM. #\n",
    "\n",
    "#     This function builds a prompt for the LLM. It takes the original query,\n",
    "#     and the returned context, and asks the model to answer the question based only\n",
    "#     on what's in the context, not what's in its weights.\n",
    "\n",
    "#     More information: https://platform.openai.com/docs/guides/chat/introduction\n",
    "\n",
    "#     Args:\n",
    "#     query (str): The original query.\n",
    "#     context (List[str]): The context of the query, returned by embedding search.\n",
    "\n",
    "#     Returns:\n",
    "#     A prompt for the LLM (List[ChatCompletionMessageParam]).\n",
    "#     \"\"\"\n",
    "\n",
    "#     system: ChatCompletionMessageParam = {\n",
    "#         \"role\": \"system\",\n",
    "#         \"content\": \"I am going to ask you a question, which I would like you to answer\"\n",
    "#         \"based only on the provided context, and not any other information.\"\n",
    "#         \"If there is not enough information in the context to answer the question,\"\n",
    "#         'say \"I am not sure\", then try to make a guess.'\n",
    "#         \"Break your answer up into nicely readable paragraphs.\",\n",
    "#     }\n",
    "#     user: ChatCompletionMessageParam = {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": f\"The question is {query}. Here is all the context you have:\"\n",
    "#         f'{(\" \").join(context)}',\n",
    "#     }\n",
    "\n",
    "#     return [system, user]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in llm(\"AI is going to\", stream=True):\n",
    "    print(text, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results on the cpu using ctransformers\n",
    "### - 27.4 seconds\n",
    "### output:\n",
    "<p>\n",
    "have an enormous impact on the future of the industry.\n",
    "This year, AI was all over the show floor and there are a number of companies that are using AI in their products. Here's a quick look at some of them:\n",
    "Fulcrum is an automated, artificial intelligence-powered software solution that helps manufacturers improve productivity and efficiency by automating processes, removing errors, and freeing up employees to focus on more complex tasks.\n",
    "Kapil Khera, co-founder and CEO of Fulcrum, told Inman: \"I don't know a single company that doesn't have AI in their product. I think it's the most important thing to come out of 2018.\"\n",
    "The company is currently working on machine learning tools and workflow applications with machine intelligence technology so you can really begin building these powerful AI-powered solutions. The full AI-focused ecosystem, with automation in product design and sophisticated analytics, will be available to all customers by the end of 2018.\n",
    "Kapil Khera, co-founder and CEO of Fulcrum\n",
    "Surely\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_response_tiny_llama(text):\n",
    "    path='/teamspace/studios/this_studio/TinyLlama-1.1B-Chat-v1.0-GPTQ'\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "    prompt = text\n",
    "    system_message = \"You are a story writing assistant\"\n",
    "    prompt_template=f'''<|system|>\n",
    "    {system_message}</s>\n",
    "    <|user|>\n",
    "    {prompt}</s>\n",
    "    <|assistant|>\n",
    "    '''\n",
    "\n",
    "    print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "    input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "    output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "    return(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_response_tiny_llama(\"what is qlora?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# result on gpu 2.8 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# handling the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_pdf_to_text(file_path):\n",
    "    import PyPDF2\n",
    "\n",
    "    read_pdf = PyPDF2.PdfReader(file_path)\n",
    "    pages = read_pdf.pages\n",
    "    res=''\n",
    "    for page in pages:\n",
    "        res+=page.extract_text()\n",
    "    return res\n",
    "pages=convert_from_pdf_to_text('/Users/matansharon/python/opensource_rag/pdf_files/A semantic loss for ontology classification.pdf')\n",
    "\n",
    "import os\n",
    "pdf_list=os.listdir(\"/Users/matansharon/python/opensource_rag/pdf_files\")\n",
    "text_list=os.listdir(\"/Users/matansharon/python/opensource_rag/text_files\")\n",
    "\n",
    "for pdf in pdf_list:\n",
    "    temp=pdf.replace(\".pdf\",\".txt\")\n",
    "    if temp not in text_list:\n",
    "        text=convert_from_pdf_to_text(\"pdf_files/\"+pdf)\n",
    "        with open(\"text_files/\"+temp, 'w') as f:\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_pdf_to_text(file_path):\n",
    "    import PyPDF2\n",
    "\n",
    "    read_pdf = PyPDF2.PdfReader(file_path)\n",
    "    pages = read_pdf.pages\n",
    "    res=''\n",
    "    for page in pages:\n",
    "        res+=page.extract_text()\n",
    "    return res\n",
    "pages=convert_from_pdf_to_text('/Users/matansharon/python/opensource_rag/pdf_files/A semantic loss for ontology classification.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client=chromadb.Client()\n",
    "import os\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "    api_key=os.environ.get(\"HuggingFace_API_KEY\"),\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "collection=client.get_or_create_collection('test',embedding_function=huggingface_ef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "persist_directory = \"chroma_storage\"\n",
    "client = chromadb.PersistentClient(path=persist_directory)\n",
    "collection=client.get_collection(\"file1_collection\")\n",
    "print(collection.get().keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "            query_texts=[\"summerize the Component Separation method for CMB using Convolutional Neural Networks document\"], n_results=5, include=[\"documents\", \"metadatas\",\"distances\"])\n",
    "print(results)\n",
    "for i in range(len(results['distances'][0])):\n",
    "    if (abs(results['distances'][0][i]-1)<0.4):\n",
    "        print(results['distances'][0][i])\n",
    "        print(results['documents'][0][i][:100])\n",
    "        print(results['metadatas'][0][i])\n",
    "        print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell is a testing cell for filtering the results of the query by the distance\n",
    "\n",
    "temp_results={\"ids\":[[]],\"distances\":[[]],\"metadatas\":[[]],\"documents\":[[]]}\n",
    "        for i in range(5):\n",
    "            if abs(1-results[\"distances\"][0][i]<0.4): \n",
    "                temp_results[\"ids\"][0].append(results[\"ids\"][0][i])\n",
    "                temp_results[\"distances\"][0].append(results[\"distances\"][0][i])\n",
    "                temp_results[\"metadatas\"][0].append(results[\"metadatas\"][0][i])\n",
    "                temp_results[\"documents\"].append(results[\"documents\"][0][i])\n",
    "        results=temp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_file_names=set()\n",
    "for line in collection.get()['metadatas']:\n",
    "    all_file_names.add(line['filename'])\n",
    "all_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in collection.get()['documents']:\n",
    "    print(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "pages=PyPDF2.PdfReader(\"/Users/matansharon/python/opensource_rag/pdf_files/qlora.pdf\").pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import word_tokenize\n",
    "# for page in pages:\n",
    "#     print(len(word_tokenize(page.extract_text())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "df=pd.read_csv(\"salaries.csv\")\n",
    "\n",
    "df_str=df.to_string().strip()\n",
    "len(word_tokenize(df_str))\n",
    "with open(\"salaries2.txt\", 'w') as f:\n",
    "    f.write(df_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows=(df_str.split(\"\\n\"))\n",
    "col_rows_names=all_rows[0]\n",
    "# print(col_rows_names.strip())\n",
    "for line in all_rows:\n",
    "    print((line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client=anthropic.Anthropic(\n",
    "        api_key=os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    )\n",
    "message = client.messages.create(\n",
    "        # model=\"claude-3-opus-20240229\",\n",
    "        model='claude-3-haiku-20240307',\n",
    "        max_tokens=4000,\n",
    "        temperature=0.2  ,\n",
    "        system=f\"\"\"\n",
    "        You are a data analyst assistant.\n",
    "        your given a dataframe that was converted into string with salaries of employees,\n",
    "        return the head of the dataframe.\n",
    "        \n",
    "        \"\"\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": df_str}\n",
    "        ]\n",
    "    )\n",
    "print(message.usage)\n",
    "print( message.content[0].text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message():\n",
    "    def __init__(self):\n",
    "        self.content=''\n",
    "        self.type=''\n",
    "    def get_content(self):\n",
    "        return self.content\n",
    "    def get_type(self):\n",
    "        return self.type\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming the dataframe is named 'df'\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df1000['salary'], bins=20)\n",
    "plt.xlabel('Salary')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Salary Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "# nltk.download('punkt')\n",
    "\n",
    "files=os.listdir(\"/Users/matansharon/python/opensource_rag/documents\")\n",
    "files\n",
    "for file in files:\n",
    "    if file.endswith(\".pdf\"):\n",
    "        print(file)\n",
    "        pages=convert_from_pdf_to_text(\"/Users/matansharon/python/opensource_rag/documents/\"+file)\n",
    "        print(len(word_tokenize(pages)))\n",
    "    elif file.endswith(\".csv\"):\n",
    "        df=pd.read_csv(\"/Users/matansharon/python/opensource_rag/documents/\"+file)\n",
    "        df_str=df.to_string().strip()\n",
    "        print(df_str)\n",
    "        # print(len(word_tokenize(df_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_convert_csv(file_path: str) -> str:\n",
    "    df=pd.read_csv(file_path)\n",
    "    # Convert the dataframe to a string\n",
    "    df=df.to_string().strip()\n",
    "    column_row=df.split(\"\\n\")[0]\n",
    "    #split the string into a list of strings\n",
    "    rows=df.split(\"\\n\")\n",
    "    pages=[]\n",
    "    page=[]\n",
    "    for i in range(1,len(rows),10):\n",
    "        page.append(column_row)\n",
    "        for j in range(i,i+10):\n",
    "            if j<len(rows):\n",
    "            \n",
    "                page.append(rows[j])\n",
    "        pages.append(page)\n",
    "        page=[]\n",
    "    return pages\n",
    "res=read_and_convert_csv(\"/Users/matansharon/python/opensource_rag/documents/salaries.csv\")\n",
    "for page in res:\n",
    "    for row in page:\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splited=df_str.split('\\n')\n",
    "for row in splited:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "anthropic_client=anthropic.Anthropic(\n",
    "        api_key=os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    )\n",
    "context=convert_from_pdf_to_text('/Users/matansharon/python/opensource_rag/documents/Programming_PyTorch_for_Deep_Learning_Creating_and_Deploying_Deep_Learning_Applications_by_Ian_Pointer_z-lib_org.pdf')\n",
    "query='summerize this book'\n",
    "response = anthropic_client.messages.create(\n",
    "        # model=\"claude-3-opus-20240229\",\n",
    "        model='claude-3-haiku-20240307',\n",
    "        max_tokens=1000,\n",
    "        temperature=0.2,\n",
    "        system=f\"\"\"\n",
    "        I am going to ask you a question, which I would like you to answer\"\n",
    "        \"based only on the provided context, and not any other information.\"\n",
    "        \"If there is not enough information in the context to answer the question,\"\n",
    "        'say \"I am not sure\", then try to make a guess.'\n",
    "        \"Break your answer up into nicely readable paragraphs.\n",
    "        here is all the context you have:{context}\"\n",
    "        \"\"\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "110840/80055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "GOOGLE_API_KEY=os.environ.get('GOOGLE_API_KEY')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-pro')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = model.generate_content(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "db_client = chromadb.PersistentClient(path='chroma_storage_pages')\n",
    "collection=db_client.get_collection('by_pages_collection')\n",
    "results = collection.query(\n",
    "            query_texts=['what is qlora?'], n_results=3, include=[\"documents\", \"metadatas\",\"distances\"],\n",
    "        )\n",
    "        \n",
    "    \n",
    "context=results[\"documents\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "reader=PyPDF2.PdfReader(\"/Users/matansharon/python/opensource_rag/pdf_files/Non-linear Welfare-Aware Strategic Learning.pdf\")\n",
    "pages=reader.pages\n",
    "text=\"\"\n",
    "for page in pages:\n",
    "    text+=(page.extract_text())\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "len(word_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "GOOGLE_API_KEY=os.environ.get('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "\n",
    "response = model.generate_content(f\"summerize the following: {text}\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "path=\"/Users/matansharon/python/opensource_rag/נתוני חברים פברואר - אפריל 2024.xlsx\"\n",
    "df=pd.read_excel(path,sheet_name='גיליון3')\n",
    "# print(df.to_string())\n",
    "print(\"length of tokens of sheet 3: \",len(word_tokenize(df.to_string()))*1.3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hours.txt\", 'w') as f:\n",
    "    f.write(df.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "client=anthropic.Anthropic(\n",
    "    api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "message = client.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        # model='claude-3-haiku-20240307',\n",
    "        max_tokens=4000,\n",
    "        temperature=0.2,\n",
    "        system=f\"\"\"\n",
    "        You are a data analyst assistant.\n",
    "        your given a dataframe that was converted into string with employees work hours,\n",
    "        follow the instructions:\n",
    "        1. Clean and format the data: Convert the string-based data into a structured format (e.g., a pandas DataFrame) to facilitate data manipulation and analysis.\n",
    "        2. Explore the data: Perform descriptive statistics, such as calculating the mean, median, and standard deviation for the various work hour categories, to understand the overall distribution and patterns.\n",
    "        3. Identify key metrics: Determine the most relevant metrics to focus on, such as total work hours, overtime, work from home, and salary, based on the business objectives and priorities.\n",
    "        4. Analyze trends: Examine the trends in the key metrics over time, looking for any significant changes or patterns that may indicate areas for improvement or further investigation.\n",
    "        5. Identify outliers and anomalies: Detect any unusual or unexpected values in the data, which could represent potential issues or areas for further exploration.\n",
    "        6. Correlate data points: Investigate the relationships between different data points, such as work hours and salary, to uncover any insights or potential dependencies.\n",
    "        7. Visualize the data: Create informative visualizations, such as line charts, bar graphs, or heatmaps, to effectively communicate the findings and trends to stakeholders.\n",
    "        \n",
    "        \n",
    "        \"\"\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": df.to_string()}\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as f:\n",
    "    f.write(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "attendance_hours = [14399.46, 2473.46, 1775.5, 739.43, 94.5, 1476.57, 3144]\n",
    "attendance_labels = ['Present for Salary', 'Vacation', 'Work from Home', 'Sick Leave', 'Military Service', 'Security Absence (Code 62)', 'Absence']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.pie(attendance_hours, labels=attendance_labels, autopct='%1.1f%%')\n",
    "plt.title('Attendance and Absence Breakdown')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#generate a code that walks through the directory and all the subdirectories and return a list of all files\n",
    "def get_all_files(path):\n",
    "    files=[]\n",
    "    for root, dirs, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            files.append(os.path.join(root,filename))\n",
    "    return files\n",
    "def read_pdf(file_path):\n",
    "    import PyPDF2\n",
    "\n",
    "    read_pdf = PyPDF2.PdfReader(file_path)\n",
    "    pages = read_pdf.pages\n",
    "    res=''\n",
    "    for page in pages:\n",
    "        res+=page.extract_text()\n",
    "    return res\n",
    "#gereate a code that read .doc or .docx files and return the text\n",
    "# def read_docx(file_path):\n",
    "#     from docx import Document\n",
    "#     doc = Document(file_path)\n",
    "#     res=''\n",
    "#     for para in doc.paragraphs:\n",
    "#         res+=para.text\n",
    "#     return res\n",
    "# from spire.doc import *\n",
    "# from spire.doc.common import *\n",
    "        \n",
    "# # Create a Document object\n",
    "# document = Document()\n",
    "# # Load a Word DOCX file\n",
    "# document.LoadFromFile(\"Sample.docx\")\n",
    "# # Or load a Word DOC file\n",
    "# #document.LoadFromFile(\"Sample.doc\")\n",
    "\n",
    "# # Save the file to a PDF file\n",
    "# document.SaveToFile(\"WordToPdf.pdf\", FileFormat.PDF)\n",
    "# document.Close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files=get_all_files(\"/Users/matansharon/python/opensource_rag/2020\")\n",
    "# from docx2pdf import convert\n",
    "# pdf_cnt=0\n",
    "# docx_cnt=0\n",
    "# doc_cnt=0\n",
    "# for file in files:\n",
    "#     if file.endwith(\".docx\"):\n",
    "#         docx_cnt+=1\n",
    "        \n",
    "#         new_path=file.replace(\".docx\",\".pdf\")\n",
    "        \n",
    "    \n",
    "# print(\"pdf:\",pdf_cnt,\"docx:\",docx_cnt,\"doc:\",doc_cnt)\n",
    "# # read_pdf('/Users/matansharon/python/opensource_rag/2020/ECO-20-0082-שינוי תהליך טיפול בשינויים הנדסיים/ECO-20-0082-שינוי תהליך טיפול בשינויים הנדסיים.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    if file.endswith(\".pdf\"):\n",
    "        print(\"pdf->\"*10)\n",
    "        print(read_pdf(file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['1186',\n",
       "  '1139',\n",
       "  '1175',\n",
       "  '1220',\n",
       "  '1023',\n",
       "  '1153',\n",
       "  '1104',\n",
       "  '1113',\n",
       "  '1088',\n",
       "  '1221'],\n",
       " 'distances': [1.2508236169815063,\n",
       "  1.254045009613037,\n",
       "  1.2796638011932373,\n",
       "  1.2918291113339946,\n",
       "  1.293965458869934,\n",
       "  1.3380935192108154,\n",
       "  1.3382604122161865,\n",
       "  1.34063720703125,\n",
       "  1.3589706420898438,\n",
       "  1.3597797106378184],\n",
       " 'metadatas': [{'filename': 'Programming_PyTorch_for_Deep_Learning_Creating_and_Deploying_Deep_Learning_Applications_by_Ian_Pointer_z-lib_org.pdf',\n",
       "   'page_number': 147},\n",
       "  {'filename': 'Programming_PyTorch_for_Deep_Learning_Creating_and_Deploying_Deep_Learning_Applications_by_Ian_Pointer_z-lib_org.pdf',\n",
       "   'page_number': 99},\n",
       "  {'filename': 'Programming_PyTorch_for_Deep_Learning_Creating_and_Deploying_Deep_Learning_Applications_by_Ian_Pointer_z-lib_org.pdf',\n",
       "   'page_number': 136},\n",
       "  {'filename': 'Programming_PyTorch_for_Deep_Learning_Creating_and_Deploying_Deep_Learning_Applications_by_Ian_Pointer_z-lib_org.pdf',\n",
       "   'page_number': 181},\n",
       "  {'filename': 'qlora.pdf', 'page_number': 22},\n",
       "  {'filename': 'Programming_PyTorch_for_Deep_Learning_Creating_and_Deploying_Deep_Learning_Applications_by_Ian_Pointer_z-lib_org.pdf',\n",
       "   'page_number': 114},\n",
       "  {'filename': 'Programming_PyTorch_for_Deep_Learning_Creating_and_Deploying_Deep_Learning_Applications_by_Ian_Pointer_z-lib_org.pdf',\n",
       "   'page_number': 62},\n",
       "  {'filename': 'Programming_PyTorch_for_Deep_Learning_Creating_and_Deploying_Deep_Learning_Applications_by_Ian_Pointer_z-lib_org.pdf',\n",
       "   'page_number': 72},\n",
       "  {'filename': 'Programming_PyTorch_for_Deep_Learning_Creating_and_Deploying_Deep_Learning_Applications_by_Ian_Pointer_z-lib_org.pdf',\n",
       "   'page_number': 45},\n",
       "  {'filename': 'Programming_PyTorch_for_Deep_Learning_Creating_and_Deploying_Deep_Learning_Applications_by_Ian_Pointer_z-lib_org.pdf',\n",
       "   'page_number': 182}],\n",
       " 'documents': ['batch_size =32\\ntrain_data_loader  = torch.utils.data.DataLoader (train_data ,\\nbatch_size =batch_size )\\noptimizer  = optim.Adam(model.parameters (), lr=2e-2)\\ncriterion  = nn.CrossEntropyLoss ()\\ndef train(model, optimizer , loss_fn,  train_loader , val_loader ,\\nepochs=20, device=\\'cuda:0\\' ):\\n    model.to(device)\\n    for epoch in range(epochs):\\n        print(f\"epoch {epoch}\")\\n        model.train()\\n        for batch in train_loader :\\n            optimizer .zero_grad ()\\n            ww, target = batch\\n            ww = ww.to(device)\\n            target= target.to(device)\\n            output = model(ww)\\n            loss = loss_fn(output, target)\\n            loss.backward ()\\n            optimizer .step()\\n        model.eval()\\n        num_correct  = 0\\n        num_examples  = 0\\n        for batch in val_loader :\\n            input, target = batch\\n            input = input.to(device)\\n            target= target.to(device)\\n            output = model(input)\\n            correct = torch.eq(torch.max(output, dim=1)[1], target).view(-1)\\n            num_correct  += torch.sum(correct).item()\\n            num_examples  += correct.shape[0]\\n        print(\"Epoch {}, accuracy = {:.2f}\"\\n        .format(epoch, num_correct  / num_examples ))\\ntrain(model,optimizer ,criterion ,\\ntrain_data_loader ,train_data_loader ,epochs=10)\\nLet’s run that code under py-spy  as before:\\npy-spy -r 99 -d 120 --flame slowloader.svg -- python slowloader.py\\nIf you open the resulting slowloader.svg , you should hopefully see something like\\nFigure 7-10 . Although the flame graph is mostly occupied with loading the images\\nand converting them to tensors, we are spending 16.87% of the sampled runtime in\\napplying random noise. Looking at the code, our implementation of BadRandom  is\\napplying noise at the PIL stage rather than at the tensor stage, so we’re at the mercy of\\nthe imaging library and NumPy rather than PyTorch itself. So our first idea would\\nlikely be to rewrite the transform so that it operates on tensors instead of the PIL\\n130 | Chapter 7: Debugging PyTorch Models',\n",
       "  '        format=\"CSV\",\\n        fields=fields,\\n        skip_header =False)\\n(train, test, valid) = twitterDataset .split(split_ratio =[0.8,0.1,0.1])\\nvocab_size  = 20002\\nTWEET.build_vocab (train, max_size  = vocab_size )\\ntrain_iterator , valid_iterator , test_iterator  = data.BucketIterator .splits(\\n(train, valid, test),\\nbatch_size  = 32,\\ndevice = device)\\nWith our data processing sorted, we can move on to defining our model.\\nCreating Our Model\\nWe use the Embedding  and LSTM  modules in PyTorch that we talked about in the first\\nhalf of this chapter to build a simple model for classifying tweets:\\nimport torch.nn  as nn\\nclass OurFirstLSTM (nn.Module):\\n    def __init__ (self, hidden_size , embedding_dim , vocab_size ):\\n        super(OurFirstLSTM , self).__init__ ()\\n        self.embedding  = nn.Embedding (vocab_size , embedding_dim )\\n        self.encoder = nn.LSTM(input_size =embedding_dim ,\\n                hidden_size =hidden_size , num_layers =1)\\n        self.predictor  = nn.Linear(hidden_size , 2)\\n    def forward(self, seq):\\n        output, (hidden,_) = self.encoder(self.embedding (seq))\\n        preds = self.predictor (hidden.squeeze(0))\\n        return preds\\nmodel = OurFirstLSTM (100,300, 20002)\\nmodel.to(device)\\nAll we do in this model is create three layers. First, the words in our tweets are\\npushed into an Embedding  layer, which we have established as a 300-dimensional vec‐\\ntor embedding. That’s then fed into a LSTM  with 100 hidden features (again, we’re\\ncompressing down from the 300-dimensional input like we did with images). Finally,\\nthe output of the LSTM (the final hidden state after processing the incoming tweet) is\\npushed through a standard fully connected layer with three outputs to correspond to\\nour three possible classes (negative, positive, or neutral). Next we turn to the training\\nloop!\\n82 | Chapter 5: Text Classification',\n",
       "  'def train(model, optimizer , loss_fn, train_data_loader , test_data_loader , epochs=20):\\n    model = model.train()\\n    iteration  = 0\\n    for epoch in range(epochs):\\n        model.train()\\n        for batch in train_loader :\\n            optimizer .zero_grad ()\\n            input, target = batch\\n            output = model(input)\\n            loss = loss_fn(output, target)\\n            writer.add_scalar (\\'loss\\', loss, epoch)\\n            loss.backward ()\\n            optimizer .step()\\n        model.eval()\\n        num_correct  = 0\\n        num_examples  = 0\\n        for batch in val_loader :\\n            input, target = batch\\n            output = model(input)\\n            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], target).view(-1)\\n            num_correct  += torch.sum(correct).item()\\n            num_examples  += correct.shape[0]\\n            print(\"Epoch {}, accuracy = {:.2f}\".format(epoch,\\n                   num_correct  / num_examples )\\n            writer.add_scalar (\\'accuracy\\' , num_correct  / num_examples , epoch)\\n        iterations  += 1\\nWhen it comes to using add_graph() , we need to send in a tensor to trace through\\nthe model as well as the model itself. Once that happens, though, you should see\\nGRAPHS  appear in TensorBoard, and as shown in Figure 7-4 , clicking the large ResNet\\nblock reveals further detail of the model’s structure.\\nFigure 7-4. Visualizing ResNet\\nTensorBoard | 119',\n",
       "  '      target_mix  = targets_mix .to(device)\\n      distribution  = torch.distributions .beta.Beta(0.5,0.5)\\n      beta = distribution .expand(torch.zeros(batch_size ).shape).sample().to(device)\\n      # We need to transform the shape of beta\\n      # to be in the same dimensions as our input tensor\\n      # [batch_size, channels, height, width]\\n      mixup = beta[:, None, None, None]\\n      inputs_mixed  = (mixup * inputs) + (1-mixup * inputs_mix )\\n      # Targets are mixed using beta as they have the same shape\\n      targets_mixed  = (beta * targets) + (1-beta * inputs_mix )\\n      output_mixed  = model(inputs_mixed )\\n      # Multiply losses by beta and 1-beta,\\n      # sum and get average of the two mixed losses\\n      loss = (loss_fn(output, targets) * beta\\n             + loss_fn(output, targets_mixed )\\n             * (1-beta)).mean()\\n      # Training method is as normal from herein on\\n      loss.backward ()\\n      optimizer .step()\\n      …\\nWhat’s happening here is after we get our two batches, we use torch.distribu\\ntion.Beta  to generate a series of mix parameters, using the expand  method to pro‐\\nduce a tensor of [1, batch_size] . We could iterate through the batch and generate\\nthe parameters one by one, but this is neater, and remember, GPUs love matrix multi‐\\nplication, so it’ll end up being faster to do all the calculations across the batch at once\\n(this is shown in Chapter 7  when fixing our BadRandom  transformation, remember!).\\nWe multiply the entire batch by this tensor, and then the batch to mix in by 1 -\\nmix_factor_tensor  using broadcasting (which we covered in Chapter 1 ).\\nWe then take the losses of the predictions against our targets for both images, and our\\nfinal loss is the mean of the sum of those losses. What’s happening there? Well, if you\\nlook at the source code for CrossEntropyLoss , you’ll see the comment The losses\\nare averaged across observations for each minibatch.  There’s also a reduc\\ntion  parameter that has a default set to mean  (we’ve used the default so far, so that’s\\nwhy you haven’t seen it before!). We need to preserve that condition, so we take the\\nmean of our combined losses.\\n164 | Chapter 9: PyTorch in the Wild',\n",
       "  'Parameters Dataset Batch size LR Steps Source Length Target Length\\n7B All 16 2e-4 10000 384 128\\n7B OASST1 16 2e-4 1875 - 512\\n7B HH-RLHF 16 2e-4 10000 - 768\\n7B Longform 16 2e-4 4000 512 1024\\n13B All 16 2e-4 10000 384 128\\n13B OASST1 16 2e-4 1875 - 512\\n13B HH-RLHF 16 2e-4 10000 - 768\\n13B Longform 16 2e-4 4000 512 1024\\n33B All 32 1e-4 5000 384 128\\n33B OASST1 16 1e-4 1875 - 512\\n33B HH-RLHF 32 1e-4 5000 - 768\\n33B Longform 32 1e-4 2343 512 1024\\n65B All 64 1e-4 2500 384 128\\n65B OASST1 16 1e-4 1875 - 512\\n65B HH-RLHF 64 1e-4 2500 - 768\\n65B Longform 32 1e-4 2343 512 1024\\nTable 9: Training hyperparameters for QL ORA finetuning on different datasets and across model sizes.\\nSelf-Instruct, Alpaca, Unnatural Instructions The Self-Instruct, Alpaca, and Unnatural Instruc-\\ntions datasets [ 59,55,26] are instruction tuning datasets collected with various approaches of model\\ndistillation from GPT-3 Instruct and ChatGPT. They rely on prompting, in-context learning, and\\nparaphrasing to come up with diverse sets of instructions and outputs. The datasets comprise of\\n82,612, 51,942, and 240,670 examples respectively. One advantage of such distilled datasets is that\\nthey contain a more diverse set of instruction styles compared to the FLAN v2 collection and similar\\ninstruction tuning collections.\\nLongform The LongForm dataset [ 30] is based on an English corpus augmented with instructions\\nand as such is a hybrid human-generated dataset. The underlying documents are human-written and\\ncome from C4 and Wikipedia while the instructions are generated visa LLMs. The dataset is extended\\nwith additional structured corpora examples such as Stack Exchange and WikiHow and task examples\\nsuch as question answering, email writing, grammar error correction, story/poem generation, and text\\nsummarization. The dataset contains 23,700 examples.\\nChip2 is part of the OIG Laion dataset. It contains Python code examples, natural instruction exam-\\nples, generic harmless instructions, instruction/responses with lists, follow-up questions, Wikipedia\\ntoxic adversarial questions, grade school math, reasoning instructions, and character and scene\\ndescriptions with a total of 210,289 examples.\\nB.2 Hyperparameters\\nWe provide the exact hyperparameters used in our QLORAfinetuning experiments. We find hyper-\\nparameters to be largely robust across datasets. We use the MMLU 5-shot dev set for validation\\nand hyperparameter tuning. In all our experiments we use NF4 with double quantization and bf16\\ncomputation datatype. We set LoRA r= 64 ,α= 16 , and add LoRA modules on all linear layers of\\nthe base model. We also use Adam beta2 of 0.999, max grad norm of 0.3 and LoRA dropout of 0.1\\nfor models up to 13B and 0.05 for 33B and 65B models. Following previous work on instruction\\nfinetuning [ 62,60] and after benchmarking other linear and cosine schedules, we use a constant\\nlearning rate schedule. We use group-by-length to group examples of similar lengths in the same\\nbatch (note this will produce a oscillating loss curve). The hyperparameters we tune for each model\\nsize are shown in Table 9.\\nB.3 Ablations\\nWhile it is general practice in the literature to only train on the response in instruction following\\ndatasets, we study the effect of training on the instruction in addition to the response in Table 10. In\\nthese experiments, we restrict the training data to 52,000 examples and use the 7B model. Over four\\ndifferent instruction tuning datasets, we find that only training on the target is beneficial to MMLU\\n23',\n",
       "  'tensor([-0.0128, -0.0131, -0.0143,  ...,  0.0000,  0.0000,  0.0000])\\ntensor.shape\\ntorch.Size([220500])\\nlabel\\n\\'15\\'\\nWe can construct a data loader by using standard PyTorch constructs:\\nexample_loader  = torch.utils.data.DataLoader (test_esc50 , batch_size  = 64,\\nshuffle = True)\\nBut before we do that, we have to go back to our data. As you might remember, we\\nshould always create training, validation, and test sets. At the moment, we have just\\none directory with all the data, which is no good for our purposes. A 60/20/20 split of\\ndata into training, validation, and test collections should suffice. Now, we could do\\nthis by taking random samples of our entire dataset (taking care to sample without\\nreplacement and making sure that our newly constructed datasets are still balanced),\\nbut again the ESC-50 dataset saves us from having to do much work. The compilers\\nof the dataset separated the data into five equal balanced folds , indicated by the first\\ndigit in the filename. We’ll have folds 1,2,3  be the training set, 4 the validation set,\\nand 5 the test set. But feel free to mix it up if you don’t want to be boring and consec‐\\nutive! Move each of the folds to test, train , and validation  directories:\\nmv 1* ../train\\nmv 2* ../train\\nmv 3* ../train\\nmv 4* ../valid\\nmv 5* ../test\\nNow we can create the individual datasets and loaders:\\nfrom pathlib import Path\\nbs=64\\nPATH_TO_ESC50  = Path.cwd() / \\'esc50\\'\\npath =  \\'test.md\\'\\ntest\\ntrain_esc50  = ESC50(PATH_TO_ESC50  / \"train\")\\nvalid_esc50  = ESC50(PATH_TO_ESC50  / \"valid\")\\ntest_esc50   = ESC50(PATH_TO_ESC50  / \"test\")\\ntrain_loader  = torch.utils.data.DataLoader (train_esc50 , batch_size  = bs,\\n                shuffle = True)\\nvalid_loader  = torch.utils.data.DataLoader (valid_esc50 , batch_size  = bs,\\n                shuffle = True)\\ntest_loader   = torch.utils.data.DataLoader (test_esc50 , batch_size  = bs,\\n                shuffle = True)\\nWe have our data all set up, so we’re all ready to look at a classification model.\\nExploring ESC-50 | 97',\n",
       "  'track_running_stats =True)\\n  (relu): ReLU(inplace)\\n  (maxpool): MaxPool2d (kernel_size =3, stride=2, padding=1,\\n  dilation =1, ceil_mode =False)\\n  (layer1): Sequential (\\n    (0): BasicBlock (\\n      (conv1): Conv2d(64, 64, kernel_size =(3, 3), stride=(1, 1),\\n      padding=(1, 1), bias=False)\\n      (bn1): BatchNorm2d (64, eps=1e-05, momentum =0.1, affine=True,\\n       track_running_stats =True)\\n      (relu): ReLU(inplace)\\n      (conv2): Conv2d(64, 64, kernel_size =(3, 3), stride=(1, 1),\\n      padding=(1, 1), bias=False)\\n      (bn2): BatchNorm2d (64, eps=1e-05, momentum =0.1, affine=True,\\n       track_running_stats =True)\\n    )\\n    (1): BasicBlock (\\n      (conv1): Conv2d(64, 64, kernel_size =(3, 3), stride=(1, 1),\\n       padding=(1, 1), bias=False)\\n      (bn1): BatchNorm2d (64, eps=1e-05, momentum =0.1, affine=True,\\n       track_running_stats =True)\\n      (relu): ReLU(inplace)\\n      (conv2): Conv2d(64, 64, kernel_size =(3, 3), stride=(1, 1),\\n      padding=(1, 1), bias=False)\\n      (bn2): BatchNorm2d (64, eps=1e-05, momentum =0.1, affine=True,\\n       track_running_stats =True)\\n    )\\n  )\\n  (layer2): Sequential (\\n    (0): BasicBlock (\\n      (conv1): Conv2d(64, 128, kernel_size =(3, 3), stride=(2, 2),\\n       padding=(1, 1), bias=False)\\n      (bn1): BatchNorm2d (128, eps=1e-05, momentum =0.1, affine=True,\\n       track_running_stats =True)\\n      (relu): ReLU(inplace)\\n      (conv2): Conv2d(128, 128, kernel_size =(3, 3), stride=(1, 1),\\n       padding=(1, 1), bias=False)\\n      (bn2): BatchNorm2d (128, eps=1e-05, momentum =0.1, affine=True,\\n       track_running_stats =True)\\n      (downsample ): Sequential (\\n        ( 0): Conv2d(64, 128, kernel_size =(1, 1), stride=(2, 2),\\n         bias=False)\\n        ( 1): BatchNorm2d (128, eps=1e-05, momentum =0.1, affine=True,\\n         track_running_stats =True)\\n      )\\n    )\\n    (1): BasicBlock (\\n      (conv1): Conv2d(128, 128, kernel_size =(3, 3), stride=(1, 1),\\n       padding=(1, 1), bias=False)\\n      (bn1): BatchNorm2d (128, eps=1e-05, momentum =0.1, affine=True,\\n       track_running_stats =True)\\n      (relu): ReLU(inplace)\\nUsing Pretrained Models in PyTorch | 45',\n",
       "  '        loss = loss_fn(outputs, labels)\\n        # Crash out if loss explodes\\n        if batch_num  > 1 and loss > 4 * best_loss :\\n            return log_lrs[10:-5], losses[10:-5]\\n        # Record the best loss\\n        if loss < best_loss  or batch_num  == 1:\\n            best_loss  = loss\\n        # Store the values\\n        losses.append(loss)\\n        log_lrs.append(math.log10(lr))\\n        # Do the backward pass and optimize\\n        loss.backward ()\\n        optimizer .step()\\n        # Update the lr for the next step and store\\n        lr *= update_step\\n        optimizer .param_groups [0][\"lr\"] = lr\\n    return log_lrs[10:-5], losses[10:-5]\\nWhat’s going on here is that we iterate through the batches, training almost as usual;\\nwe pass our inputs through the model and then we get the loss from that batch. We\\nrecord what our best_loss  is so far, and compare the new loss against it. If our new\\nloss is more than four times the best_loss , we crash out of the function, returning\\nwhat we have so far (as the loss is probably tending to infinity). Otherwise, we keep\\nappending the loss and logs of the current learning rate, and update the learning rate\\nwith the next step along the way to the maximal rate at the end of the loop. The plot\\ncan then be shown using the matplotlib  plt function:\\nlogs,losses = find_lr()\\nplt.plot(logs,losses)\\nfound_lr  = 1e-2\\nNote that we return slices of the lr logs and losses. We do that simply because the\\nfirst bits of training and the last few (especially if the learning rate becomes very large\\nquite quickly) tend not to tell us much information.\\nThe implementation in fast.ai’s library also includes weighted smoothing, so you get\\nsmooth lines in your plot, whereas this snippet produces spiky output. Finally,\\nremember that because this function does actually train the model and messes with\\nthe optimizer’s learning rate settings, you should save and reload your model before‐\\nhand to get back to the state it was in before you called find_lr()  and also\\nFinding That Learning Rate | 55',\n",
       "  \"        for batch in train_loader :\\n            optimizer .zero_grad ()\\n            inputs, target = batch\\n            inputs = inputs.to(device)\\n            target = targets.to(device)\\n            output = model(inputs)\\n            loss = loss_fn(output, target)\\n            loss.backward ()\\n            optimizer .step()\\n            training_loss  += loss.data.item()\\n        training_loss  /= len(train_iterator )\\n        model.eval()\\n        num_correct  = 0\\n        num_examples  = 0\\n        for batch in val_loader :\\n            inputs, targets = batch\\n            inputs = inputs.to(device)\\n            output = model(inputs)\\n            targets = targets.to(device)\\n            loss = loss_fn(output,targets)\\n            valid_loss  += loss.data.item()\\n            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1],\\n          target).view(-1)\\n            num_correct  += torch.sum(correct).item()\\n            num_examples  += correct.shape[0]\\n        valid_loss  /= len(valid_iterator )\\n        print('Epoch: {}, Training Loss: {:.2f},\\n        Validation  Loss: {:.2f},\\n        accuracy  = {:.2f}'.format(epoch, training_loss,\\n        valid_loss , num_correct  / num_examples ))\\nThat’s our training function, and we can kick off training by calling it with the\\nrequired parameters:\\ntrain(simplenet , optimizer , torch.nn.CrossEntropyLoss (),\\n      train_data_loader , test_data_loader ,device)\\nThe network will train for 20 epochs (you can adjust this by passing in a value for\\nepoch  to train() ), and you should get a printout of the model’s accuracy on the vali‐\\ndation set at the end of each epoch.\\nY ou have trained your first neural network—congratulations! Y ou can now use it to\\nmake predictions, so let’s look at how to do that.\\nMaking Predictions\\nWay back at the start of the chapter, I said we would make a neural network that\\ncould classify whether an image is a cat or a fish. We’ve now trained one to do just\\nthat, but how do we use it to generate a prediction for a single image? Here’s a quick\\n28 | Chapter 2: Image Classification  with PyTorch\",\n",
       "  'Now, having two data loaders isn’t too much trouble, but it does make the code a little\\nmore complicated. If you run this code, you might error out because the batches are\\nnot balanced as final batches come out of the loaders, meaning that you’ll have to\\nwrite extra code to handle that case. The authors of the mixup paper suggest that you\\ncould replace the mix data loader with a random shuffle of the incoming batch. We\\ncan do this with torch.randperm() :\\nshuffle = torch.randperm (inputs.size(0))\\ninputs_mix  = inputs[shuffle]\\ntargets_mix  = targets[shuffle]\\nWhen using mixup in this way, be aware that you are much more likely to get colli‐\\nsions  where you end up applying the same parameter to the same set of images,\\npotentially reducing the accuracy of training. For example, you could have cat1 mixed\\nwith fish1, and draw a beta parameter of 0.3. Then later in the same batch, you pull\\nout fish1 and it gets mixed with cat1 with a parameter of 0.7—making it the same\\nmix! Some implementations of mixup—in particular, the fast.ai implementation—\\nresolve this issue by replacing our mix parameters with the following:\\nmix_parameters  = torch.max(mix_parameters , 1 - mix_parameters )\\nThis ensures that the nonshuffled batch will always have the highest component when\\nbeing merged with the mix batch, thus eliminating that potential issue.\\nOh, and one more thing: we performed the mixup transformation after  our image\\ntransformation pipeline. At this point, our batches are just tensors that we’ve added\\ntogether. This means that there’s no reason mixup training should be restricted to\\nimages. We could use it on any type of data that’s been transformed into tensors,\\nwhether text, image, audio, or anything else.\\nWe can still do a little more to make our labels work harder for us. Enter another\\napproach that is now a mainstay of state-of-the-art models: label smoothing .\\nLabel Smoothing\\nIn a similar manner to mixup, label smoothing  helps to improve model performance\\nby making the model less sure of its predictions. Instead of trying to force it to pre‐\\ndict 1 for the predicted class (which has all the problems we talked about in the previ‐\\nous section), we instead alter it to predict 1 minus a small value, epsilon . We can\\ncreate a new loss function implementation that wraps up our existing CrossEntropy\\nLoss  function with this functionality. As it turns out, writing a custom loss function is\\njust another subclass of nn.Module :\\nclass LabelSmoothingCrossEntropyLoss (nn.Module):\\n    def __init__ (self, epsilon=0.1):\\n        super(LabelSmoothingCrossEntropyLoss , self).__init__ ()\\n        self.epsilon = epsilon\\nData Augmentation: Mixed and Smoothed | 165']}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "db=chromadb.PersistentClient(path='chroma_storage_pages')\n",
    "collection=db.get_collection('by_pages_collection')\n",
    "query='test_train_split'\n",
    "\n",
    "results = collection.query(\n",
    "            query_texts=[query], n_results=10, include=[\"documents\", \"metadatas\",\"distances\"],\n",
    "        )\n",
    "# import numpy as np\n",
    "#filter the results by the distance the criteria is that the distance should be by this formula: abs(1-distance)<0.4   \n",
    "\n",
    "def filter_results(data):\n",
    "    # Unpack data\n",
    "    ids = data.get('ids', [])\n",
    "    distances = data.get('distances', [])\n",
    "    metadatas = data.get('metadatas', [])\n",
    "    documents = data.get('documents', [])\n",
    "    \n",
    "    filtered_results = {'ids': [], 'distances': [], 'metadatas': [], 'documents': []}\n",
    "    \n",
    "    # Iterate over distances and filter based on the criteria\n",
    "    for i, distance_list in enumerate(distances):\n",
    "        for j, distance in enumerate(distance_list):\n",
    "            if abs(1 - distance) < 0.4:\n",
    "                filtered_results['ids'].append(ids[i][j])\n",
    "                filtered_results['distances'].append(distance)\n",
    "                filtered_results['metadatas'].append(metadatas[i][j])\n",
    "                filtered_results['documents'].append(documents[i][j])\n",
    "                \n",
    "    return filtered_results\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "filter_results(results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
