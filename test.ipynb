{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in llm(\"AI is going to\", stream=True):\n",
    "    print(text, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results on the cpu using ctransformers\n",
    "### - 27.4 seconds\n",
    "### output:\n",
    "<p>\n",
    "have an enormous impact on the future of the industry.\n",
    "This year, AI was all over the show floor and there are a number of companies that are using AI in their products. Here's a quick look at some of them:\n",
    "Fulcrum is an automated, artificial intelligence-powered software solution that helps manufacturers improve productivity and efficiency by automating processes, removing errors, and freeing up employees to focus on more complex tasks.\n",
    "Kapil Khera, co-founder and CEO of Fulcrum, told Inman: \"I don't know a single company that doesn't have AI in their product. I think it's the most important thing to come out of 2018.\"\n",
    "The company is currently working on machine learning tools and workflow applications with machine intelligence technology so you can really begin building these powerful AI-powered solutions. The full AI-focused ecosystem, with automation in product design and sophisticated analytics, will be available to all customers by the end of 2018.\n",
    "Kapil Khera, co-founder and CEO of Fulcrum\n",
    "Surely\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_response_tiny_llama(text):\n",
    "    path='/teamspace/studios/this_studio/TinyLlama-1.1B-Chat-v1.0-GPTQ'\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "    prompt = text\n",
    "    system_message = \"You are a story writing assistant\"\n",
    "    prompt_template=f'''<|system|>\n",
    "    {system_message}</s>\n",
    "    <|user|>\n",
    "    {prompt}</s>\n",
    "    <|assistant|>\n",
    "    '''\n",
    "\n",
    "    print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "    input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "    output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "    return(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_response_tiny_llama(\"what is qlora?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# result on gpu 2.8 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# handling the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m text\u001b[38;5;241m=\u001b[39mconvert_from_pdf_to_text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf_files/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mpdf)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_files/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtemp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: write() argument must be str, not list"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pdf_list=os.listdir(\"/Users/matansharon/python/opensource_rag/pdf_files\")\n",
    "text_list=os.listdir(\"/Users/matansharon/python/opensource_rag/text_files\")\n",
    "\n",
    "for pdf in pdf_list:\n",
    "    temp=pdf.replace(\".pdf\",\".txt\")\n",
    "    if temp not in text_list:\n",
    "        text=convert_from_pdf_to_text(\"pdf_files/\"+pdf)\n",
    "        with open(\"text_files/\"+temp, 'w') as f:\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_pdf_to_text(file_path):\n",
    "    import PyPDF2\n",
    "\n",
    "    read_pdf = PyPDF2.PdfReader(file_path)\n",
    "    pages = read_pdf.pages\n",
    "    res=''\n",
    "    for page in pages:\n",
    "        res+=page.extract_text()\n",
    "    return res\n",
    "pages=convert_from_pdf_to_text('/Users/matansharon/python/opensource_rag/pdf_files/A semantic loss for ontology classification.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client=chromadb.Client()\n",
    "import os\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "    api_key=os.environ.get(\"HuggingFace_API_KEY\"),\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "collection=client.get_or_create_collection('test',embedding_function=huggingface_ef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
