Tabular Embedding Model (TEM): Finetuning
Embedding Models For Tabular RAG Applications
Sujit Khanna
sujit@synergia-ai.netShishir Subedi
shishirsubedi41@gmail.com
Synergia
Abstract
In recent times Large Language Models have exhibited tremendous capabilities,
especially in the areas of mathematics, code generation and general-purpose rea-
soning. However for specialized domains especially in applications that require
parsing and analyzing large chunks of numeric or tabular data even state-of-the-art
(SOTA) models struggle. In this paper, we introduce a new approach to solving
domain-specific tabular data analysis tasks by presenting a unique RAG workflow
that mitigates the scalability issues of existing tabular LLM solutions. Specifi-
cally, we present Tabular Embedding Model (TEM), a novel approach to fine-tune
embedding models for tabular Retrieval-Augmentation Generation (RAG) appli-
cations. Embedding models form a crucial component in the RAG workflow and
even current SOTA embedding models struggle as they are predominantly trained
on textual datasets and thus underperform in scenarios involving complex tabular
data. We choose the domain of financial markets for model evaluation to demon-
strate TEM’s ability to handle intricate and high-dimensional datasets, an area
where existing models typically falter. The evaluation results showcase that our
approach not only outperforms current SOTA embedding models in this domain
but also does so with a notably smaller and more efficient model structure.
1 Introduction
Large Language Models have exhibited incredible capabilities in recent times across diverse sets of
applications like mathematical reasoning Azerbayev et al. [2024], general-purpose problem-solving
ope [2024], Guo et al. [2023], Murr et al. [2023] and code generation Nijkamp et al. [2023]. LLMs
are also highly capable information or knowledge retrieval machines, however, they have some
limitations in domain-specific tasks or knowledge-intensive applications. They are also notorious
for producing hallucinations Huang et al. [2023], as they have training cut-off dates and their entire
knowledge is abstracted in their trained weights. Since re-training LLMs is time-consuming and very
costly the alternative is to provide the most recent/relevant information to the LLM in context via
the Retrieval-Augmentation Generation approach (RAG) Zhao et al. [2024] Lewis et al. [2021].In
a generic RAG process given an input query, the retriever looks up the most relevant information
in a knowledge base by semantic similarity search and sends it as additional context to the LLM to
answer the query. This pipeline makes an LLM overcome its issues around domain-specific tasks
and helps in accurately performing the tasks outlined by the user.
The crucial components in RAG pipelines are the base LLM like GPT4, an embedding model, and
a vector database. The embedding model is responsible for converting the external knowledge base
Tabular Embedding ModelarXiv:2405.01585v1  [cs.AI]  28 Apr 2024into compressed embedding and is indexed into a vector database. Embedding models are also
useful for retrieving the most relevant information from this vector store based on a user’s query
by doing similarity search. This makes the embedding model a crucial component of the RAG
pipeline. The existing SOTA embedding models are trained dominantly on textual datasets which
leads to sub-optimal performance on RAG applications that need tooling or numeric data. There
are a few multi-task RAG approaches like LLM-Embedder, Zhang et al. [2023] that are trained to
retrieve examples, tools, and knowledge bases but we were unable to find any trained specifically
for tabular data applications.
RAG applications in tabular data tasks are a niche but an important domain. Recent developments
in tool usage have further amplified LLMs ability to analyze large-scale structured tabular data like
CSV files or SQL tables. An example of such an application is asking LLM to "Identify best
performing stock by returns from S&P500 index components over the last 6 months" . For
simpler tabular data tasks a generic embedding model suffices however when the nature of questions
becomes complex; these models fail to retrieve the correct data chunks and begin hallucinating. In
this paper we present our approach to solving this problem by finetuning a lightweight open-source
embedding model for a tabular RAG task called Tabular Embedding Model or TEM. We present
a unique embedding approach for tabular data that bypasses the need to embed the entire dataset,
such an approach is highly scalable and can be applied to multiple tables and databases. We also
outlined our RAG pipeline custom-built for this task by including a separate data analysis agent that
only needs the relevant CSV files or tables from the retriever to execute the user’s query. In this
paper, we are focused on evaluating the model’s performance in the domain of financial markets
as it can encompass a diverse set of tasks from a host of interrelated and independent datasets. A
semi-automated role-playing framework was used to generate the training and evaluation dataset
from GPT-4. We found our finetuned model significantly outperforms existing SOTA embedding
models for tabular data applications despite being much smaller in size.
2 Related Work
2.1 Table Retrieval and Q & A
Existing research in this space focuses on training LLMs directly on tabular data by handcrafting
instruction finetuning datasets to imbibe it with tabular data understandingZhang et al. [2024c].
Other approaches in this domain like Liu et al. [2022] Yang et al. [2024] Gong et al. [2020] perform
pre-training on LMs like GPT-3 with mapping of natural language sentence and its corresponding
table to the output of the answer. However, these approaches are not scalable when the analysis
of tabular data runs into millions of rows. Another issue with a generalist tabular LLM is that
the nuances involved in different applications are ignored especially in niche areas like financial
markets. In recent times retriever-based approaches like Chen et al. [2021] proposed an approach
based on early fusion and cross-block reader techniques for Q & A tasks over both free text and
tables. Other table retrieval research focuses on using intrinsic and extrinsic similarities for the
retrieval process Shraga et al. [2020]. This approach is further improved in T-RAG Pan et al. [2022]
that parses through a table corpus to directly locate the correct answer from the table cell. Such
an approach creates obstacles for complex user queries when the solution to a problem requires
analyzing millions of rows across a plethora of tables, this adds a lot of latency in the RAG process
and creates issues in practical applications where speed of execution is crucial.
2.2 Finetuning Embedding Models
Recent advances in finetuning approaches have empowered LLMs to enhance their knowledge and
understanding on less-popular or low-frequency concepts Soudani et al. [2024] compares the RAG
vs Finetuning approaches and identifies that both significantly enhance the capabilities of LLMs
on domain-specific tasks. Wang et al. [2024] exhibited how using a simple finetuning approach
when used on an open-source decoder-only LLMs trained on a diverse set of synthetic datasets for
hundreds of thousands of text embedding tasks across nearly 100 languages can achieve SOTA per-
formance. Recently Zhang et al. [2024a] introduced a novel approach of fine-tuning a black-box
embedding model by augmenting it with a trainable open-sourced embedding model on domain-
specific tasks. Zhang et al. [2024b] proposed an RAFT framework that combines RAG with Fine-
tuning LLMs where given a question, and a set of retrieved documents, this approach trains the
LLM to ignore those documents that don’t help in answering the question, this approach was shown
2to significantly improve the performance of LLM on domain-specific tasks. The above finetuning
approaches are predominantly based on enhancing text-based retrieval and embeddings. A multi-
application embedding model is proposed by Zhang et al. [2023] that supports the diverse retrieval
augmentation needs of LLMs with one unified embedding model that captures distinct semantic re-
lationships by modeling mutual interference across different tasks. However, we were unable to find
any research that aimed at finetuning an embedding model specific to tabular data analysis applica-
tions. Most of the existing approaches when directly applied to such a task fail to generalize well and
often run into scalability issues. The finetuning approach with the Tabular-RAG pipeline overcomes
these problems and outperforms even the SOTA embedding models on our custom dataset relevant
for financial tabular data analysis tasks.
3 TEM (Tabular Embedding Model)
In this section, we describe the TEM a new approach to finetuning smaller open-sourced embedding
model that is trained on general language corpus for a sophisticated tabular RAG application. We
first introduce the classical workflow of using embedding models in tabular rag applications, present
our RAG workflow and indexing strategy for tabular data applications, describe a semi-automated
dataset-generating process, and finally present a novel finetuning framework for tabular embedding
model.
3.1 Embedding tabular data in RAG pipeline
A traditional embedding process for indexing the non-tabular data consists of creating chunks of
documents, creating embeddings via a Embedding Model like OpenAI’s text-embedding-3-large,
and then indexing and storing them in a vector database. This approach fails for tabular data like
CSV files or an SQL table because
• Tabular data can run into billions of rows, where creating chunks adds a lot of scalability
issues
• Sending relevant chunks to the LLM at the end of the RAG process will quickly stuff the
context window, which would make any analysis of the relevant data useless
• Creating chunks adds a lot of redundancies, where the majority of chunks are irrelevant to
a user’s question
Figure 1 contrasts the tabular data indexing approach vs generic approach.
3.2 RAG Pipeline
As mentioned in the previous subsection for efficient performance of RAG on tabular data tasks, we
need to integrate a data analysis agent into our workflow. Doing this reduces the burden on the RAG
pipeline to ingest the entire dataset for analysis. Instead, the task of the RAG pipeline is to recom-
mend the most relevant CSV files/tables based on the user questions. The data analysis agent built
on top of the LLM then uses the most relevant file and performs the data analysis requested by the
user. This pipeline is described in more detail in Figure 2 and crucial components of this pipeline are
•Context: This component provides additional context to the base LLM that enables it to
pick the most appropriate set of files from the output of the embedding model and generate
the code relevant to the user’s query
•Code Evaluator: This component evaluates the code from the base LLMs and provides
feedback to the LLM to modify the code if it has any errors or mistakes. This is done via
a built-in reflection framework to identify any possible errors, and debug and resolve them
to create accurate and optimized code.
•Code Executor: As the name suggests this component is responsible for executing the
final version of the code
•Final Response: This component is responsible for deciding how the final output of the
code should be saved or displayed for the user
3Figure 1: Generic indexing approach vs Tabular Embedding Approach
Figure 2: Proposed Tabular RAG Workflow
3.3 Training data for finetuning
Our research is primarily focused on tabular data analysis in financial markets, we chose data from
this domain as there are a plethora of data sources that can explain different market effects. There
are plenty of financial instruments and asset classes across which we can analyze the data and
financial datasets are highly dimensional and non-linear, which adds another layer of complexity to
tabular data analysis tasks.
Our dataset consists of questions across many stocks, macro variables, and data from options markets
as well as from other asset classes like commodities, bonds, etc. Specifically, our dataset contains
4Figure 3: Custom finetuning dataset
a mapping of questions to relevant files, where each question can be mapped to [1, ..n]files where
0< n < = 5files. Files can belong to the same subset i.e. all files belonging to equities or any other
combination of asset classes.
We use a semi-automated process of generating the dataset using a mixture of role-playing and a few
shot prompts to generate a diverse set of questions. We initially provided the following information
in context to the LLM. (We used GPT4 for generating the dataset)
• A dictionary mapping file name to its description and column names with its definitions
• Guidelines for generating the questions i.e. specific set of rules to follow in the data gener-
ation process
The next step involves a role-play-based approach to generate a diverse set of questions. In this
role-play, the user asks LLM to assume multiple roles such as a fundamental analyst, macro trader,
machine learning expert, data scientist as well as a retail trader. A prompt template is used for role-
playing which also provides a few shot examples to generate a diverse set of questions. This process
is described in Figure 3. In this role play, we also adjust for the number of relevant files required for
each question. The hierarchical flow of such an approach is shown in Figure 4.
3.4 Finetuning methodology
Our finetuning methodology consists of 2 steps, in the first step we initialize the base embedding
model with "New Word Embeddings" Hewitt [2021], and the second step involves finetuning our
model with the dataset generated in the first step
3.4.1 New words embedding initialization
The current finetuning approaches mostly use instruction finetuning datasets to train a language
model for a downstream task. A better approach is to first expand the vocabulary of the model
by adding new words. This can be done by resizing the base model’s embeddings to make new
embeddings for the words in the updated vocabulary i.e. vocabulary for the downstream task.
However this approach poses a problem during the training process where sampling from the
existing distribution of reduces the probabilities of words post-expansion. More specifically Let
wi:nbe a sequence of words in the vocabulary V= 1....n. Let our language model be characterized
as
Pθ(wi/w1:i−1) =eht
i−1mwi
Pn
j=1eht
i−1mjwhere hi−1=ϕθ(w1:t−1)∈Rdandmi∈Rdis the embedding
for word i. Post expansion when we add new words to vocal n+ 1̸∈V, we sample new word
embedding mn+1from the initial distribution.
So our new LM is characterized as
5Figure 4: Semi-automated data generation process via role-play
Pθi′(wi/w1:i−1), where θ′=θ∪ {mn+1}such that,
Pθ′(wi/w1:i−1) =eht
i−1mwi
Z+Pn
j=1eht
i−1mn+1.
The relation between pre and post-expansion LM can therefore be represented as,
Pθ′(wi/w1:i−1) =Pθ(wi/w1:i−1)×1
1+eht
i−1mn+1
Z
From the above relation if Z is small relative to the new distribution, then the probabilities of all
pre-expansion words decrease a lot. The KL divergence between pre and post-word expansion is
KL(pθ′∥pθ) = log
1 +1+eht
i−1mn+1
Z
This can cause huge divergences between pre and post-expansion models, especially in case of
small norm initialization. A simple way to circumvent this is to average all initial embeddings and
instantiate new word embeddings with them. Formally we have
µ= 1/n×Pn
imi
σ= 1/n×(M−µ)T(M−µ)
mn+1∼ N(µ, σ)
6Figure 5: Sample Questions generated via role-playing framework
This change would bound the KL divergence to KL(p′
θ||pθ) =log(1
1+1
n).As an added advantage
the bounded probabilities are that gradients are well-behaved as the norm of the gradient balances
extreme probabilities.
3.4.2 Finetuning the model
We use BGE-lage-en-v1.5 as our base model on top of which we perform new words embedding
initialization explained in the previous step. The dimensions and sequence length for this model are
1024 and 512 respectively. Our reasons for using this model were two-fold,
• We wanted to identify a light embedding model that could be trained on a local laptop
• Model that is comparable in performance with other SOTA models.
As explained in previous sections our dataset consists of mapping of questions to relevant files, such
thatq7→ {p|1≤p≤5}. Figure 5 shows the examples for each value of n.
We use multiple negative ranking (MNR) loss functions in our framework as it’s proven its perfor-
mance in information retrieval tasks. This loss function is also helpful for training sets with only
positive pairs as in our case, also known as anchor positive pairs. This loss function uses each exam-
ple in a mini-batch as a negative sample for all other examples. For a given anchor-positive pair, the
loss is calculated based on the similarity of the anchor to its positive pair and the negative samples
(other examples in the batch). i.e. The loss function promotes learning embeddings where questions
are closer to their corresponding relevant context and farther from other contexts in the batch
The following steps are involved in the finetuning process
•Batch Preparation: For each training step, you prepare a batch of question-context pairs.
Let’s assume the batch size is N
•Loss Calculation: The loss for each question qiwith its relevant mapping of files as
context piin the batch is computed against all other contexts in the same batch, treated as
negative examples:
L(qi, pi) =log(esim(qi,pi)
PN
i=1esim, (qi,pi)), where sim(qi, pi)denotes the cosine similarity score
between the between the embeddings of question qiwith its relevant mapping of files as
context pi
7•Average Loss: The average loss over the batch is calculated, which we aim to minimize
Lbatch =1
NPN
i=1L(qi, pi)
Our optimization framework uses AdamW with a linear warmup scheduler. AdamW optimizer ex-
hibits superior performance in finetuning applications Kingma and Ba [2017], and is better equipped
to deal with sparse gradients than other optimizers.
Wt+1=W′
t+1+ηλW t
W′
t+1=Wt−η∗Bt
Where W′
t+1is Weight Update Without Weight Decay and Btis bias correction factor.
The linear warmup scheduler increases the learning rate ηlinearly during the warmup phase and
then decays ηlinearly to zero
Additionally, we use a batch size of 5 and finetune the model for 50 epochs. Because of the
lightweight nature of this model, we were able to finish finetuning this model on a Macbook M3max
chip with 64GB RAM and 40 core GPU in little less than 8 hours.
4 Evaluation
Due to the niche nature of the task; we use the test set created using the same role-playing approach
described in the previous section.
4.1 Baseline models
For benchmarking the performance of our approach we use the best-in-class embedding models
from both open and proprietary sources described below.
•SFR-Embedding-Mistral : An embedding model by salesforce which is built on top of
E5-mistral-7b-instruct and Mistral-7B-v0.1 models. It has embedding dimensions of 4096,
a context window of 32768, and a size of 14.22 GB
•text-embedding-3-large : This is the most recent and best-performing embedding model
from openAI. It has embedding dimensions of 3072 and a context window of 8191. Its size
is unknown as it’s a proprietary OpenAI model
•BGE-large-en-v1.5: This is a general-purpose embedding model published by BAAI. It
has embedding dimensions of 1024, a context window of 512, and has a size of 1.34 GB
4.2 Results
We analyze the results of the 4 models across a host of metrics relevant to our task i.e. we only
include metrics that can assess the performance for our specific retrieval task. We have excluded
metrics like Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (nDCG),
as the performance of the downstream task by the agent is dependent on getting the correct top-k set
of files while ignoring its ranking. Looking at the table we can see our finetuned model significantly
outperforms the benchmark models across all metrics. The performance is relatively better for n<=3,
which can be attributed to smaller embeddings and the context window of our model.
Embedding models are generally trained on a large and diverse corpus of text data; the nuances and
patterns of tabular and numeric data are generally ignored in the datasets. For domain-specific appli-
cations like tabular RAG, this generally leads to sub-optimal performance. The table below outlines
the results of 3 baseline models against our finetuned model across precision@10, recall@10, and hit
rate. We used a value of k= 10 as our applications generally need files in the range 1<=n <= 6
this k value should be sufficient to include all relevant files. In this study, we’re ignoring the rank
orders of the relevant files as the data science agent only needs relevant files and does not care about
rank ordering.
From table 1 we can see that our finetuned model significantly outperformed other baseline models
across all metrics. The model outperformed the next best model text-embedding-3-large across hit
rate, precision@10, and recall@10 quite comfortably,
8Table 1: Tabular RAG Performance
Evaluation Metric SFR-Embedding-Mistral text-embedding-3-large bge-large-en-v1.5 bge-large-en-v1.5-finetuned
Precision@10 0.2025 0.2041 0.1702 0.2160
Recall@10 0.7477 0.7578 0.6643 0.7989
Hit Rate@10 0.3892 0.3984 0.3100 0.4420
Table 2: Hit Rate@10 breakdown with N value
N SFR-Embedding-Mistral text-embedding-3-large bge-large-en-v1.5 bge-large-en-v1.5-finetuned Number of questions
1 0.7727 0.7784 0.75577 0.7955 176
2 0.7143 0.6786 0.5982 0.8482 112
3 0.3629 0.2823 0.1774 0.3871 124
4 0.1434 0.1434 0.0430 0.1720 279
5 0.0909 0.0450 0.0227 0.0682 44
6 0.0057 0.0057 0.0 0.0057 23
We see lower precision values in this table due to our choice of k= 10 , since the majority of
questions have relevant files at most 4 (more than 91 %), the number of false positives will be much
higher as all 10 files will be included in this evaluation. Similarly, the recall@10 values will be high
since k > n will mean more files from our RAG pipeline would be in the universe of relevant files.
Our measure of Hit rate@10 is calculated as follows
hitt=
1if files n⊆files k
0otherwise
Where files nis all relevant files for a question and files kare files from the RAG output ( k= 10
in our case)
We also provide the breakdown of hit rate@10 across different values of N; from Table 2 we see
the finetuned models outperform other embedding models for almost all values of n except n= 5
where the SFR-Embedding model performs better. This can be attributed to the large context size
and large embedding dimensions of this model which helps in better capturing the embedding of
questions requiring a large number of files to solve a particular question.
5 Conclusion
In this paper, we outlined why even SOTA embedding models trained for general purpose Retrieval
applications fail at task-specific applications. Tabular RAG is a very niche application that is critical
for hyper-scaling generative AI applications in the area of data science and finance. SOTA embed-
ding models fail at this application as they are trained on generic datasets, we proposed a novel
approach to this problem by finetuning a lightweight BGE-large-en model on our custom dataset.
We found that performing new word embedding initialization before finetuning improved the per-
formance of the model. The evaluation results showed that our model outperforms best-in-class
embedding models by a significant margin. We posit that for niche task-specific RAG applications
finetuning a light embedding model using our approach even on a local laptop would outperform
large SOTA embedding models like SFR-Embedding-Mistral and open-AI’s ext-embedding-3-large.
References
Gpt-4 technical report, 2024.
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Al-
bert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model
for mathematics, 2024.
Wenhu Chen, Ming-Wei Chang, Eva Schlinger, William Yang Wang, and William W. Cohen. Open
question answering over tables and text. In International Conference on Learning Representa-
tions , 2021. URL https://openreview.net/forum?id=MmCRswl1UYl .
Heng Gong, Yawei Sun, Xiaocheng Feng, Bing Qin, Wei Bi, Xiaojiang Liu, and Ting Liu.
TableGPT: Few-shot table-to-text generation with table structure reconstruction and content
9matching. In Donia Scott, Nuria Bel, and Chengqing Zong, editors, Proceedings of the 28th Inter-
national Conference on Computational Linguistics , pages 1978–1988, Barcelona, Spain (Online),
December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.
coling-main.179. URL https://aclanthology.org/2020.coling-main.179 .
Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Supryadi, Linhao Yu, Yan Liu, Jiaxuan
Li, Bojian Xiong, and Deyi Xiong. Evaluating large language models: A comprehensive survey,
2023.
John Hewitt. Initializing new word embeddings for pretrained language models. https://nlp.
stanford.edu/~johnhew/vocab-expansion.html , 2021.
Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong
Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucination in large
language models: Principles, taxonomy, challenges, and open questions, 2023.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe
Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.
Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou.
Tapex: Table pre-training via learning a neural sql executor, 2022.
Lincoln Murr, Morgan Grainger, and David Gao. Testing llms on code generation with varying
levels of prompt specificity, 2023.
Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,
and Caiming Xiong. Codegen: An open large language model for code with multi-turn program
synthesis, 2023.
OpenAI. Gpt-4 technical report, 2023, archivePrefix=arXiv, primaryClass=q-fin.ST.
Feifei Pan, Mustafa Canim, Michael Glass, Alfio Gliozzo, and James Hendler. End-to-end table
question answering via retrieval-augmented generation, 2022.
Microsoft Research. Sparks of artificial general intelligence:early experiments with gpt-4, 2023,
archivePrefix=arXiv, primaryClass=q-fin.ST.
Roee Shraga, Haggai Roitman, Guy Feigenblat, and Mustafa Canim. Ad hoc table retrieval using
intrinsic and extrinsic similarities. In Yennun Huang, Irwin King, Tie-Yan Liu, and Maarten van
Steen, editors, WWW ’20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 , pages
2479–2485. ACM / IW3C2, 2020. doi: 10.1145/3366423.3379995. URL https://doi.org/
10.1145/3366423.3379995 .
Heydar Soudani, Evangelos Kanoulas, and Faegheh Hasibi. Fine tuning vs. retrieval augmented
generation for less popular knowledge, 2024.
Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improv-
ing text embeddings with large language models, 2024.
Yazheng Yang, Yuqi Wang, Sankalok Sen, Lei Li, and Qi Liu. Unleashing the potential of large
language models for predictive tabular tasks in data science, 2024.
Mingtian Zhang, Shawn Lan, Peter Hayes, and David Barber. Mafin: Enhancing black-box embed-
dings with model augmented fine-tuning, 2024a.
Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. Retrieve anything to
augment large language models, 2023.
Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E.
Gonzalez. Raft: Adapting language model to domain specific rag, 2024b.
10Tianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun. Tablellama: Towards open large generalist
models for tables, 2024c.
Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling
Yang, Wentao Zhang, and Bin Cui. Retrieval-augmented generation for ai-generated content: A
survey, 2024.
11