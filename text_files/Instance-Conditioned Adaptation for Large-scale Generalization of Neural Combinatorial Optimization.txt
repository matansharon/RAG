Instance-Conditioned Adaptation for Large-scale Generalization
of Neural Combinatorial Optimization
Changliang Zhou* 1Xi Lin* 2Zhenkun Wang1Xialiang Tong3Mingxuan Yuan3Qingfu Zhang2
Abstract
The neural combinatorial optimization (NCO)
approach has shown great potential for solving
routing problems without the requirement of ex-
pert knowledge. However, existing constructive
NCO methods cannot directly solve large-scale
instances, which significantly limits their appli-
cation prospects. To address these crucial short-
comings, this work proposes a novel Instance-
Conditioned Adaptation Model (ICAM) for better
large-scale generalization of neural combinatorial
optimization. In particular, we design a powerful
yet lightweight instance-conditioned adaptation
module for the NCO model to generate better so-
lutions for instances across different scales. In
addition, we develop an efficient three-stage re-
inforcement learning-based training scheme that
enables the model to learn cross-scale features
without any labeled optimal solution. Experimen-
tal results show that our proposed method is ca-
pable of obtaining excellent results with a very
fast inference time in solving Traveling Salesman
Problems (TSPs) and Capacitated Vehicle Rout-
ing Problems (CVRPs) across different scales. To
the best of our knowledge, our model achieves
state-of-the-art performance among all RL-based
constructive methods for TSP and CVRP with up
to 1,000 nodes.
1. Introduction
The Vehicle Routing Problem (VRP) plays a crucial role in
various logistics and delivery applications, as its solution
directly affects transportation cost and service efficiency.
However, efficiently solving VRPs is a challenging task due
to their NP-hard nature. Over the past few decades, exten-
*Equal contribution1Southern University of Science and
Technology, Shenzhen, China2City University of Hong
Kong, Hong Kong SAR, China3Huawei Noahâ€™s Ark Lab,
Shenzhen, China. Correspondence to: Zhenkun Wang
<wangzhenkun90@gmail.com >.sive heuristic algorithms, such as LKH3 (Helsgaun, 2017)
and HGS (Vidal, 2022), have been proposed to address
different VRP variants. Although these approaches have
shown promising results for specific problems, the algo-
rithm designs heavily rely on expert knowledge and a deep
understanding of each problem. It is very difficult to de-
sign an efficient algorithm for a newly encountered problem
in real-world applications. Moreover, their required run-
time can increase exponentially as the problem size grows.
As a result, these limitations greatly hinder the practical
application of classical heuristic algorithms.
Over the past few years, different neural combinatorial op-
timization (NCO) methods have been explored to solve
various problems efficiently (Bengio et al., 2021; Li et al.,
2022). In this work, we focus on the constructive NCO
method (also known as the end-to-end method) that builds a
learning-based model to directly construct an approximate
solution for a given instance without the need for expert
knowledge (Vinyals et al., 2015; Kool et al., 2019; Kwon
et al., 2020). In addition, constructive NCO methods usually
have a faster runtime compared to classical heuristic algo-
rithms, making them a desirable choice to tackle real-world
problems with real-time requirements. Existing constructive
NCO methods can be divided into two categories: super-
vised learning (SL)-based (Vinyals et al., 2015; Xiao et al.,
2023) and reinforcement learning (RL)-based ones (Nazari
et al., 2018; Bello et al., 2016). The SL-based method re-
quires a lot of problem instances with labels (i.e., the optimal
solutions of these instances) as its training data. However,
obtaining sufficient optimal solutions for some complex
problems is unavailable, which impedes its practicality. RL-
based methods learn NCO models by interacting with the
environment without requiring labeled data. Nevertheless,
due to memory and computational constraints, it is unrealis-
tic to train the RL-based NCO model directly on large-scale
problem instances.
Current RL-based NCO methods typically train the model
on small-scale instances (e.g., with 100nodes) and then
attempt to generalize it to larger-scale instances (e.g., with
more than 1,000nodes) (Kool et al., 2019; Kwon et al.,
2020). Although these models demonstrate good perfor-
mance on instances of similar scales to the ones they were
1arXiv:2405.01906v1  [cs.AI]  3 May 2024Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization
trained on, they struggle to generate reasonable good solu-
tions for instances with much larger scales. Recently, two
different types of attempts have been explored to address the
crucial limitation of RL-based NCO on large-scale general-
ization. The first one is to perform an extra search procedure
on model inference to improve the quality of solution over
greedy generation (Hottung et al., 2022; Choo et al., 2022).
However, this approach typically requires expert-designed
search strategies and can be time-consuming when dealing
with large-scale problems. The second approach is to train
the model on instances of varying scales (Khalil et al., 2017;
Cao et al., 2021). The challenge of this approach lies in
effectively learning cross-scale features from these varying-
scale training data to enhance the modelâ€™s generalization
performance.
Some recent works reveal that incorporating auxiliary infor-
mation (e.g., the scales of the training instances) in training
can improve the modelâ€™s convergence efficiency and gen-
eralization performance. However, these methods incorpo-
rate auxiliary information into the decoding phase without
including the encoding phase. Although these methods
can improve the inference efficiency, the model fails to be
deeply aware of the auxiliary information, resulting in unsat-
isfactory generalization performance on large-scale problem
instances. In this work, we propose a powerful Instance-
Conditioned Adaptation Model (ICAM) to improve the
large-scale generalization performance for RL-based NCO.
Our contributions can be summarized as follows:
â€¢We design a novel and powerful instance-conditioned
adaptation module for RL-based NCO to efficiently
leverage the instance-conditioned information (e.g., in-
stance scale and distance between each node pair) to
generate better solutions across different scales. The
proposed module is lightweight with low computa-
tional complexity, which can further facilitate training
on instances with larger scales.
â€¢We develop a three-stage RL-based training scheme
across instances with different scales, which enables
our model to learn cross-scale features without any
labeled optimal solution.
â€¢We conduct various experiments on different routing
problems to demonstrate that our proposed ICAM can
generate promising solutions for cross-scale instances
with a very fast inference time. To the best of our
knowledge, it achieves state-of-the-art performance
among all RL-based constructive methods for CVRP
and TSP instances with up to 1,000nodes.
2. Related Works
Non-conditioned NCO Most NCO methods are trained
on a fixed scale (e.g., 100 nodes). These models usuallyperform well on the instances with the scale they are trained
on, but their performance could drop dramatically on in-
stances with different scales (Kwon et al., 2020; Xin et al.,
2020; 2021). To mitigate the poor generalization perfor-
mance, an extra search procedure is usually required to find
a better solution. Some widely used search methods in-
clude beam search (Joshi et al., 2019; Choo et al., 2022),
Monte Carlo tree search (MCTS) (Xing & Tu, 2020; Fu
et al., 2021; Qiu et al., 2022; Sun & Yang, 2023), and active
search (Bello et al., 2016; Hottung et al., 2022). However,
these procedures are very time-consuming, could still per-
form poorly on instances with quite different scales, and
might require expert-designed strategies on a specific prob-
lem (e.g., MCTS for TSP). Recently, some two-stage ap-
proaches, such as divide-and-conquer (Kim et al., 2021; Hou
et al., 2022) and local reconstruction (Li et al., 2021; Pan
et al., 2023; Cheng et al., 2023; Ye et al., 2023), have been
proposed. Although these methods have a better generaliza-
tion ability, they usually require expert-designed solvers and
ignore the dependency between the two stages, which makes
model design difficult, especially for non-expert users.
Varying-scale Training in NCO Directly training the
NCO model on instances with different scales is another
popular way to improve its generalization performance. This
straightforward approach can be traced back to Khalil et al.
(2017), which tries to train the model on instances with
50âˆ’100nodes to improve its generalization performance
to instances with up to 1,200nodes. Furthermore, Joshi et al.
(2020) systematically tests the generalization performance
of NCO models by training on different TSP instances
with20âˆ’50nodes. Subsequently, a series of works have
been developed to utilize the varying-scale training scheme
to improve their own NCO modelsâ€™ generalization perfor-
mance (Lisicki et al., 2020; Cao et al., 2021; Manchanda
et al., 2022; Gao et al., 2023; Zhou et al., 2023). Similar
to the varying-size training scheme, a few SL-based NCO
methods learn to construct partial solutions with various
sizes during training and achieve a robust generalization per-
formance on instances with different scales (Drakulic et al.,
2023; Luo et al., 2023). Nevertheless, in real-world applica-
tions, it could be very difficult to obtain high-quality labeled
solutions for SL-based model training. RL-based models
also face the challenge of efficiently capturing cross-scale
features from varying-scale training data, which severely
hinders their generalization ability on large-scale problems.
Information-aware NCO Recently, several works have
indicated that incorporating auxiliary information (e.g., the
distance between each pair of nodes) can facilitate model
training and improve generalization performance. In Kim
et al. (2022b), the scale-related feature is added to the de-
coderâ€™s context embedding to make the model scale-aware
during the decoding phase. Jin et al. (2023), Son et al.
2Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization
(2023) and Wang et al. (2024) use the distance to bias the
compatibility calculation, thereby guiding the model toward
more efficient exploration. Gao et al. (2023) employs a local
policy network to catch distance and scale knowledge and
integrates it into the compatibility calculation. In Li et al.
(2023), the distance-related feature is utilized to adaptively
refine the node embeddings so as to improve the model ex-
ploration. Overall, these methods all incorporate auxiliary
information into the decoding process to improve the infer-
ence efficiency. However, this additional plugin way cannot
efficiently integrate auxiliary information into the encoding
of nodes and fails to enable the model to be deeply aware of
the knowledge of distance and scale.
Current node Other nodes High -preference set
Small -scale instance
(100 nodes)
Large -scale instance
(1,000 nodes)
Figure 1. Node Selection Bias on Instances with Different Scales .
As the scale increases, the density of nodes increases. Therefore,
the model tends to select the next node from a smaller sub-region of
the current node when dealing with large-scale instances compared
to small-scale ones.
3. Instance-Conditioned Adaptation
3.1. Motivation and Key Idea
Each instance has some specific information that benefits
the adaptability and generalization of models. By providing
this instance-conditioned information, the model can bet-
ter comprehend and address the problems, especially when
dealing with large-scale problems. The node-to-node dis-
tances and scale are two fundamental kinds of information
in routing problems, and both types of information are vi-
tal. We need to make the model aware of scale changes to
improve generalization. Meanwhile, we also need to allow
the model to be aware of the node-to-node distances to en-
hance exploration and reduce the search space, which in
turn improves its training efficiency.
As the example of the two TSP instances shown in Figure 1,
the two adjacent nodes in the optimal solution are normally
within a specific sub-region, and the distance between them
should not be too far. Likewise, the model tends to select
the next node from a sub-region of the current node. For
the node outside the sub-region, the farther it is from the
current node, the lower the corresponding selection bias
Distance 
MatrixScale
ğ‘=5Instance -Conditioned Information
Decoder Encoder ğ»(ğ¿)
LÃ—
NCO ModelFigure 2. Intuitive Idea of Instance-Conditioned Adaptation.
By providing instance-conditioned information in both the en-
coding and decoding processes, the model is expected to better
comprehend and address problem instances.
is. In addition, the nodes of instances with different scales
exhibit significant variations in density. The scale is larger,
and the corresponding node distribution is denser. Therefore,
the model bias should vary according to the scale.
Instance-Conditioned Adaptation Function This work
proposes to integrate the scale and node-to-node distances
via an instance-conditioned adaptation function. We de-
note the function as f(N, d ij), where Nis the scale of the
problem instance, and dijrepresents the distance between
each node iand each node j. As shown in Figure 2, the
f(N, d ij)aims to capture features related to instance scale
and node-to-node distances, and feed them into the modelâ€™s
encoding and decoding processes, respectively. Based on
the changing instances, the model could dynamically bias
the selection of nodes under the effect of f(N, d ij), thereby
making better decisions in RL-based training. To enable
f(N, d ij)to learn better features of large-scale general-
ization, we still need improvements in the following two
aspects:
â€¢Lightweight and Fast Model: As the RL-based
training on large-scale instances consumes enormous
computational time and memory, we need a more
lightweight yet quick model structure so that large-
scale instances can be included in the training data;
â€¢Efficient Training Scheme: We need more efficient
training schemes to accelerate model convergence,
especially when training on large-scale problem in-
stances.
3.2. Instance-Conditioned Adaptation Model
As shown in Figure 3, the proposed ICAM also adopts the
encoder-decoder structure, which is Transformer-like as
3Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization
ğœ½ğ‘’ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ ğœ½ğ‘‘ğ‘’ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿğ‘“(ğ‘,ğ‘‘ğ‘–ğ‘—) ğ‘=5Distance Matrix Scale
ğ‘‘11ğ‘‘12â€¦ğ‘‘1ğ‘
ğ‘‘21ğ‘‘22â€¦ğ‘‘2ğ‘
â€¦â€¦â€¦â€¦
ğ‘‘ğ‘1ğ‘‘ğ‘2â€¦ğ‘‘ğ‘ğ‘Compatibility with 
Adaptation BiasAdaptation Attention 
Free Module
Figure 3. The Proposed ICAM. In our model, two essential types of instance-conditioned information are integrated into both the encoder
and the decoder. Specifically, we utilize AAFM to replace all MHA operations in both the encoder and decoder. Moreover, we combine
the adaptation function with the compatibility calculation in the decoder.
many existing NCO models (Kool et al., 2019; Kim et al.,
2022a; Luo et al., 2023). It is developed from a very well-
known NCO model POMO (Kwon et al., 2020), and the
details of POMO are provided in Appendix A.
Given an instance X={xi}N
i=1,xirepresents the features
of each node (e.g., the coordinates of each city in TSP).
These node features are transformed into the initial embed-
dings H(0)= (h(0)
1, . . . , h(0)
N)via a linear projection. The
initial embeddings pass through the Lattention layers to
get the node embeddings H(L)= (h(L)
1, . . . , h(L)
N). The
attention layer consists of a Multi-Head Attention (MHA)
sub-layer and a Feed-Forward (FF) sub-layer.
During the decoding process, the model generates a solution
in an autoregressive manner. For the example of TSP, in the
t-step construction, the context embedding is composed of
the first visited node embedding and the last visited node
embedding, i.e., ht
(C)= [h(L)
Ï€1,h(L)
Ï€tâˆ’1]. The new context
embedding Ë†ht
(C)is then obtained via the MHA operation
onht
(C)andH(L). Finally, the model yields the selection
probability for each unvisited node pÎ¸(Ï€t=i|X, Ï€ 1:tâˆ’1)
by calculating compatibility on Ë†ht
(C)andH(L).
Adaptation Attention Free Module The MHA operation
is the core component of the Transformer-like NCO model.
In the mode of self-attention, MHA performs a scaled dot-
product attention for each head, the self-attention calculation
can be written as
Q=XWQ, K =XWK, V =XWV, (1)Attention( Q, K, V ) = softmaxQKT
âˆšdk
V, (2)
where Xrepresents the input, WQ,WK, andWVare three
learning matrices, dkis the dimension for K. The MHA
incurs the primary memory usage and computational cost,
which poses challenges for training on large-scale instances.
Moreover, the specific design of the MHA makes it difficult
to intuitively integrate the instance-conditioned information
(i.e., the adaptation function f(N, d ij)).
We propose a novel module called Adaptation Attention
FreeModule (AAFM), as shown in Figure 4, to replace
the MHA operation in both the encoder and the decoder.
AAFM is based on the AFT-full operation of the Attention
Free Transformer (AFT) (Zhai et al., 2021), which has lower
computation and space complexity but can achieve similar
performance to the MHA. The details of the AFT are pro-
vided in Appendix B. We substitute the original bias wof
AFT-full with our adaptation function f(N, d ij), i.e.,
AAFM( Q, K, V, A ) =Ïƒ(Q)âŠ™exp(A)Â·(exp( K)âŠ™V)
exp(A)Â·exp(K),
(3)
where Ïƒis Sigmoid function, âŠ™represents the element-wise
product, and Aij=f(N, d ij)denotes the adaptation bias
between node iand node j. The detailed calculation of
AAFM is shown in Figure 5.
In AAFM, instance-conditioned information is integrated
in a more appropriate and ingenious manner, which enables
our model to comprehend knowledge such as distance and
scale more efficiently. Furthermore, our AAFM exhibits
lower computation and space complexity than MHA, which
4Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization
ğ´11ğ´12â€¦ğ´1ğ‘
ğ´21ğ´22â€¦ğ´2ğ‘
â€¦â€¦â€¦â€¦
ğ´ğ‘1ğ´ğ‘2â€¦ğ´ğ‘ğ‘Linear Linear Linear
ğ¾âˆˆâ„ğ‘Ã—dh ğ‘‰âˆˆâ„ğ‘Ã—dh ğ‘„âˆˆâ„ğ‘Ã—dh ğ´âˆˆâ„ğ‘Ã—ğ‘
Adaptation Attention Free Module
ğ‘Œâˆˆâ„ğ‘Ã—dhğ¡ğŸ(â„“âˆ’1)ğ¡ğŸ(â„“âˆ’1)â€¦ğ¡ğ‘µ(â„“âˆ’1)Scale
Node -to-node DistancesX âˆˆâ„ğ‘Ã—2
Figure 4. The Structure of AAFM. Note that in decoder, dijin
Aijis the distance between the current node and each node, and the
node masking state in current step tis added to Aijadditionally.
could bring a more lightweight and faster model.
Compatibility with Adaptation Bias We also integrate
the adaptation function f(N, d ij)into the compatibility cal-
culation. The new compatibility, denoted as ut
i, can be
expressed as
ut
i=ï£±
ï£²
ï£³Î¾Â·tanh(Ë†ht
(C)(h(L)
i)T
âˆšdk+Atâˆ’1,i)ifiÌ¸âˆˆ {Ï€1:tâˆ’1}
âˆ’âˆ otherwise,
(4)
pÎ¸(Ï€t=i|X, Ï€ 1:tâˆ’1) =eut
i
PN
j=1eut
j, (5)
where Î¾is the clipping parameter, Ë†ht
(C)andh(L)
iare cal-
culated via AAFM instead of MHA. Atâˆ’1,irepresents the
adaptation bias between each remaining node and the cur-
rent node. Finally, the probability of generating a complete
solution Ï€for instance Xis calculated as
pÎ¸(Ï€|X) =NY
t=2pÎ¸(Ï€t|X, Ï€ 1:tâˆ’1). (6)
By integrating the adaptation bias in the compatibility cal-
culation, the modelâ€™s performance can be further enhanced.
3.3. Varying-scale Training Scheme
We develop a three-stage training scheme to enable the
model to be aware of instance-conditioned information more
effectively. We describe the three training stages as follows:
Stage 1: Warming-up on Small-scale Instances We em-
ploy a warm-up procedure in the first stage. Initially, the
ğ‘„ ğ¾ ğ‘‰
Sigmoid
MulExp Exp
MatMul Mul+Mask (opt.)ğ´
MatMul
DivFigure 5. The Detailed Calculation Process of AAFM.
model is trained for several epochs on small-scale instances.
For example, we use a total of 256,000randomly generated
TSP100 instances for each epoch in the first stage to train
the model for 100epochs. A warm-up training can make
the model more stable in the subsequent training.
Stage 2: Learning on Varying-scale Instances In the sec-
ond stage, we train the model on varying-scale instances for
much longer epochs. We let the scale Nbe randomly sam-
pled from the discrete uniform distribution Unif ([100,500])
for each batch. Considering the GPU memory constraints,
we decrease the batch size with the scale increases. The loss
function (denoted as LPOMO ) used in the first and second
stages is the same as in POMO. The gradient ascent with an
approximation of the loss function can be written as
âˆ‡Î¸LPOMO (Î¸)â‰ˆ1
BNBX
m=1NX
i=1Gm,iâˆ‡Î¸logpÎ¸ 
Ï€i|Xm
,
(7)
Gm,i=R 
Ï€i|Xm
âˆ’bi(Xm), (8)
bi(Xm) =1
NNX
j=1R 
Ï€j|Xm
for all i. (9)
where R 
Ï€i|Xm
represents the return (e.g., tour length)
of instance Xmgiven a specific solution Ï€i. Equation (9) is
a shared baseline as introduced in Kwon et al. (2020).
Stage 3: Top- kElite Training Under the POMO struc-
ture,Ntrajectories are constructed in parallel for each in-
stance during training. In the third stage, we want the model
to focus more on the best ktrajectories among all Ntrajec-
tories. To achieve this, we design a new loss LTop, and its
5Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization
gradient ascent can be expressed as
âˆ‡Î¸LTop(Î¸)â‰ˆ1
BkBX
m=1kX
i=1Gm,iâˆ‡Î¸logpÎ¸ 
Ï€i|Xm
.
(10)
We combine LTopwithLPOMO as the joint loss in the
training of the third stage, i.e.,
LJoint=LPOMO +Î²Â· LTop. (11)
where Î²âˆˆ[0,1]is a coefficient balancing the original loss
and the new loss.
4. Experiments
In this section, we conduct a comprehensive comparison be-
tween our proposed model and other classical and learning-
based solvers using Traveling Salesman Problem (TSP) and
Capacitated Vehicle Routing Problem (CVRP) instances of
different scales.
Problem Setting For TSP and CVRP, the instances of
training and testing are generated randomly, following Kool
et al. (2019). For the test set, we generate 10,000instances
for100-node, and 128instances for each of 200-,500-, and
1,000-node, respectively. Specifically, for CVRP instances
of different scales, we use capacities of 50,80,100, and
250, respectively (Drakulic et al., 2023; Luo et al., 2023).
Model Setting The adaptation function f(N, d ij)should
be problem-depended. For TSP and CVRP, we define it as
f(N, d ij) =âˆ’Î±Â·log2NÂ·dijâˆ€i, jâˆˆ1, . . . , N, (12)
where Î±is a learnable parameter, and it is initially set to 1.
The embedding dimension of our model is set to 128, and
the dimension of the feed-forward layer is set to 512. We
set the number of attention layers in the encoder to 12. The
clipping parameter Î¾= 50 in Equation (4) to obtain the
better training convergence (Jin et al., 2023). We train and
test all experiments using a single NVIDIA GeForce RTX
3090 GPU with 24GB memory.
Training For all models, we use Adam (Kingma & Ba,
2014) as the optimizer, initial learning rate Î·is10âˆ’4. Every
epoch, we process 1,000batches for all problems. For each
instance, Ndifferent tours are always generated in parallel,
each of them starting from a different city (Kwon et al.,
2020). The rest of the training settings are as follows:
1.In the first stage of the process, we set different batch
sizes for different problems due to memory constraints:
256for TSP and 128for CVRP. We use problem in-
stances for TSP 100and CVRP 100to train the cor-
responding model for 100epochs. Additionally, the
capacity for each CVRP instance is fixed at 50.2.In the second stage, the scale Nis randomly sampled
from the discrete uniform distribution Unif([100,500])
and optimize memory usage by adjusting batch sizes
according to the changed scales. For TSP, the batch
sizebs=
160Ã—(100
N)2
, with a training duration of
2,200 epochs. In the case of CVRP, the batch size
bs=
128Ã—(100
N)2
, with a training duration of 700
epochs. Furthermore, the capacity of each batch is
consistently set by random sampling from the discrete
uniform distribution Unif ([50,100]).
3.In the last stage, we adjust the learning rate Î·to10âˆ’5
across all models to enhance model convergence and
training stability. The parameter Î²andÎ»are set to
0.1and20, respectively, as specified in Equation (10)
and Equation (11). The training period is standardized
to200epochs for all models, and other settings are
consistent with the second stage.
Overall, we train the TSP model for 2,500epochs and the
CVRP model for 1,000epochs. For more details on model
hyperparameter settings, please refer to Appendix C.
Baseline We compare ICAM with the following methods:
1.Classical solver : Concorde (Applegate et al., 2006),
LKH3 (Helsgaun, 2017) and HGS (Vidal, 2022);
2.Constructive NCO : POMO (Kwon et al., 2020),
MDAM (Xin et al., 2021), ELG (Gao et al., 2023),
Pointerformer (Jin et al., 2023), BQ (Drakulic et al.,
2023) and LEHD (Luo et al., 2023);
3.Two-stage NCO : Att-GCN+MCTS (Fu et al., 2021),
DIMES (Qiu et al., 2022), TAM (Hou et al., 2022),
SO (Cheng et al., 2023), DIFUSCO (Sun & Yang,
2023), H-TSP (Pan et al., 2023) and GLOP (Ye et al.,
2023).
Metrics and Inference We use the solution lengths, op-
timality gaps, and total inference times to evaluate the per-
formance of each method. Specifically, the optimality gap
measures the discrepancy between the solutions generated
by learning and non-learning methods and the optimal so-
lutions, which are obtained using Concorde for TSP and
LKH3 for CVRP. Note that the inference times for classi-
cal solvers, which run on a single CPU, and for learning-
based methods, which utilize GPUs, are inherently different.
Therefore, these times should not be directly compared.
For most NCO baseline methods, we directly execute the
source code provided by the authors with default settings.
We report the original results as published in correspond-
ing papers for methods like Att-GCN+MCTS, DIMES, SO.
Following the approach in Kwon et al. (2020), we report
6Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization
Table 1. Experimental results on TSP and CVRP with uniformly distributed instances. The results marked with an asterisk (*) are directly
obtained from the original papers.
TSP100 TSP200 TSP500 TSP1000
Method Obj. Gap Time Obj. Gap Time Obj. Gap Time Obj. Gap Time
Concorde 7.7632 0.000% 34m 10.7036 0.000% 3m 16.5215 0.000% 32m 23.1199 0.000% 7.8h
LKH3 7.7632 0.000% 56m 10.7036 0.000% 4m 16.5215 0.000% 32m 23.1199 0.000% 8.2h
Att-GCN+MCTS* 7.7638 0.037% 15m 10.8139 0.884% 2m 16.9655 2.537% 6m 23.8634 3.224% 13m
DIMES AS+MCTS* âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ 16.84 1.76% 2.15h 23.69 2.46% 4.62h
SO-mixed* âˆ’ âˆ’ âˆ’ 10.7873 0.636% 21.3m 16.9431 2.401% 32m 23.7656 2.800% 55.5m
DIFUSCO greedy+2-opt* 7.78 0.24% âˆ’ âˆ’ âˆ’ âˆ’ 16.80 1.49% 3.65m 23.56 1.90% 12.06m
H-TSP âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ 17.549 6.220% 23s 24.7180 6.912% 47s
GLOP (more revisions) 7.7668 0.046% 1.9h 10.7735 0.653% 42s 16.8826 2.186% 1.6m 23.8403 3.116% 3.3m
BQ greedy 7.7903 0.349% 1.8m 10.7644 0.568% 9s 16.7165 1.180% 46s 23.6452 2.272% 1.9m
LEHD greedy 7.8080 0.577% 27s 10.7956 0.859% 2s 16.7792 1.560% 16s 23.8523 3.168% 1.6m
MDAM bs50 7.7933 0.388% 21m 10.9173 1.996% 3m 18.1843 10.065% 11m 27.8306 20.375% 44m
POMO aug Ã—8 7.7736 0.134% 1m 10.8677 1.534% 5s 20.1871 22.187% 1.1m 32.4997 40.570% 8.5m
ELG aug Ã—8 7.7988 0.458% 5.1m 10.8400 1.274% 17s 17.1821 3.998% 2.2m 24.7797 7.179% 13.7m
Pointerformer aug Ã—8 7.7759 0.163% 49s 10.7796 0.710% 11s 17.0854 3.413% 53s 24.7990 7.263% 6.4m
ICAM single trajec. 7.8328 0.897% 2s 10.8255 1.139% <1s 16.7777 1.551% 1s 23.7976 2.931% 2s
ICAM 7.7991 0.462% 5s 10.7753 0.669% <1s 16.6978 1.067% 4s 23.5608 1.907% 28s
ICAM aug Ã—8 7.7747 0.148% 37s 10.7385 0.326% 3s 16.6488 0.771% 38s 23.4854 1.581% 3.8m
CVRP100 CVRP200 CVRP500 CVRP1000
Method Obj. Gap Time Obj. Gap Time Obj. Gap Time Obj. Gap Time
LKH3 15.6465 0.000% 12h 20.1726 0.000% 2.1h 37.2291 0.000% 5.5h 37.0904 0.000% 7.1h
HGS 15.5632 -0.533% 4.5h 19.9455 -1.126% 1.4h 36.5611 -1.794% 4h 36.2884 -2.162% 5.3h
GLOP-G (LKH3) âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ 39.6507 6.903% 1.7m
BQ greedy 16.0730 2.726% 1.8m 20.7722 2.972% 10s 38.4383 3.248% 47s 39.2757 5.892% 1.9m
LEHD greedy 16.2173 3.648% 30s 20.8407 3.312% 2s 38.4125 3.178% 17s 38.9122 4.912% 1.6m
MDAM bs50 15.9924 2.211% 25m 21.0409 4.304% 3m 41.1376 10.498% 12m 47.4068 27.814% 47m
POMO aug Ã—8 15.7544 0.689% 1.2m 21.1542 4.866% 6s 44.6379 19.901% 1.2m 84.8978 128.894% 9.8m
ELG aug Ã—8 15.9973 2.242% 6.3m 20.7361 2.793% 19s 38.3413 2.987% 2.6m 39.5728 6.693% 15.6m
ICAM single trajec. 16.1868 3.453% 2s 20.7509 2.867% <1s 37.9594 1.962% 1s 38.9709 5.070% 2s
ICAM 15.9386 1.867% 7s 20.5185 1.715% 1s 37.6040 1.007% 5s 38.4170 3.577% 35s
ICAM aug Ã—8 15.8720 1.442% 47s 20.4334 1.293% 4s 37.4858 0.689% 42s 38.2370 3.091% 4.5m
Table 2. Empirical results on CVRPLib Set-X(Uchoa et al., 2017).
BKS refers to the â€œBest Known Solutionâ€. The results marked
with an asterisk (*) are directly obtained from the original papers.
Nâ‰¤200 200<Nâ‰¤500 500<Nâ‰¤1000 Total Avg.time
(22 instances) (46 instances) (32 instances) (100 instances)
BKS 0.00% 0.00% 0.00% 0.00% âˆ’
LEHD 11.35% 9.45% 17.74% 12.52% 1.58s
BQ* âˆ’ âˆ’ âˆ’ 9.94% âˆ’
POMO 9.76% 19.12% 57.03% 29.19% 0.35s
ELG 5.50% 5.67% 5.74% 5.66% 0.61s
ICAM 5.14% 4.44% 5.17% 4.83% 0.34s
three types of results: those using a single trajectory, the
best result from multiple trajectories, and results derived
from instance augmentation.
Experimental Results The experimental results on uni-
formly distributed instances are reported in Table 1. Our
method stands out for consistently delivering superior infer-
ence performance, complemented by remarkably fast infer-
ence times, across various problem instances. Although it
cannot surpass Att-GCN+MCTS on TSP100 and POMO on
CVRP100, the time it consumes is significantly less. Att-GCN+MCTS takes 15minutes compared to our 37seconds.
On TSP1000, our model impressively reduces the optimality
gap to less than 3% in just 2seconds. When switching to a
multi-greedy strategy, the optimality gap further narrows to
1.9% in 30seconds. With the instance augmentation, ICAM
can achieve the optimality gap of 1.58% in less than 4min-
utes. To the best of our knowledge, for TSP and CVRP up to
1,000nodes, our model shows state-of-the-art performance
among all RL-based constructive NCO methods.
Results on Benchmark Dataset We further evaluate the
performance of each method using the well-known bench-
mark datasets from CVRPLib Set-X (Uchoa et al., 2017).
In these evaluations, instance augmentation is not employed
for any of the methods. The detailed results are presented
in Table 2, showing that our method consistently maintains
the best performance. ICAM achieves the best performance
across instances of all scale ranges. Among all learning-
based models, ICAM has the fastest inference time. This
also shows the outstanding generalization performance of
ICAM. According to our knowledge, in the Set-X tests, our
method has achieved the best performance to date.
7Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization
Table 3. Comparative results in the capacity setting of TAM (Hou et al., 2022) with scale â‰¥1000. â€œTimeâ€ represents the per-instance
runtime. The results marked with an asterisk (*) are directly obtained from the original papers. Note that except for CVRP3000 and
CVRP4000, the optimal solutions obtained using LKH3 are from the original TAM paper.
CVRP1K CVRP2K CVRP3K CVRP4K CVRP5K
Method Obj. Gap Time(s) Obj. Gap Time(s) Obj. Gap Time(s) Obj. Gap Time(s) Obj. Gap Time(s)
LKH3 46.44 0.000% 6.15 64.93 0.000% 20.29 89.90 0.000% 41.10 118.03 0.000% 80.24 175.66 0.000% 151.64
TAM-AM* 50.06 7.795% 0.76 74.31 14.446% 2.2 âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ 172.22 -1.958% 11.78
TAM-LKH3* 46.34 -0.215% 1.82 64.78 -0.231% 5.63 âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ 144.64 -17.659% 17.19
TAM-HGS* âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ 142.83 -18.690% 30.23
GLOP-G (LKH3) 45.90 -1.163% 0.92 63.02 -2.942% 1.34 88.32 -1.758% 2.12 114.20 -3.245% 3.25 140.35 -20.101% 4.45
LEHD greedy 43.96 -5.340% 0.79 61.58 -5.159% 5.69 86.96 -3.270% 18.39 112.64 -4.567% 44.28 138.17 -21.342% 87.12
BQ greedy 44.17 -4.886% 0.55 62.59 -3.610% 1.83 88.40 -1.669% 4.65 114.15 -3.287% 11.50 139.84 -20.389% 27.63
ICAM single trajec. 43.58 -6.158% 0.02 62.38 -3.927% 0.04 89.06 -0.934% 0.10 115.09 -2.491% 0.19 140.25 -20.158% 0.28
ICAM 43.07 -7.257% 0.26 61.34 -5.529% 2.20 87.20 -3.003% 6.42 112.20 -4.939% 15.50 136.93 -22.048% 29.16
Comparison on Larger-scale Instances We also conduct
experiments on instances for TSP and CVRP with larger
scales, the instance augmentation is not employed for all
methods due to computational efficiency. For CVRP, fol-
lowing Hou et al. (2022), the capacities for instances with
1,000, 2,000, and larger scales are set at 200, 300, and 300,
respectively. We perform our model on the dataset gen-
erated by the same settings. Except for CVRP3000 and
CVRP4000 instances where LKH3 is used to obtain their
optimal solution, the optimal solutions of other instances
are from the original paper.
As shown in Table 3, on CVRP instances with scale â‰¥1000,
our method outperforms the other methods, including GLOP
with LKH3 solver and all TAM variants, on all problem in-
stances except for CVRP3000. On CVRP3000, ICAM is
slightly worse than LEHD. LEHD is an SL-based model and
consumes much more solving time than ICAM. As shown
in Appendix D, the superiority of ICAM is not so obvious
on TSP instances with scale >1,000. Its performance is
slightly worse than the two SL-based NCO models, BQ and
LEHD. Nevertheless, on TSP 2000 and TSP 3000 instances,
we achieve the best results in RL-based constructive meth-
ods, and on TSP 4000 and TSP 5000 instances, we are only
slightly worse than ELG. Overall, our method still has a
good large-scale generalization ability.
5. Ablation Study
ICAM vs. POMO To improve the modelâ€™s ability to
be aware of scale, we implement a varying-scale training
scheme. Given that our model is an advancement over the
POMO framework, we ensure a fair comparison by training
a new POMO model using our training settings. The results
of the two models are reported in Appendix E.1.
Effects of Different Stages Our training is divided into
three different stages, each contributing significantly to
the overall effectiveness. The performance improvements
achieved at each stage are detailed in Appendix E.2.Effects of Adaptation Function Given that we apply the
adaptation function outlined in Equation (12) to both the
AAFM and the subsequent compatibility calculation, we
conducted three different experiments to validate the effi-
cacy of this function. Detailed results of these experiments
are available in Appendix E.3.
Parameter Settings in Stage 3 In the final stage, we man-
ually adjust the Î²andkvalues as specified in Equation (10).
The experimental results for two settings, involving different
values, are presented in Appendix E.4.
Efficient Inference Strategies for Different Models To
further improve model performance, many inference strate-
gies are developed for different NCO models. For example,
BQ employs beam search, while LEHD uses the Random
Re-Construct (RRC) in inference. We investigate the effects
of the inference strategy on different models. The analysis
is provided in Appendix E.5.
6. Conclusion, Limitation, and Future Work
In this work, we have proposed a novel ICAM to improve
large-scale generalization for RL-based NCO. The instance-
conditioned information is more effectively integrated into
the modelâ€™s encoding and decoding via a powerful yet
lightweight AAFM and the new compatibility calculation.
In addition, we have developed a three-stage training scheme
that enables the model to learn cross-scale features more effi-
ciently. The experimental results on various TSP and CVRP
instances show that ICAM achieves promising generaliza-
tion abilities compared with other representative methods.
ICAM demonstrates superior performance with greedy de-
coding. However, we have observed its poor applicability
to other complex inference strategies (e.g., RRC and beam
search). In the future, we may develop a suitable inference
strategy for ICAM. Moreover, the generalization perfor-
mance of ICAM over differently distributed datasets should
be investigated in the future.
8Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization
Impact Statement
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.
References
Applegate, D., Bixby, R., Chvatal, V ., and Cook, W. Con-
corde tsp solver, 2006.
Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.
arXiv preprint arXiv:1607.06450 , 2016.
Bello, I., Pham, H., Le, Q. V ., Norouzi, M., and Bengio,
S. Neural combinatorial optimization with reinforcement
learning. arXiv preprint arXiv:1611.09940 , 2016.
Bengio, Y ., Lodi, A., and Prouvost, A. Machine learning
for combinatorial optimization: a methodological tour
dâ€™horizon. European Journal of Operational Research ,
290(2):405â€“421, 2021.
Cao, Y ., Sun, Z., and Sartoretti, G. Dan: Decentral-
ized attention-based neural network for the minmax
multiple traveling salesman problem. arXiv preprint
arXiv:2109.04205 , 2021.
Cheng, H., Zheng, H., Cong, Y ., Jiang, W., and Pu, S. Select
and optimize: Learning to aolve large-scale tsp instances.
InInternational Conference on Artificial Intelligence and
Statistics , pp. 1219â€“1231. PMLR, 2023.
Choo, J., Kwon, Y .-D., Kim, J., Jae, J., Hottung, A., Tierney,
K., and Gwon, Y . Simulation-guided beam search for
neural combinatorial optimization. Advances in Neural
Information Processing Systems , 35:8760â€“8772, 2022.
Drakulic, D., Michel, S., Mai, F., Sors, A., and Andreoli, J.-
M. Bq-nco: Bisimulation quotienting for efficient neural
combinatorial optimization. In Thirty-seventh Conference
on Neural Information Processing Systems , 2023.
Fu, Z.-H., Qiu, K.-B., and Zha, H. Generalize a small
pre-trained model to arbitrarily large tsp instances. In
Proceedings of the AAAI Conference on Artificial Intelli-
gence , volume 35, pp. 7474â€“7482, 2021.
Gao, C., Shang, H., Xue, K., Li, D., and Qian, C. To-
wards generalizable neural solvers for vehicle routing
problems via ensemble with transferrable local policy.
arXiv preprint arXiv:2308.14104 , 2023.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition ,
pp. 770â€“778, 2016.Helsgaun, K. An extension of the lin-kernighan-helsgaun
tsp solver for constrained traveling salesman and vehicle
routing problems. Roskilde: Roskilde University , 12,
2017.
Hottung, A., Kwon, Y .-D., and Tierney, K. Efficient ac-
tive search for combinatorial optimization problems. In
International Conference on Learning Representations ,
2022.
Hou, Q., Yang, J., Su, Y ., Wang, X., and Deng, Y . Generalize
learned heuristics to solve large-scale vehicle routing
problems in real-time. In The Eleventh International
Conference on Learning Representations , 2022.
Ioffe, S. and Szegedy, C. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
InInternational Conference on Machine Learning , pp.
448â€“456. PMLR, 2015.
Jin, Y ., Ding, Y ., Pan, X., He, K., Zhao, L., Qin, T., Song, L.,
and Bian, J. Pointerformer: Deep reinforced multi-pointer
transformer for the traveling salesman problem. In The
Thirty-Seventh AAAI Conference on Artificial Intelligence ,
2023.
Joshi, C. K., Laurent, T., and Bresson, X. An efficient
graph convolutional network technique for the travelling
salesman problem. arXiv preprint arXiv:1906.01227 ,
2019.
Joshi, C. K., Cappart, Q., Rousseau, L.-M., and Lau-
rent, T. Learning the travelling salesperson prob-
lem requires rethinking generalization. arXiv preprint
arXiv:2006.07054 , 2020.
Khalil, E., Dai, H., Zhang, Y ., Dilkina, B., and Song,
L. Learning combinatorial optimization algorithms over
graphs. Advances in Neural Information Processing Sys-
tems, 30, 2017.
Kim, M., Park, J., et al. Learning collaborative policies
to solve np-hard routing problems. Advances in Neural
Information Processing Systems , 34:10418â€“10430, 2021.
Kim, M., Park, J., and Park, J. Sym-nco: Leveraging
symmetricity for neural combinatorial optimization. Ad-
vances in Neural Information Processing Systems , 35:
1936â€“1949, 2022a.
Kim, M., Son, J., Kim, H., and Park, J. Scale-conditioned
adaptation for large scale combinatorial optimization. In
NeurIPS 2022 Workshop on Distribution Shifts: Connect-
ing Methods and Applications , 2022b.
Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980 , 2014.
9Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization
Kool, W., van Hoof, H., and Welling, M. Attention, learn to
solve routing problems! In International Conference on
Learning Representations , 2019.
Kwon, Y .-D., Choo, J., Kim, B., Yoon, I., Gwon, Y ., and
Min, S. Pomo: Policy optimization with multiple optima
for reinforcement learning. Advances in Neural Informa-
tion Processing Systems , 33:21188â€“21198, 2020.
Li, B., Wu, G., He, Y ., Fan, M., and Pedrycz, W. An
overview and experimental study of learning-based op-
timization algorithms for the vehicle routing problem.
IEEE/CAA Journal of Automatica Sinica , 9(7):1115â€“
1138, 2022.
Li, J., Ma, Y ., Cao, Z., Wu, Y ., Song, W., Zhang, J., and
Chee, Y . M. Learning feature embedding refiner for
solving vehicle routing problems. IEEE Transactions on
Neural Networks and Learning Systems , 2023.
Li, S., Yan, Z., and Wu, C. Learning to delegate for large-
scale vehicle routing. Advances in Neural Information
Processing Systems , 34:26198â€“26211, 2021.
Lisicki, M., Afkanpour, A., and Taylor, G. W. Evaluating
curriculum learning strategies in neural combinatorial
optimization. arXiv preprint arXiv:2011.06188 , 2020.
Luo, F., Lin, X., Liu, F., Zhang, Q., and Wang, Z. Neural
combinatorial optimization with heavy decoder: Toward
large scale generalization. In Thirty-seventh Conference
on Neural Information Processing Systems , 2023.
Manchanda, S., Michel, S., Drakulic, D., and Andreoli, J.-M.
On the generalization of neural combinatorial optimiza-
tion heuristics. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases , pp.
426â€“442. Springer, 2022.
Nazari, M., Oroojlooy, A., Snyder, L., and Tak Â´ac, M. Rein-
forcement learning for solving the vehicle routing prob-
lem. Advances in neural information processing systems ,
31, 2018.
Pan, X., Jin, Y ., Ding, Y ., Feng, M., Zhao, L., Song, L.,
and Bian, J. H-tsp: Hierarchically solving the large-scale
travelling salesman problem. In Proceedings of the AAAI
Conference on Artificial Intelligence , 2023.
Qiu, R., Sun, Z., and Yang, Y . Dimes: A differentiable
meta solver for combinatorial optimization problems. Ad-
vances in Neural Information Processing Systems , 35:
25531â€“25546, 2022.
Son, J., Kim, M., Kim, H., and Park, J. Meta-sage: Scale
meta-learning scheduled adaptation with guided explo-
ration for mitigating scale shift on combinatorial opti-
mization. In International Conference on Machine Learn-
ing. PMLR, 2023.Sun, Z. and Yang, Y . Difusco: Graph-based diffusion
solvers for combinatorial optimization. arXiv preprint
arXiv:2302.08224 , 2023.
Uchoa, E., Pecin, D., Pessoa, A., Poggi, M., Vidal, T., and
Subramanian, A. New benchmark instances for the ca-
pacitated vehicle routing problem. European Journal of
Operational Research , 257(3):845â€“858, 2017.
Ulyanov, D., Vedaldi, A., and Lempitsky, V . Instance nor-
malization: The missing ingredient for fast stylization.
arXiv preprint arXiv:1607.08022 , 2016.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Å., and Polosukhin, I. Atten-
tion is all you need. Advances in Neural Information
Processing Systems , 30, 2017.
Vidal, T. Hybrid genetic search for the cvrp: Open-source
implementation and swap* neighborhood. Computers &
Operations Research , 140:105643, 2022.
Vinyals, O., Fortunato, M., and Jaitly, N. Pointer networks.
Advances in neural information processing systems , 28,
2015.
Wang, Y ., Jia, Y .-H., Chen, W.-N., and Mei, Y . Distance-
aware attention reshaping: Enhance generalization of
neural solver for large-scale vehicle routing problems.
arXiv preprint arXiv:2401.06979 , 2024.
Williams, R. J. Simple statistical gradient-following algo-
rithms for connectionist reinforcement learning. Machine
learning , 8:229â€“256, 1992.
Xiao, Y ., Wang, D., Li, B., Wang, M., Wu, X., Zhou, C.,
and Zhou, Y . Distilling autoregressive models to obtain
high-performance non-autoregressive solvers for vehi-
cle routing problems with faster inference speed. arXiv
preprint arXiv:2312.12469 , 2023.
Xin, L., Song, W., Cao, Z., and Zhang, J. Step-wise deep
learning models for solving routing problems. IEEE
Transactions on Industrial Informatics , 17(7):4861â€“4871,
2020.
Xin, L., Song, W., Cao, Z., and Zhang, J. Multi-decoder
attention model with embedding glimpse for solving ve-
hicle routing problems. In Proceedings of the AAAI Con-
ference on Artificial Intelligence , volume 35, pp. 12042â€“
12049, 2021.
Xing, Z. and Tu, S. A graph neural network assisted monte
carlo tree search approach to traveling salesman problem.
IEEE Access , 8:108418â€“108428, 2020.
Ye, H., Wang, J., Liang, H., Cao, Z., Li, Y ., and Li, F.
Glop: Learning global partition and local construction for
10Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization
solving large-scale routing problems in real-time. arXiv
preprint arXiv:2312.08224 , 2023.
Zhai, S., Talbott, W., Srivastava, N., Huang, C., Goh, H.,
Zhang, R., and Susskind, J. An attention free transformer.
arXiv preprint arXiv:2105.14103 , 2021.
Zhou, J., Wu, Y ., Song, W., Cao, Z., and Zhang, J. Towards
omni-generalizable neural methods for vehicle routing
problems. In International Conference on Machine Learn-
ing, 2023.
11Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization
A. POMO Structure
x1x2â€¦ xN
Context EmbeddingLinear Projection
ğ¡ğŸ(ğ‘³)ğ¡ğŸ(ğ‘³)â€¦ğ¡ğ‘µ(ğ‘³)Multi -Head Attention
Add & Instance Norm
Feed Forward
Add & Instance NormLÃ—
Multi -Head AttentionCompatibilitySoftmaxAutoregressiveğœ‹11ğœ‹21â€¦ğœ‹ğ‘1
ğœ‹12ğœ‹22â€¦ğœ‹ğ‘2
ğœ‹1ğ‘ğœ‹2ğ‘â€¦ğœ‹ğ‘ğ‘â€¦
ğœ‹ğ‘¡1ğœ‹ğ‘¡2â€¦ğœ‹ğ‘¡ğ‘
Figure 6. The Illustration of POMO Model. Note that POMO constructs Ntrajectories for a single instance, so the decoder outputs the
corresponding node selection for each trajectory at the current time step t.
As shown in Figure 6, POMO can be parameterized by Î¸, although POMO is derived from AM, it still has several differences
compared to AM. The implementation of POMO is shown as follows:
Encoder The encoder generates the embedding of each node based on the node coordinates as well as problem-specific
information (e.g., user demand for CVRP), aiming at embedding the graph information of the problem into high-dimension
vectors.
In the AM structure, the positional encoding is removed. And POMO disuse Layer Normalization (LN) (Ba et al., 2016),
which is used by Transformer, and Batch Normalization (BN)(Ioffe & Szegedy, 2015), which is used by AM, POMO uses
Instance Normalization (IN) (Ulyanov et al., 2016)
Given an instance X={xi}N
i=1, first of all, the encoder takes node features xiâˆˆRdxas model input, and transforms
them to initial embeddings h(0)
iâˆˆRdhthrough a linear projection, i.e., h(0)
i=Wx
ixi+bx
ifori= 1, . . . , N . The initial
embeddings {h(0)
1, . . . , h(0)
N}pass through the Lattention layers in turn, and are finally transformed to the final node
embeddings H(L)= (h(L)
1, . . . , h(L)
N).
Similar to the traditional Transformer, the attention layer of POMO consists of two sub-layers: a Multi-Head Attention
(MHA) sub-layer and a Feed-Forward (FF) sub-layer. Both of them use Instance Normalization and skip-connection (He
et al., 2016). Let H(â„“âˆ’1)= (h(â„“âˆ’1)
1, . . . , h(â„“âˆ’1)
N)be the input of the â„“-th attention layer for â„“= 1, . . . , L . The outputs of its
MHA and FF sub-layer in terms of the i-th node are calculated as:
Ë†h(â„“)
i= IN(â„“)(h(â„“âˆ’1)
i + MHA(â„“)(h(â„“âˆ’1)
i, H(â„“âˆ’1))), (13)
h(â„“)
i= IN(â„“)(Ë†h(â„“)
i+ FF(â„“)(Ë†h(â„“)
i)), (14)
where IN(Â·)denotes Instance Normalization, MHA( Â·)is Multi-Head Attention operation in Equation (13) (Vaswani et al.,
2017), and FF(Â·)in Equation (14) represents a fully connected neural network with the ReLU activation.
Decoder Based on the node embeddings generated by the encoder, the decoder adopts autoregressive mode to step-by-step
extend the existing partial solution and generate a feasible problem solution based on mask operation.
The decoder constructs the solution based on context embedding and node embeddings from the encoder step-by-step. In
the latest source code provided by the authors of POMO, for each time step, POMO adopts a simpler context embedding
rather than AM, and it has no graph embedding, i.e., Â¯h(L)=1
NPN
i=1h(L)
iâˆˆRdh.
12Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization
For the TSP, tâˆˆ {1, . . . , N }, the context embedding ht
(C)âˆˆR2Ã—dhis expressed as:
ht
(C)=(
[h(L)
Ï€1,h(L)
Ï€tâˆ’1] t >1
None t= 1,(15)
where h(L)
Ï€1is the embedding of the first visited node, in POMO, each node is considered as the first visited node, h(L)
Ï€tâˆ’1is
the embedding of the last visited node. And [Â·,Â·]denotes the concatenation operator.
LetË†ht
(C)âˆˆRdhrepresents the new context embedding. Ë†ht
(C)is calculated via a MHA operation:
Ë†ht
(C)= MHA(C)(ht
(C), H(L)). (16)
Finally, POMO computes the compatibility ut
iusing Equation (17), the output probabilities pÎ¸(Ï€t|s, Ï€1:tâˆ’1)is defined as1:
ut
i=ï£±
ï£²
ï£³Î¾Â·tanh(Ë†ht
(C)(h(L)
i)T
âˆšdk)ifiÌ¸âˆˆ {Ï€1:tâˆ’1}
âˆ’âˆ otherwise, (17)
where Î¾= 10 is a given clipping parameter in POMO, and then the probability pÎ¸(Ï€t=i|X, Ï€ 1:tâˆ’1)of the next visiting
node is obtained through Equation (5). Thereby, the probability of generating a complete solution Ï€for instance Xcan be
calculated in Equation (6).
Training According to Kwon et al. (2020), POMO is trained by the REINFORCE (Williams, 1992), and it uses gradient
ascent with an approximation in Equation (7).
B. Attention Free Transformer
According to Zhai et al. (2021), given the input X, AFT first transforms it to obtain Q, K, V by the corresponding linear
projection operation, respectively. Then, the calculation of AFT is expressed as:
Q=XWQ, K =XWK, V =XWV, (18)
Yi=Ïƒ(Qi)âŠ™PN
j=1exp (Kj+wi,j)âŠ™VjPN
j=1exp (Kj+wi,j)(19)
where WQ, WK, WVare three learnable matrices, âŠ™is the element-wise product, Ïƒqdenotes the nonlinear function applied
to the query Q, the default function is Sigmoid, wâˆˆRNÃ—Nis the pair-wise position biases, and each wi,jis a scalar. In
AFT, for every specified target position i, it executes a weighted average of values, and this averaged result is then integrated
with the query by performing an element-wise multiplication.
The basic version of AFT outlined in Equation (19) is called AFT-full, and it is the version that we have adopted. AFT
includes three additional variants: AFT-local, AFT-simple and AFT-conv. Owing to the removal of the multi-head mechanism,
AFT exhibits reduced memory usage and increased speed during both the training and testing, compared to the traditional
Transformer. In fact, AFT can be viewed as a specialized form of MHA, where each feature dimension is treated as an
individual head. A complexity analysis comparing â€AFT-fullâ€ with these other variants is provided in Table 4. Further
details are available in the related work section mentioned above.
C. Implementation Details
For the TSP and CVRP, we are adapted from the POMO model (Kwon et al., 2020), and we modify certain settings to suit
our specific requirements. We remove the weight decay method because we observe that the addition of weight decay is
useless for improving the model generalization performance. Note that for the CVRP model, we have implemented the
gradient clipping technique, setting the max norm parameter to 5, to prevent the risk of exploding gradients. The rest of the
settings are consistent with the original POMO model, and detailed information about the hyperparameter settings of our
models can be found in Table 5.
1In the latest source code provided by the authors of POMO, Ë†ht
(C)and(h(L)
i)Thave no imposed learnable matrices.
13Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization
Table 4. Complexity comparison of AFT-Full and other AFT variants. Here N, d, s denote the sequence length, feature dimension, and
local window size.
Model Time Space
Transformer O(N2d) O(N2+Nd)
AFT-full O(N2d) O(Nd)
AFT-simple O(Nd) O(Nd)
AFT-local O(Nsd), s < N O(Nd)
AFT-conv O(Nsd), s < N O(Nd)
Table 5. Model hyperparameter settings in TSP and CVRP.
TSP CVRP
Optimizer Adam
Clipping parameter 50
Initial learning rate 10âˆ’4
Learning rate of stage 3 10âˆ’5
Weight decay âˆ’
Initial Î±value 1
Loss function of stage 1 & 2 LPOMO
Loss function of stage 3 LJoint
Parameter Î²of stage 3 0.1
Parameter kof stage 3 20
The number of encoder layer 12
Embedding dimension 128
Feed forward dimension 512
Scale of stage 1 100
Scale of stage 2 & 3 [100,500]
Batches of each epoch 1,000
Epochs of stage 1 100
Epochs of stage 3 200
Epochs of stage 2 2,200 700
Capacity of stage 1 âˆ’ 50
Capacity of stage 2 & 3 âˆ’ [50,100]
Gradient clipping âˆ’ max norm= 5
Batch size of stage 1 256 128
Batch size of stage 2 & 3
160Ã—(100
N)2 
128Ã—(100
N)2
Total epochs 2,500 1 ,000
D. The Results on TSP Instances with Larger-scale
As shown in Table 6, our performance is slightly worse than the two SL-based NCO models, BQ and LEHD. Nevertheless,
on TSP 2000 and TSP 3000 instances, we achieve the best results in the RL-based constructive methods. And on TSP 4000
and TSP 5000 instances, we are only slightly worse than ELG. Overall, our method still has a good large-scale generalization
ability. Meanwhile, this observed trend reveals an important research direction: enhancing existing adaptation bias techniques
to sustain and improve the modelâ€™s performance in larger-scale TSP instances. Such advancements would enable the model
to effectively tackle more extensive problem spaces while retaining its efficient solution-generation capabilities.
14Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization
Table 6. Comparative results on TSP instances with scale >1000. â€œTimeâ€ represents the per-instance runtime.
TSP2K TSP3K TSP4K TSP5K
Method Obj. Gap Time (s) Obj. Gap Time (s) Obj. Gap Time (s) Obj. Gap Time (s)
LKH3 32.45 0.000% 144.67 39.60 0.000% 176.13 45.66 0.000% 455.46 50.94 0.000% 710.39
LEHD greedy 34.71 6.979% 5.60 43.79 10.558% 18.66 51.79 13.428% 43.88 59.21 16.237% 85.78
BQ greedy 34.03 4.859% 1.39 42.69 7.794% 3.95 50.69 11.008% 10.50 58.12 14.106% 25.19
POMO 50.89 56.847% 4.70 65.05 64.252% 14.68 77.33 69.370% 35.12 88.28 73.308% 64.46
ELG 36.14 11.371% 6.68 45.01 13.637% 19.66 52.67 15.361% 44.84 59.47 16.758% 81.63
ICAM 34.37 5.934% 1.80 44.39 12.082% 5.62 53.00 16.075% 12.93 60.28 18.338% 24.51
E. Detailed Ablation Study
Please note that, unless stated otherwise, the results presented in the ablation study reflect the best result from multiple
trajectories. We do not employ instance augmentation in this ablation study, and the performance on TSP instances is used
as the primary criterion for evaluation.
E.1. ICAM vs. POMO
Table 7. Comparison of ICAM and POMO on TSP and CVRP instances with different scales in the same training settings.
TSP100 TSP200 TSP500 TSP1000
Method Obj. Gap Time Obj. Gap Time Obj. Gap Time Obj. Gap Time
Concorde 7.7632 0.000% 34m 10.7036 0.000% 3m 16.5215 0.000% 32m 23.1199 0.000% 7.8h
POMO (original) single trajec. 7.8312 0.876% 2s 11.1710 4.367% <1s 22.1027 33.781% 1s 35.1823 52.173% 2s
POMO (original) 7.7915 0.365% 8s 10.9470 2.274% 1s 20.4955 24.053% 9s 32.8566 42.114% 1.1m
POMO (original) aug Ã—8 7.7736 0.134% 1m 10.8677 1.534% 5s 20.1871 22.187% 1.1m 32.4997 40.570% 8.5m
POMO single trajec. 8.1330 4.763% 2s 11.1578 4.243% <1s 17.3638 5.098% 1s 25.1895 8.952% 2s
POMO 7.8986 1.744% 8s 10.9080 1.910% 1s 17.0568 3.240% 9s 24.6571 6.649% 1.1m
POMO aug Ã—8 7.8179 0.704% 1m 10.8272 1.154% 5s 16.9530 2.612% 1.1m 24.5097 6.011% 8.5m
ICAM single trajec. 7.8328 0.897% 2s 10.8255 1.139% <1s 16.7777 1.551% 1s 23.7976 2.931% 2s
ICAM 7.7991 0.462% 5s 10.7753 0.669% <1s 16.6978 1.067% 4s 23.5608 1.907% 28s
ICAM aug Ã—8 7.7747 0.148% 37s 10.7385 0.326% 3s 16.6488 0.771% 38s 23.4854 1.581% 3.8m
CVRP100 CVRP200 CVRP500 CVRP1000
Method Obj. Gap Time Obj. Gap Time Obj. Gap Time Obj. Gap Time
LKH3 15.6465 0.000% 12h 20.1726 0.000% 2.1h 37.2291 0.000% 5.5h 37.0904 0.000% 7.1h
POMO (original) single trajec. 16.0264 2.428% 2s 21.9590 8.856% <1s 50.2240 34.905% 1s 150.4555 305.645% 2s
POMO (original) 15.8368 1.217% 10s 21.3529 5.851% 1s 48.2247 29.535% 10s 143.1178 285.862% 1.2m
POMO (original) aug Ã—8 15.7544 0.689% 1.2m 21.1542 4.866% 6s 44.6379 19.901% 1.2m 84.8978 128.894% 9.8m
POMO single trajec. 16.3210 4.311% 2s 20.9470 3.839% <1s 38.2987 2.873% 1s 39.6420 6.879% 2s
POMO 16.0200 2.387% 10s 20.6380 2.307% 1s 37.8702 1.722% 10s 39.0244 5.214% 1.2m
POMO aug Ã—8 15.8575 1.348% 1.2m 20.4851 1.549% 6s 37.6902 1.238% 1.2m 38.7652 4.515% 9.8m
ICAM single trajec. 16.1868 3.453% 2s 20.7509 2.867% <1s 37.9594 1.962% 1s 38.9709 5.070% 2s
ICAM 15.9386 1.867% 7s 20.5185 1.715% 1s 37.6040 1.007% 5s 38.4170 3.577% 35s
ICAM aug Ã—8 15.8720 1.442% 47s 20.4334 1.293% 4s 37.4858 0.689% 42s 38.2370 3.091% 4.5m
As detailed in Table 7, in our three-stage training scheme, POMO also obtains better performance on large-scale instances
compared to the original model, yet it falls short of ICAMâ€™s performance. In contrast to POMO, ICAM excels in capturing
cross-scale features and perceiving instance-conditioned information. This ability notably enhances model performance in
solving problems across various scales. Detailed information on the modelâ€™s performance at different stages can be found in
Appendix E.2.
E.2. The Effects of Different Stages
As illustrated in Table 8, after the first stage, the model performs outstanding performance with small-scale instances but
underperforms when dealing with large-scale instances. After the second stage, there is a marked improvement in the ability
to solve large-scale instances. By the end of the final stage, the overall performance is further improved. Notably, in our
15Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization
Table 8. Comparison between different stages on TSP instances with different scales.
TSP100 TSP200 TSP500 TSP1000
After stage 1 0.514% 1.856% 7.732% 12.637%
After stage 2 0.662% 0.993% 1.515% 2.716%
After stage 3 0.462% 0.669% 1.067% 1.907%
ICAM, the capability to tackle small-scale instances is not affected despite the instance scales varying during the training.
E.3. The Effects of Instance-Conditioned Adaptation Function
Table 9. The detailed ablation study on instance-conditioned adaptation function. Here AFM denotes that AAFM removes the adaptation
bias, and CAB is the compatibility with the adaptation bias.
TSP100 TSP200 TSP500 TSP1000
AFM 1.395% 2.280% 4.890% 8.872%
AFM+CAB 0.956% 1.733% 4.081% 7.090%
AAFM 0.514% 0.720% 1.135% 2.241%
AAFM+CAB 0.462% 0.669% 1.067% 1.907%
The data presented in Table 9 indicates a notable enhancement in the solving performance across various scales when
instance-conditioned information, such as scale and node-to-node distances, is integrated into the model. This improvement
emphasizes the importance of including detailed, fine-grained information in the model. It also highlights the critical role of
explicit instance-conditioned information in improving the adaptability and generalization capabilities of RL-based models.
In particular, the incorporation of richer instance-conditioned information allows the model to more effectively comprehend
and address the challenges, especially in the context of large-scale problems.
E.4. Parameter Settings in Stage 3
Table 10. Comparison between different parameters in Stage 3 on TSP instances with 1,000nodes.
single trajec. no augment.
Î²= 0 Î²= 0.1Î²= 0.5Î²= 0.9Î²= 0 Î²= 0.1Î²= 0.5Î²= 0.9
k= 20 2.996% 2.931% 3.423% 3.480% 2.039% 1.907% 1.859% 1.875%
k= 50 âˆ’ 3.060% 3.123% 3.328% âˆ’ 1.935% 1.892% 1.857%
k= 100 âˆ’ 2.979% 3.201% 3.343% âˆ’ 1.948% 1.899% 1.899%
As indicated in Table 10, when trained using LJoint as outlined in Equation (11), our model shows further improved
performance. When using the multi-greedy search strategy, we observe no significant performance variation among different
models at various kvalues. However, increasing the Î²coefficients, while yielding a marginal improvement in performance
with the multi-greedy strategy, notably diminishes the solving efficiency in the single-trajectory mode. Given the challenges
in generating Ntrajectories for a single instance as the instance scale increases, we are focusing on optimizing the model
effectiveness specifically in the single trajectory mode to obtain the best possible performance.
E.5. Comparison of Different Inference Strategies
As detailed in Table 11, upon attempting to replace the instance augmentation strategy with alternative inference strategies,
it is observed that there is no significant improvement in the performance of our model. However, the incorporation of
RRC technology into the LEHD model and the implementation of beam search technology into the BQ model both result
16Instance-Conditioned Adaptation for Large-scale Generalization of Neural Combinatorial Optimization
in substantial enhancements to the performance of respective models. This highlights a crucial insight: different models
require different inference strategies to optimize their performance. Consequently, it is essential to investigate more effective
strategies to achieve further improvements in the performance of ICAM.
Table 11. Experimental results with different inference strategies on TSP instances.
TSP100 TSP200 TSP500 TSP1000
Method Obj. Gap Time Obj. Gap Time Obj. Gap Time Obj. Gap Time
Concorde 7.7632 0.000% 34m 10.7036 0.000% 3m 16.5215 0.000% 32m 23.1199 0.000% 7.8h
BQ greedy 7.7903 0.349% 1.8m 10.7644 0.568% 9s 16.7165 1.180% 46s 23.6452 2.272% 1.9m
BQ bs16 7.7644 0.016% 27.5m 10.7175 0.130% 2m 16.6171 0.579% 11.9m 23.4323 1.351% 29.4m
LEHD greedy 7.8080 0.577% 27s 10.7956 0.859% 2s 16.7792 1.560% 16s 23.8523 3.168% 1.6m
LEHD RRC100 7.7640 0.010% 16m 10.7096 0.056% 1.2m 16.5784 0.344% 8.7m 23.3971 1.199% 48.6m
ICAM single trajec. 7.8328 0.897% 2s 10.8255 1.139% <1s 16.7777 1.551% 1s 23.7976 2.931% 2s
ICAM 7.7991 0.462% 5s 10.7753 0.669% <1s 16.6978 1.067% 4s 23.5608 1.907% 28s
ICAM RRC100 7.7950 0.409% 2.4m 10.7696 0.616% 14s 16.6886 1.012% 2.4m 23.5488 1.855% 16.8m
ICAM bs16 7.7915 0.365% 1.3m 10.7672 0.594% 14s 16.6889 1.013% 1.5m 23.5436 1.833% 10.5m
ICAM aug Ã—8 7.7747 0.148% 37s 10.7385 0.326% 3s 16.6488 0.771% 38s 23.4854 1.581% 3.8m
F. Licenses for Used Resources
Table 12. List of licenses for the codes and datasets we used in this work
Resource Type Link License
Concorde (Applegate et al., 2006) Code https://github.com/jvkersch/pyconcorde BSD 3-Clause License
LKH3 (Helsgaun, 2017) Code http://webhotel4.ruc.dk/ Ëœkeld/research/LKH-3/ Available for academic research use
HGS (Vidal, 2022) Code https://github.com/chkwon/PyHygese MIT License
H-TSP (Pan et al., 2023) Code https://github.com/Learning4Optimization-HUST/H-TSP Available for academic research use
GLOP (Ye et al., 2023) Code https://github.com/henry-yeh/GLOP MIT License
POMO (Kwon et al., 2020) Code https://github.com/yd-kwon/POMO/tree/master/NEW_py_ver MIT License
ELG (Gao et al., 2023) Code https://github.com/gaocrr/ELG MIT License
Pointerformer (Jin et al., 2023) Code https://github.com/pointerformer/pointerformer Available for academic research use
MDAM (Xin et al., 2021) Code https://github.com/liangxinedu/MDAM MIT License
LEHD (Luo et al., 2023) Code https://github.com/CIAM-Group/NCO_code/tree/main/single_objective/LEHD Available for any non-commercial use
BQ (Drakulic et al., 2023) Code https://github.com/naver/bq-nco CC BY-NC-SA 4.0 license
CVRPLib Dataset http://vrp.galgos.inf.puc-rio.br/index.php/en/ Available for academic research use
We list the used existing codes and datasets in Table 12, and all of them are open-sourced resources for academic usage.
17