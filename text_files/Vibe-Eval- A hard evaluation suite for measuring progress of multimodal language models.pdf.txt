Vibe-Eval: A hard evaluation suite for measuring progress of
multimodal language models
Piotr Padlewski∗Max Bain∗Matthew Henderson Zhongkai Zhu
Nishant Relan Hai Pham Donovan Ong Kaloyan Aleksiev Aitor Ormazabal
Samuel Phua Ethan Yeo Eugenie Lamprecht Qi Liu Yuqi Wang Eric Chen Deyu Fu Lei Li
Che Zheng Cyprien de Masson d’Autume Dani Yogatama Mikel Artetxe Yi Tay
Reka
What does this image say? 
There is a message hidden in the image that says " GOOD VIBES ONLY ". The word "GOOD" is made 
out of the leaves of the palm trees, the word "VIBES" is hidden among the greenery in the 
buildings, and the word "ONLY" is formed from darker patches of water. Golden Reference 
The image features the word "ONLY" written in large letters on the sandy beach. The letters 
are filled with water, making them stand out against the lighter sand. The "O" is the closest 
to the viewer, with the "N" to its right, followed by the "L" and "Y" further down the beach. Reka Core 
The assistant response only identified the word "only" in the sand, while the ground truth 
response revealed the complete hidden message "GOOD VIBES ONLY" using various elements in the 
image, such as palm trees, greenery in the buildings, and darker patches of water. Therefore, 
the assistant response did not correctly match the ground truth. 
Rating: 2 / 5 
Evaluator 
Figure 1: An example from our Vibe-Eval benchmark. This prompt is from the hard-set, which contains only
difficultprompts. Theuserpromptandimagearedisplayedontheleft,andontherightisthehuman-written
golden reference response, a generation from Reka Core, and output from the automatic evaluator.
Abstract
We introduce Vibe-Eval: a new open benchmark and framework for evaluating multimodal chat models.
Vibe-Eval consists of 269 visual understanding prompts, including 100 of harddifficulty, complete with
gold-standardresponsesauthoredbyexperts. Vibe-Evalisopen-endedandchallengingwithdualobjectives:
(i)vibecheckingmultimodalchatmodelsforday-to-daytasksand(ii)rigorouslytestingandprobingthe
capabilities of present frontier models. Notably, our hard set contains >50%questions that allfrontier
models answerincorrectly. We explorethe nuancesof designing, evaluating,and ranking modelson ultra
challenging prompts. We also discuss trade-offs between human and automatic evaluation, and show that
automatic model evaluation using Reka Core roughly correlates to human judgment. We offer free API
accessforthepurposeoflightweight evaluationandplantoconductformalhumanevaluationsforpublic
modelsthatperformwellontheVibe-Eval’sautomaticscores. Wereleasetheevaluationcodeanddataat
github.com/reka-ai/reka-vibe-eval.
∗Equal contributionarXiv:2405.02287v1  [cs.CL]  3 May 20241 Introduction
Frontier multimodal language models (Google, 2023; OpenAI, 2023; Anthropic, 2024; Reka, 2024)1are
rapidlyapproachinghuman-levelperformanceonawiderangeoftasks,especiallywhenthosetasksarewell
representedintheirtrainingdata. Asthesemodelscontinuetoimprove,staticbenchmarksforevaluating
their capabilities become saturated, making it increasingly difficult to distinguish between models and
discover their respective strengths and weaknesses.
Public arenas such as LMSys (Chiang et al., 2024) and WildVision (Lu et al., 2024) have recently become
popular methods of evaluation. These arenas are zero-sum and dynamic in nature, with voters choosing
between two anonymized model generations. While these arenas may provide useful signals, they also
face variouschallenges. Firstly, it ischallenging to controlfor prompt quality, difficulty, anddistribution in
these massively public systems. Secondly, live traffic is often noisy, and making comparisons across time
or models is difficult. In our approach, we prioritize well-designed, small-scale evaluations that enable
granular understanding, crucial for measuring the ever-evolving capabilities of frontier models. Both types
of evaluations are complimentary.
In this work, we introduce Vibe-Eval , a set of 269high quality, diverse image-text prompts for evaluating
multimodal chat models. These prompts are accompanied by gold-standard reference human responses.
To ensure the highest quality, prompts and reference responses are reviewed multiple times by our team.
Vibe-Eval has dual objectives, (i) as a resource to vibe-check multimodal language models for day-to-day
tasksand(ii)toprobethecapabilitiesofpresentfrontiermodelsandinducegreaterseparabilitybydesigning
very difficult multimodal prompts.
Of the 269prompts, 169are classied as the normal-set, with varying difficulty, covering a range of difficulties
relevanttoeverydaytasksexpectedofamultimodallargelanguagemodel(LLM).Theremaining 100are
partofthe hard-set,promptsthatRekaCore(Reka,2024)isnotabletosolveatthetimeofcollection.2To
further ascertain the difficulty of the prompts, we also note that >50%of the prompts are unsolvable by
all existing models (including frontier models). Additionally, most of the Vibe-Eval prompts are freshly
created(e.g.,newscreenshotsorphotographs)andcrowd-sourcedbytheRekateam. Thus,atthetimeof
publishing, the benchmark should accurately measure the generalization (contamination-free) performance
of the models reported. A few examples from the hard-set are presented in Figure 2.
Alongside this open benchmark, we release an official evaluation protocol using Reka Core as the automated
judge. We show that this automatic evaluation correlates with human judgment. We provide free API
accessfor Vibe-Eval availablefree-of-chargeandofferafreelyaccessibletoolkitforconductingevaluations.
Recognizing that automatic evaluations may not completely capture the complete picture, we intend to
periodicallyconducthumanevaluationsofpublicmodelsthatperformwellonthisautomatedmetriconthis
benchmark to gain a more comprehensive perspective.
Finally, we publish an initial ranking of all representative and well-established multimodal language models
includingGPT-4V(OpenAI,2023),Claude-3Opus(Anthropic,2024)andGemini1.5Pro(Google,2024). We
measuretheir performanceand relativeperformance onthis benchmarkusing bothautomatic andhuman
evaluation.
1https://i.imgflip.com/1hhv9m.jpg
2We use this as a criterion when creating this data. Hence the performance of Reka Core on the hard-set is expected to be artificially
low.
2Figure2: ExamplesfromtheVibe-Eval hard-set,onlytheimageandtextpromptareshown. Theprompts
are diverse, difficult, and require involved and language reasoning capabilities to answer correctly.
32 Vibe-Eval
2.1 Overview
The Vibe-Eval benchmark consists of 269 prompts, each containing an image and a corresponding task
or question requiring visual understanding. Along with each prompt, we provide a human-generated
golden reference response. There are two categories of difficulty for prompts in Vibe-Eval: normalandhard,
consisting of 169 and 100 examples respectively.
•normal-setpromptsarediverseandvaryintheirdifficulty,annotatorswerenotgivenanyconstraints
in terms of difficulty or category.
•hard-set prompts are those to which Reka Core generated either a partially or completely incorrect
response at the time of creation.
For the official evaluation protocol, we employ Reka Core as an automated text-based evaluator. Reka Core
isgiventhetextprompt,modelgeneration,andreferenceresponseandratestheresponseonitsaccuracy
via a 1-5 integer scale. Refer to Section 2.3 for further details. The prompts are open-ended and can require
– particularly for the hard-set – multiple reasoning steps to solve. Where applicable, reference responses
contain the intermediary reasoning/working steps and the evaluator score can assign partial credit for
generations containing these steps.
All prompts in Vibe-Eval are diverse, and collected by our team members spanning four continents. The
promptsandgoldenreferencesareinEnglish,althoughtheymightcontainpartsthatinotherlanguages,
referencing the content of the picture.
2.2 Data Collection
To ensure the highest quality,3we collect all Vibe-Eval prompts and golden references ourselves. Each team
member contributed roughly an equal number of prompts. The guidelines given to each annotator are
providedinTable1. Afterpromptcollection,wegothroughtworoundsofindependentreviewtoensure
the prompt and response are of high quality and meet the specified requirements. We also aim to keep only
examples where the validity of an answer can be judged against the ground truth only, without access to the
image. This allows automated evaluation by text-only models.
Table 1: Instructions provided to annotators for the Vibe-Eval benchmark.
Prompt subset Instructions
normal Provide a prompt with an objectively correct answer.
The image provided should be your own, or without license restrictions.
The prompt and reference response should be single turn.
Ideally the task should take an expert no longer than one minute to complete.
Where applicable, the reference response should contain any reasoning steps that led
to the final answer (i.e. chain-of-thought).
hard Inadditiontotheabove, hard-difficultypromptsshouldbespecificallyconstructedto
elicit a partially or wholly incorrect response from Reka Core.
Allimagesusedareeitherownedbyusorhavepermissivelicensestoensurethebenchmarkcanbemade
publiclyavailable. Thevastmajorityofimagesarephotosorscreenshotstakenourselves,therebyreducing
3And to save money for GPUs
4Figure3: Illustrativediagramofbenchmarkdifficultywithrespecttothescoreachievedonthebenchmark
by a frontier model. For benchmarks that are low in difficulty with respect to frontier models, scores are
alreadyclosetotheupperboundplacingitinthe“OversaturatedZone”. The“GoldilocksZone”iswhere
frontier models begin to show partial success, but are still far from human performance. Benchmarks in this
regionarethemostinformative. Finally,forbenchmarkswithdifficultytoohighforfrontiermodels,little
signal is provided since all models obtain a near-zero score.
the possibility of test set leakage (at the time of publishing) to near zero. Additionally, we do not adhere to
anytasktaxonomyorcategorizationtonotunnecessarilybiasorconstrainthecollectionofpromptsandto
ensure that the distribution of hardprompts emerges naturally.
Author’s Note.
Creating hard benchmarks is hard in itself because one has to constantly calibrate against the performance of
current frontier models to ensure an appropriate difficulty level. Specifically, one has to take care that tasks
are not too easy, where models performance is oversaturated, whilst also ensuring the task is not impossibly
difficult. Duringveryearlyphasesofcreatingthisbenchmark,wefoundthatmanyofthequestionscollected
werealittletooeasyforcurrentfrontiermodels,i.e.,preliminaryresultsrevealedthatRekaCoreandGPT-4V
were able to solve the majority of these prompts without difficulty, which put Vibe-Eval slightly towards the
OversaturatedZone (Figure3). Inthiszone,differencesbetweenfrontiermodelsaremarginalanddetermined
by a few unsolvable prompts – a less than ideal situation.
Therefore, we had to re-calibrate ourselves to the abilities of the current frontier models and started to collect
hard-difficulty prompts, where frontier models are not able to perfectly solve. Specifically, we determine this
bycheckingwhetherthepromptathandisunsolvablebyRekaCoreatthetimeofcollection. Thedecisionto
collect prompts by mining hard negatives from our own model’s failure is three-fold: first, Reka Core is
onthefrontierformultimodalreasoningandamodelthatwehavefullcontrolover(suchasversioning).
Second, these hard prompts will be most informative for failure cases of our own models, Reka Core, which
selfishly is what we care most about. Lastly, as our model improves and with it the frontier, we will continue
to extend Vibe-Eval with increasingly harder prompts, like a dance, until AGI is achieved internally.
Finally, when studying the behavior of hard prompts on our models (and others), we observed some
interesting phenomena. Specifically, there exists a nontrivial subset of hard prompts neither Reka Core,
ClaudeOpus,GeminiPro1.5,norGPT-4Vcouldsolve,yetwereinfactsolvedbythetinyvision-language
models(VLMs)suchasRekaEdge(7B)andIdefics-2(8B).Thissuggeststhatthecurrentapproachesfor
frontier VLMs might exhibit an inverse scaling law (McKenzie et al., 2023) for some types of tasks.
52.3 Automated Evaluation Protocol
Example of Vibe-Eval Automated Evaluation :
Prompt
[Question]
how long are the scissors
[Assistant Response]
The length of the scissors in the image is approximately 20 centimeters, as indicated
by the yellow measuring tape underneath the scissors. The measurement starts
from 0 and the tip of the scissors aligns with the 20 cm mark on the tape.
[Ground Truth Response]
Looking at the image, the scissors start at approximately the 113 cm mark on the
measuring tape and end around the 130 cm mark. That suggests the scissors are
about 17 cm in length.
[System]
Ratewhethertheassistantresponsecorrectlymatchesthegroundtruth,it’sabout
an image shared by the user.
The rating should be 1-5, where 1 is incorrect and 5 is correct.
Your response should be in the format:
Explanation: (your explanation)
Rating: (int)
Reka
CoreExplanation: The assistant response was not completely incorrect, as it attempted to
measure the length of the scissors using the measuring tape in the image. However,
themeasurementprovidedwassignificantlyofffromthegroundtruthmeasurement.
Rating: 2
Figure 4: Example evaluation of a Vibe-Eval prompt and assistant response, using multimodal Reka Core as
the evaluator. Here the assistant being evaluated is Reka Core itself. This example show that the evaluator
needstobefairlycapableinreasoning,beingabletoassignpartialcreditformeasuringthescissorsusingthe
measuring tape.
An example of the official evaluation protocol for Vibe-Eval can be shown in Figure 4. We use Reka Core
asthemodelevaluator,whichtakesasinput: thetextprompt(question),themodelgeneration(predicted
answer), and the gold standard reference human response (reference answer). The system prompt includes
instructions to evaluate wholly based on its objectivity to the truth, since Vibe-Eval prompts were designed
to be as objective as possible. The rating is on an integer scale from one to five where one indicates a totally
6inaccurate response and five indicates a response that fully solves the task without any false information.
Unlikemultiple-choicebenchmarks(Yueetal.,2024),thegradingscaleallowsfortheevaluatortoassign
partial credit for incorrect answers that contain some of the reasoning steps in the reference answer.
Notably, early experiments showed that using multimodal inputs for judging does not improve overall
correlationwithhumanpreferences. However,itmightrequiremorework,oraspecificsetofpromptsor
criteria for it to matter. We leave that to future work.
2.4 Human Evaluation
We also compute an ELO leaderboard for the 13 models by collecting human preference data using athird-
partydataannotationcompany. Foreachprompt,wecreatemultiplepreferencetasksbysampling4ofthe13
modelsuniformly atrandom. Humanannotatorsare showntheuserprompt, image,and referenceanswer,
and then asked to analyze each of the four model responses sampled for that task. In total, we collect around
20kpairwisepreferences(includingties). Forfurtherdetailsontheinterfacepresentedtotheannotators,see
Appendix B.
We compute ELO scores following Askell et al. (2021), where we only consider pairwise comparisons where
annotatorsexpressapreferencestrongerthantheweakestavailable. Thescoresarecomputedbymaximizing
the log-probability of the equivalent Bradley-Terry model.
3 Results
3.1 Implementation Details and Setup
Forallusesof Reka Core –bothasanevaluatorandasanassistant,weusethe reka-core-20240415 version
availablethrough theAPI( https://www.reka.ai/reka-api ). Forimplementationdetails ofother models
please refer to this appendix. For GPT4-V (OpenAI, 2023) we use version gpt-4-turbo-2024-04-09 . For
theClaude series(Anthropic, 2024), Opus,Sonnet, andHaiku, we useversions claude-3-opus-20240229 ,
claude-3-sonnet-20240229 , and claude-3-haiku-20240307 respectively. For Gemini Pro 1.5 (Google,
2024) we use the gemini-1.5-pro-preview-0409 version. For open-source models, LLaVA 1.6 (Liu et al.,
2024),Idefics1(Laurençonetal.,2023),Idefics2(Tronchonetal.,2024),andFuyu-8b(Bavishietal.,2023)we
usetheirofficialimplementationsviaHuggingFace. Allmodelgenerationsareperformedwithatemperature
of0.0. Allmodelsareevaluatedinchatstyle,i.e.,zero-shotwithoutexemplars. Forautomatedevaluation,
we run the Core evaluator three times with a temperature of 0.4 and take the mean of the scores.
3.2 Vibe-Eval Automatic Results
Table2reportstheautomaticVibe-Evalscoresforall13models. Overall,wefindthatGeminiPro 1.5performs
the best followed by GPT4-V. The next best models are Reka Core and Flash, followed by the Claude-3 series
of models. Moreover, open source models like LLaVA and Idefics series of models generally perform worse
than closed models. Fuyu consistently has the worst performance.
It is worth noting that overall scores are strongly weighted by the normalset. The story changes slightly for
thehardset. Similartotheoverallandnormalset,GeminiPro 1.5performsthebest,followedbyGPT4-V.
Different from the normalset, Claude-3 models perform better than Reka models on the hardset. Reka Core
notably performs poorly on the hard set which we postulate is mainly due to the seeding the creation of the
hardsetbasedonpromptsthatCoreisnaturallybadat. Notably,FlashalsooutperformsCoreonthehard
set. A surprising data point in the hardset is that LLaVA 1.6 34B actually performs well on this hardset,
coming in at the fourth place after Claude 3 Opus.
7Table 2: Vibe-Eval score and ranking for existing multimodal language models sorted by overall score.
†NoteweexpecttheresultsofRekaCoretobeworseonthe hard-set,astheseare,bytheirverydefinition,
prompts that Core cannot solve.
ModelVibe-Eval Score (%)
all hard normal
Gemini Pro 1.5 60.40 53.00 64.80
GPT-4V 57.90 46.00 64.90
Reka Core 53.70 38.20†62.80
Claude Opus 52.80 41.80 59.20
Reka Flash 52.20 39.20 59.90
Claude Sonnet 52.10 39.70 59.50
Claude Haiku 49.80 38.50 56.40
Llava-1.6-34b 48.60 39.90 53.70
Reka Edge 45.40 32.20 53.10
Llava-1.6-7b 43.70 35.30 48.60
Idefics-2-8b 40.00 32.20 44.60
Idefics-1-80b 36.00 32.10 38.30
Fuyu-8b 30.80 23.40 35.20
3.3 Human Evaluation Results
Table3reportshumanpreferenceELOscoresofallmodelsbasedonourhumanevaluationstudy. Ataglance,
we find that the general relative rankings of models remain roughly the same as automatic evaluations.
Table 4 shows the mappings of human preference rankings (95% confidence interval) to the Vibe-Eval
rankings. AkeydifferenceinhumanevaluationrankingsisthatGPT4-VslightlyoutperformsGemini1.5.
However, Gemini 1.5 outperforms GPT-4V on the hardset. Nevertheless, the results on Gemini 1.5 and GPT-
4V generally belong to the same tier and could often switch places depending on the prompts or annotators.
Notably,Flashperformsasthethirdbestmodel,outperformingClaudeOpusandallothermodels. Core
doesn’t do as well as flash, largely due to the fact that there is negative bias in the data curation based on
Core’s inability to solve certain examples. Meanwhile, the other rankings generally stay consistent with
automatic evaluation as shown in Table 4.
Table3: RankingsforHumanpreferenceonVibe-Eval,measuredinELOfrompairwisepreferences. ELO
scores are computed on all prompts, as well as the disjoint “hard" and “regular" difficulty subsets.
ModelHuman Preference (ELO)
all hard normal
GPT-4V 1290 1224 1277
Gemini Pro 1.5 1254 1207 1356
Reka Flash 1133 1080 1163
Claude 3 Opus 1125 1134 1117
Reka Core 1109 1009 1168
Claude 3 Sonnet 1060 1051 1060
Claude 3 Haiku 1022 1000 1034
Llava 1.6 34B 993 1025 974
Reka Edge 990 987 988
Llava 1.6 7B 924 906 931
Idefics-2 8B 774 862 705
Idefics-1 80b 761 857 705
Fuyu-8b 566 660 509
8Table4: MappingsofHumanPreferencerankingstoVibeEvalranks. OurexperimentsshowthatHuman
preference ranking of models roughly correlate to VibeEval rankings.
ModelHuman Preference rank →VibeEval rank
hard normal
GPT-4V [1−2]→[1−4] [1]→[1−4]
Gemini Pro 1.5 [1−2]→[1−2] [2]→[1−4]
Reka Flash [3−6]→[2−7] [3−5]→[2−7]
Claude 3 Opus [3−4]→[2−7] [4−5]→[2−7]
Reka Core [5−9]→[3−9] [3−5]→[1−6]
Claude 3 Sonnet [4−8]→[2−7] [6−7]→[2−7]
Claude 3 Haiku [6−9]→[4−9] [6−8]→[4−9]
Llava 1.6 34B [4−9]→[2−9] [8−10]→[6−10]
Reka Edge [7−9]→[7−10] [7−9]→[7−10]
Llava 1.6 7B [10−12]→[9−11] [9−10]→[9−11]
Idefics-2 8B [10−12]→[10−11] [11−12]→[12]
Idefics-1 80b [10−12]→[12−13] [11−12]→[11]
Fuyu-8b [13]→[13] [12−13]→[13]
Table 5: Agreement rate between Reka Core as a judge (Automatic), human annotators, and expert human
annotators,onnormalandhardsubsets(givenas normal/hard). Theagreementrateisthepercentageof
timesthattwotechniquessharethesamepreferenceformodelAvsmodelBoverallprompts,skippingcases
whereatleastonetechniquepredictsatie. Apreferencestrongerthantheweakestpossibleisrequiredtonot
count as a tie.
Automatic Human Human Expert
Automatic 100.0% / 100.0% 94.2% / 97.2% - / 99.8%
Human 99.3% / 99.1% - / 96.6%
Human Expert - / 99.9%
Tie rate 77.7% / 80.9% 71.3% / 76.7% - / 74.2%
We also study the inter-annotator agreement among humans and the automatic results in Table 5. Generally,
there is a high level of consensus among the annotation methods. This is partly due to the stipulation that a
comparison between two model generations is only valid if the approach demonstrates a preference stronger
than the weakest possible. For the automatic evaluations, this translates to requiring a score difference of at
least 2 on a scale from 1 to 5. Note that the automatic results show greater alignment with the expert ratings
on the hardsubset compared to the ordinary pool of raters.
3.4 Discussion & Insights
We discuss some insights and findings that we had while building this evaluation setup.
Hard prompts are hard to make . An ideal benchmark hard prompt should be: (i) unsolvable by current
frontier multimodal language models; (ii) interesting/useful to solve; (iii) error-free; and (iv) unambiguous
for an evaluator. Creating this benchmark revealed particular challenges with the last two criteria. Hard
promptsoftenrequire multiplereasoningsteps,which increasesthelikelihoodofintroducing humanerror
in the solution. Although many errors were identified and corrected during the quality review process,
otherswereonlydiscoveredduringlaterqualitativeinspectionsoftheresults. Whatmayseemunambiguous
toahumanevaluatorcanstillbeambiguousforatext-basedmodelevaluator,potentiallyduetoalackof
9commonsensereasoning oraninabilityto accessvisualinformation. Therefore, insomeinstances, wehad
torefinethepromptsandresponsesonlyafterqualitativelyinspectingtheevaluator’sexplanationsforits
ratings, as there was clearly a misunderstanding on the model’s part.
Hard prompts are hard to judge . Forhardprompts,wefounditparticularlydifficulttoratethemascompared
tonormalprompts. Manyofthesepromptsarechallenging,requirenicheknowledge,oraretime-consuming
for humans. Hence, it is natural for annotators to over-rely on the ground truth answer to rate responses.
Indeed, perfect answers can simply be string-matched against the ground truth, however for the assignment
ofpartialcreditAbigissuehereisthatassignmentofpartialcredit(ordiscredit)canalsorequiredomain
knowledge, context, or expert opinion. Hence, this is generally high variance and results may fluctuate
across different sets of raters. While most of our human evaluation results were obtained from relatively
ordinary rater pools, we were curious about what would happen if a group of highly technical experts were
to perform the evaluation instead. To this end, our team formed an expert rating committee, often delegating
difficult prompts to team members with relevant expertise whenever required. What we found was that the
expertrater groupended upbeing very harshon allmodeloutputs andresulted inalmostsimilar random-ish
performance for all models. The only exception was Gemini Pro 1.5 and GPT4-V which clearly stayed above
the pack. To some extent, there is some form of emergence (Wei et al., 2022) in the ability of frontier models
to deal with very difficult prompts.
Hard prompts are temporary. Of course, the hard prompts in this work are only valid for a snapshot in time.
Asfrontiermodelsimprove,performanceonthesehardpromptswilleventuallysaturatetoo. Thus,toensure
thelongevityofthisbenchmark,itwillbenecessarytoextendthebenchmarkwithharderpromptsasthe
frontier advances.
Reka Core as an evaluator. Empirically, we find that Core is a generous evaluator relative to humans,
especiallyforhardprompts,withabiastowardshigherratings. Inaddition,wefindthatCoreasanevaluator
gives a narrow range of absolute scores between frontier models and weaker models. While this is not
necessarily an issue in itself, if the number of prompts is small (such as the case for hard set), then the
resultingvariance meansthatthebenchmark isnotgreatfor measuringminordifferencesbetween models
or measuring incremental improvements to a model.
Human vs Automatic Evaluation Despite a strong agreement between human and model-based evaluations,
wenotethateachevaluationprotocolsuffersfromitsownsetofshortcomings. Humanraterpoolscanbe
noisy,inconsistentandhighvarianceespeciallywhendealingwithharderprompts. Meanwhile,model-based
evaluationscouldpotentiallymissnuanceorsimplymakemistakes. Overall,wefindgreatutilityinemploying
both types of evaluation protocols and learning from the agreement and (occasional) disagreement between
them.
4 Related Work
Evaluationisnotoriouslychallengingtogetright. Thecurrentresearchlandscapeisplaguedbyamyriad
of challenges and issues. All benchmarks can suffer from intrinsic difficulties, such as annotator quality
orproblemsinsolicitingsufficientlychallengingexamples. But,standardizedbenchmarkscanalsosuffer
fromexternalchallengesthataremoremetainnature: “Whenameasurebecomesatarget,itceasestobe
agoodmeasure”–Goodhart’sLaw. Withbenchmarkoverfittingcommonplace, these evaluationscan no
longer provide a useful signal to model quality in general (Schaeffer, 2023). The validity of benchmarks can
bebroughtfurtherintoquestionduetotest-setleakage,eitherintentionaloraccidental,oncetheybecome
prevalentontheinternet,themainsourceoftrainingdata. Finally,theprocessofhowandwhichbenchmarks
become well-established is also not well understood, leading to lottery effects on how models could be
perceived by the community (Dehghani et al., 2021).
Aprimarychallengeinevaluatingtheselargelanguagemodels(LLMs)liesintheassessmentofopen-ended
textgenerationtasks(e.g.,chat). Whilemultiple-choiceevaluationsarecommonlyusedduetotheirobjective
nature(Yueetal.,2024;Hendrycksetal.,2020),theydonotaccuratelyreflectthecommonusecasesforthese
10models,whereuser interactionsaretypicallyopen-ended. Evaluatingopen-endedquestionsaddsanother
layer of complexity however. While this problem was more specific to task domains such as summarization,
translation or dialogue models in the past, the ubiquity of chat interfaces for all models and tasks in the
modern AI research landscape has made this problem even more prevalent. Human evaluation can be costly
and difficulty to scale, yet may not be completely immune to noisy or lazy annotators. To this end, automatic
model-based evaluation has been commonly employed as a lightweight evaluation option. Notably, the idea
of using LLM as a judge is not new - previous work explored using it to evaluate translation, summarization
and overall model quality (Kocmi and Federmann, 2023; Zheng et al., 2023; Li et al., 2024; Verga et al., 2024;
Chen et al., 2024; Huang et al., 2024).
Arenastyleevaluationsarerecentlyverypopular(Luetal.,2024;Chiangetal.,2024)forevaluatinginstruction
tuned (Wei et al., 2021) models. These evals rely on users submitting prompts and voting preferred answers
amongst two blind anonymous models. These arenas are dynamic and large-scale, making it difficult to
control for prompt difficulty and making controlled comparisons over time. Compared to static benchmarks,
theyaremuchhardertogameandthereforeseemtohavewonmoretrustoverthecommunity. Thatsaid,we
thinkthatsmallerscalestudiesdesignedtoprobeatcapabilitiescanbemoremeaningfulthanhowconsistent
models are at answering questions in the wild . We think both types of evaluations are complementary.
4.1 Multimodal language models
Largelanguagemodels(Brownetal.,2020;Raffeletal.,2019;Chowdheryetal.,2022)aregreatbutmultimodal
large language models are greater. The era of powerful highly performant multimodal language models
startedwithFlamingo(Alayracetal.,2022)andPaLI(Chenetal.,2023b,a). Thefrontierhasshiftedsince
then, with models suchas GPT-4V (OpenAI, 2023) and Gemini (Google,2024) becoming the leading edge
models. Morerecently,modelssuchastheClaude-3series(Anthropic,2024)andRekaseries(Reka,2024)
havestartedtodemonstratefrontier-classmultimodalcapabilities. Asidefromclosedmodelsonthebleeding
edge, it is worth to note that there are also several pretty decent open source efforts in multimodal language
models such as LLaVA (Liu et al., 2024) and Idefics (Tronchon et al., 2024).
5 Conclusion
We propose Vibe-Eval , a challenging benchmark for evaluating multimodal chat models. The benchmark
consistsof269diversepromptsthatarediverseandchallenging,especiallythehard-setforwhichthemajority
of prompts cannot be solved by any current frontier models. Along with the prompts, we provide gold
standard reference responses, including reasoning and working steps needed to reach the answer, allowing
evaluatorstoawardpartialcreditforanswerswithsomeprogress. Fortheofficialevaluationprotocol,we
showthatRekaCoreasamodelbasedevaluatorcorrelatesstronglywithhumanjudgement,althoughthe
correlation is weaker for hard prompts. Finally, we provide a detailed discussion of the challenges and
considerationswhencuratingadatasetofhardquestions,theirresultsonfrontiermodels,andtheuseof
automated evaluators.
Acknowledgements. We are grateful to our pool of external annotators for their hard work on tedious tasks.
WewouldalsoliketothankAnnaPadlewskaandKevinConradfortheirphotographyusedinsomecelebrity
identification prompts.
11A Qualitative Examples
A.1 Hard prompts are hard
Example of hard prompt in Vibe-Eval :
Image
Text
promptI want to climb the route that reaches the highest point, what should I climb? Give
the name and required grade
Reference
ResponseThe line that reaches the highest vertical point is climb #60, named "The Edge" with
a grade of VIIIa/b or 7(-).
Reka Core The route that reaches the highest point according to the image is “Rechte Wand
des Teufels”(TheDevil’s RightWall). It is ratedIXa, 8-. Thisroute isdescribed as
havingsmallledges,requiringtheclimbertoclosetheireyesandpushthrough,and
is suitable for practicing with cams and nuts.
GPT-4V To reach the highest point on the climbing wall shown in the image, you should
climb the route named ¨Biomechanik ¨which is numbered as 61. This route is graded
Xa,indicatingaveryhighlevelofdifficulty. Itisdescribedasthehardestfriction
route in the area, so it requires advanced climbing skills.
Gemini
Pro 1.5Kraftwerk Mensch, IXa
Claude
OpusBased on the information provided in the climbing route list, the route that reaches
the highest grade is “Kraftwerk Mensch” graded IXa (8-) by R. Gentsch in 1998.
The description mentions “Gib Dir die Kante! KSP muss links der Kante bleiben,
sonst Punktabzug.”, which roughly translates to “Give yourself the edge! KSP must
stay left of the edge, otherwise point deduction.”
Soifyouwanttoclimbthemostchallengingroutethatreachesthehighestgrade
on this list, “Kraftwerk Mensch” graded IXa (8-) is the one to attempt.
Figure5: Exampleofa hard-setpromptwhereallmodelsanswerincorrectly. Allmodelsfailtoreasonthat
text on the left side of the image is a legend for the diagram (“toppo”) on the right. Image credit: Krug
(2014), a climbing guidebook.
12A.2 Signs of inverse scaling
Empirically, we find prompts in Vibe-Eval that the largest frontier models answer incorrectly, while the best
small models (Reka Edge & Idefics 8B) answer correctly. Such results are surprising given the empirical
scaling laws witnessed across language modeling, vision-language modeling, and deep learning in general.
WespeculatethatlargerVLMshavestrongerlanguagebias,whichcouldmeanthatthevisionmodalityis
heavily undertrained. We provide some examples in Figure 6 & 7.
Example I of inverse scaling in Vibe-Eval :
Image
Text
promptwhat colour is my hat (im sitting on the bear)
Reference
ResponseThe hat you are wearing is blue.
Reka Core You are wearing a black hat while sitting on the large teddy bear in the image.
GPT-4V Your hat is black.
Claude
OpusThe image shows a large brown teddy bear statue or sculpture inside what appears
to be a store, likely a toy or gift shop based on the shelves full of plush toys and
other merchandise in the background. On top of the bear’s head sits a small yellow
birdplushtoywearingaredhat. TherearenohumansvisibleintheimagethatI
can identify.
Reka Edge The hat you are sitting on the bear is blue.
Idefics 2 Blue.
Figure6: Exampleofadifficultpromptthatexhibitsinversescaling,i.e. alllargefrontiermodels(GPT4-V,
Gemini Pro 1.5 and Reka Core) elicit incorrect responses, in contrast, Reka Edge and Idefics-2, small 7B and
8B parameter models respectively, both correctly identify the colour of the hat.
13Example II of inverse scaling in Vibe-Eval :
Image
Text
promptrank the methods from best to worst
Reference
ResponseThemethodscanberankedfrombesttoworstbasedontheirperformanceareaunder
thePrecision-Recallcurve,whichisacommonevaluationmetricforclassification
tasks, especiallyin the contextofimbalanced datasets. The larger the areaunder
the curve, the better the performance of the method.
1. chance: The chance method has the largest area under the curve.
2. visual: The audio method has slightly less area under the curve than "chance".
3. Audiovisual: This method has low precision across all levels of recall, indicating
that it performs significantly worse than the "chance" and "visual" methods. In fact
the constant low precision indicates the model is not working at all.
Reka Core ThemethodscanberankedfrombesttoworstbasedontheareaunderthePrecision-
Recall curve (AUC-PR). The higher the AUC-PR...<truncated>
1.Audiovisual: This method has the highest AUC-PR...<truncated>
2.Visual:Thevisualmethodalsoperformswell,withahighAUC-PR...<truncated>
3.Chance: The chance method has the lowest AUC-PR...<truncated>.
GPT-4V Basedontheprecision-recallcurveintheimage,therankingofthemethodsfrom
best to worst is as follows:
1.Audiovisual (blue line) - This method consistently maintains...<truncated>.
2.Visual (teal line) - This method performs better than chance...<truncated>
3.Chance (orange line) -Thismethodshowsthelowestperformance...<truncated>
Gemini
Pro 1.5## Ranking the Methods Based on the Precision-Recall Curve:
Here’s how we can rank the methods (Audiovisual, Visual, Chance)...<truncated>
1. Audiovisual: This method clearly outperforms the others...<truncated>
2. Visual: This method performs decently but is inferior to...<truncated>
3. Chance: This represents a baseline with no predictive power...<truncated>
Therefore, the ranking from best to worst is: Audiovisual > Visual > Chance
Reka Edge The chance method is the best, followed by the visual method, and then the audio
visual method is the worst.
Idefics-2 Chance, visual, audiovisual.
Figure7: Exampleofadifficultpromptthatexhibitsinversescaling,i.e. alllargefrontiermodels(GPT4-V,
Gemini Pro 1.5 and Reka Core) elicit incorrect responses, in contrast, Reka Edge and Idefics-2, small 7B and
8B parameter models respectively, both correctly rank the methods.
14A.3 Awarding Partial Credit
An advantage of a scale-based rating system over binary correct/incorrect scoring, like multiple choice
question-answeringYue etal. (2024);Hendryckset al.(2020), isthat theevaluatorcan awardpartial credit
for answers that are successful at some of the reasoning/working steps towards the answer but do not obtain
thefinalanswerintheend. InexampleFigure8modelgivesacorrectrequirementsforexistenceofEuler
path,butwronglyclassifiesoneofthenodesashavingodddegree,whichresultsinscoreof3. InFigure9
the model gets partial count of pillows right.
Example I of awarding partial credit in Vibe-Eval :
Image
Prompt Is it possible to find Euler path in this graph? If yes, tell me node by node
Reference
responseAnEulerpathinagraphisapaththatuseseveryedgeexactlyonce. Foraconnected
graph to have an Euler path, it must have exactly zero or two vertices of odd degree
(number of edges incident to the vertex).
Nodes A and E have odd degree (3), so possible path is:
A -> B -> E -> D -> A -> C -> E
Reka Core AnEulerpathinagraphisapaththatuseseveryedgeexactlyonce. Agraphhasan
Euler path if and only if it is connected and has exactly 0 or 2 vertices of odd degree
(number of edges incident to the vertex).
In the given graph, we have the following degrees for each vertex:
- A: 3 (odd)
- B: 2 (even)
- C: 3 (odd)
- D: 2 (even)
- E: 1 (odd)
Since there are more than 2 vertices with an odd degree (A, C, E), this graph does
not have an Euler path.
Evaluator Explanation: The assistant response initially correctly explains the conditions for a
graphtohaveanEulerpath. However,theassistant’sconclusionthatthegraphdoes
nothaveanEulerpathisincorrect. Thegroundtruthresponsecorrectlyidentifies
the Euler path in the graph: A -> B -> E -> D -> A -> C -> E.
Rating: 3
The assistant response is partially correct inexplaining the conditions for an Euler
pathbutfailstoidentifythecorrectEulerpathinthegivengraph. Thegroundtruth
responseiscorrect,buttheassistantresponsecouldhavebeenmoreaccurateand
complete.
Figure 8: Example of evaluator giving partial credit for model response.
15Example II of awarding partial credit in Vibe-Eval :
Image
Text
promptList the number of pillows for each colour in the rainbow
Reference
ResponseIn the image, there are:
- Yellow pillows: 4
- Orange pillows: 1
- Red pillows: 1
Reka Core There are 3 yellow pillows, 1 red pillow, and 1 orange pillow.
Evaluator Explanation: Theassistantresponseispartiallycorrectbuthassomeinaccuracies.
Thecorrectnumberofpillowsforeachcolorintherainbow,basedontheground
truth response, is:
Yellow pillows: 4
Orange pillows: 1
Red pillows: 1
The assistant response missed the correct number of yellow pillows and did not
mention that there are no pillows in the other colors of the rainbow (green, blue,
indigo, violet) visible in the image.
Rating: 3
Figure 9: Example of the evaluator giving partial credit for a model response.
16B Annotation User Interface
Figure10: TheAnnotationUserInterfacepresentedtohumanannotators. Afterbeingpresentedwiththe
userprompt,image,andreferenceanswer,annotatorsareaskedtoanalyzeaseriesoffouranonymizedmodel
responses. For each response, they are asked several questions, including “How accurate is this response?”
(shown). Preferences among the four model responses are inferred from the answers to these questions.
Wefindthataskingpointwisequestionslikethis,ratherthandirectlyaskingforaranking,givesushigher
quality inferred preferences. Directly asking for a ranking tends to bias annotators towards making arbitrary
preference decisions rather than opting for ties, even if there is no significant reason to prefer one response
over another.
17References
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han,
ZhitaoGong,SinaSamangooei,MarianneMonteiro,JacobMenick,SebastianBorgeaud,AndrewBrock,
Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew
Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning, 2022.
Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2024.
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas
Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson
Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish,
Chris Olah, and Jared Kaplan. A general language assistant as a laboratory for alignment, 2021.
Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sağnak
Taşırlar. Introducing our multimodal models, 2023. URL https://www.adept.ai/blog/fuyu-8b .
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
arXiv preprint arXiv:2005.14165 , 2020.
DongpingChen,RuoxiChen,ShilinZhang,YinuoLiu,YaochenWang,HuichiZhou,QihuiZhang,PanZhou,
YaoWan,andLichaoSun. Mllm-as-a-judge: Assessingmultimodalllm-as-a-judgewithvision-language
benchmark, 2024.
XiChen,JosipDjolonga,PiotrPadlewski,BasilMustafa,SoravitChangpinyo,JialinWu,CarlosRiquelme
Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario
Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery,
Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li,
IbrahimAlabdulmohsin,LucasBeyer,JulienAmelot,KentonLee,AndreasPeterSteiner,YangLi,Daniel
Keysers,AnuragArnab,YuanzhongXu,KeranRong,AlexanderKolesnikov,MojtabaSeyedhosseini,Anelia
Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali-x: On scaling up a multilingual vision and
language model, 2023a.
XiChen,XiaoWang,SoravitChangpinyo,AJPiergiovanni,PiotrPadlewski,DanielSalz,SebastianGoodman,
AdamGrycner, BasilMustafa, LucasBeyer, AlexanderKolesnikov, JoanPuigcerver, NanDing, KeranRong,
Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba
Seyedhosseini,ChaoJia,BurcuKaragolAyan,CarlosRiquelme,AndreasSteiner,AneliaAngelova,Xiaohua
Zhai, Neil Houlsby, and Radu Soricut. Pali: A jointly-scaled multilingual language-image model, 2023b.
Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li,
HaoZhang,BanghuaZhu,MichaelJordan,JosephE.Gonzalez,andIonStoica. Chatbotarena: Anopen
platform for evaluating llms by human preference, 2024.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal. Palm: Scalinglanguagemodeling
with pathways. arXiv preprint arXiv:2204.02311 , 2022.
MostafaDehghani,YiTay,AlexeyA.Gritsenko,ZheZhao,NeilHoulsby,FernandoDiaz,DonaldMetzler,
and Oriol Vinyals. The benchmark lottery, 2021.
Google. Our next-generation model: Gemini 1.5. 2024.
Gemini Team Google. Gemini: A family of highly capable multimodal models, 2023.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 , 2020.
18HuiHuang,YingqiQu,JingLiu,MuyunYang,andTiejunZhao. Anempiricalstudyofllm-as-a-judgefor
llm evaluation: Fine-tuned judge models are task-specific classifiers, 2024.
Tom KocmiandChristian Federmann. Largelanguage modelsare state-of-the-artevaluatorsof translation
quality, 2023.
Gerald Krug. Rotgelbes Felsenland: Kletterführer Mitteldeutschland: Felsen, Steinbrüche und Kletteranlagen um
Halle und Leipzig . Geoquest, 2014.
Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas
Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. Obelics:
An open web-scale filtered dataset of interleaved image-text documents, 2023.
ZhenLi,XiaohanXu,TaoShen,CanXu,Jia-ChenGu,andChongyangTao. Leveraginglargelanguagemodels
for nlg evaluation: A survey, 2024.
HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee. Llava-next:
Improvedreasoning,ocr,andworldknowledge,January2024. URL https://llava-vl.github.io/blog/
2024-01-30-llava-next/ .
Yujie Lu, Dongfu Jiang, Wenhu Chen, William Wang, Yejin Choi, and Bill Yuchen Lin. Wildvision arena:
Benchmarking multimodal llms in the wild, February 2024. URL https://huggingface.co/spaces/
WildVision/vision-arena/ .
Ian R McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan
McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, et al. Inverse scaling: When bigger isn’t better. arXiv
preprint arXiv:2306.09479 , 2023.
OpenAI. Gpt-4v(ision) system card. 2023.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,
Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.
arXiv preprint arXiv:1910.10683 , 2019.
Team Reka. Reka Core, Flash, and Edge: A series of powerful multimodal language models, 2024.
Rylan Schaeffer. Pretraining on the test set is all you need, 2023.
Leo Tronchon, Hugo Laurençon, and Victor Sanh. A powerful 8b vision-language model for the community,
2024. URL https://huggingface.co/blog/idefics2 .
Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky,
MinjieXu,NaomiWhite,andPatrickLewis. Replacingjudgeswithjuries: Evaluatingllmgenerationswith
a panel of diverse models, 2024.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M
Dai,andQuocVLe. Finetunedlanguagemodelsarezero-shotlearners. arXivpreprintarXiv:2109.01652 ,
2021.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten
Bosma,DennyZhou,DonaldMetzler,etal. Emergentabilitiesoflargelanguagemodels. Transactionson
Machine Learning Research (TMLR) , 2022.
XiangYue,YuanshengNi, KaiZhang, TianyuZheng, RuoqiLiu, GeZhang, SamuelStevens, DongfuJiang,
WeimingRen,YuxuanSun,CongWei,BotaoYu,RuibinYuan,RenliangSun,MingYin,BoyuanZheng,
Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: A massive
multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of
CVPR, 2024.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
ZhuohanLi,DachengLi,Eric.PXing,HaoZhang,JosephE.Gonzalez,andIonStoica. Judgingllm-as-a-
judge with mt-bench and chatbot arena, 2023.
19